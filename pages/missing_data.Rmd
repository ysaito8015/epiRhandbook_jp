---
title:  |  
  ![](../images/R Handbook Logo.png)
author: ""
date: "Produced `r format(Sys.time(), '%A %d %B %Y')`"
output:
  html_document:
    code_folding: show
    highlight: zenburn
    number_sections: no
    theme: sandstone
    toc: yes
    toc_collapse: no
    toc_depth: 3
    toc_float: yes
params:
    run_page_ind: TRUE
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "_outputs_knitted") })
---

```{r, child= '_page_setup.Rmd', eval = params$run_page_ind, include = F}
```

<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Missing data {#transformation .tabset .tabset-fade}

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "missingness.png"))
```

<!-- ======================================================= -->
## Overview {.tabset .tabset-fade}

This page will cover:  

1) Useful functions for assessing missingness  
2) Assess missingness in a dataframe  
3) Plotting missingness over time  
4) Handling how `NA` is displayed in plots  
5) Imputation  



<!-- ======================================================= -->
## Useful functions {.tabset .tabset-fade}
<h2> Useful functions </h2>

The following are useful functions when assessing or handling missing values:  


**`is.na()` and `!is.na()`**  

To identify missing values use `is.na()` or its opposite (with `!` in front). Both are from **base** R.  
These return a logical vector (`TRUE` or `FALSE`). Remember that you can `sum()` the resulting vector to count the number `TRUE`, e.g. `sum(is.na(linelist$date_outcome))`.   

```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)
is.na(my_vector)
!is.na(my_vector)
```

**`na.omit()`**  

This function, if applied to a dataframe, will remove rows with *any* missing values. It is also from **base** R.  
If applied to a vector, it will remove `NA` values from the vector it is applied to. For example:  

```{r}
sum(na.omit(my_vector))
```

**`na.rm = TRUE`**  

Often a mathematical function will by default *include* `NA` in calculations, which results in the function returning `NA` (this is designed intentionally, to make you aware that you have missing data).  
You can usually avoid this by removing missing values from the calculation, by including the argument `na.rm = TRUE` (na.rm stands for "remove `NA`").  

```{r}
mean(my_vector)

mean(my_vector, na.rm = TRUE)
```



<!-- ======================================================= -->
## Assess a dataframe {.tabset .tabset-fade}
<h2> Assess a dataframe </h2>




<!-- ======================================================= -->
## Missingness over time {.tabset .tabset-fade}
<h2> Missingness over time </h2>


Change in percent of weekly observations that are missing in X column.  

```{r}
outcome_missing <- linelist %>%
  mutate(week = lubridate::floor_date(date_onset, "week")) %>% 
  group_by(week) %>% 
  summarize(n_obs = n(),
            outcome_missing = sum(is.na(outcome) | outcome == ""), # include "" because this is character
            outcome_p_miss = outcome_missing / n_obs) %>%
  reshape2::melt(id.vars = c("week")) %>%
  filter(grepl("_p_", variable))
```

Then we plot the proportion missing as a line, by week

```{r}
ggplot(data = outcome_missing)+
    geom_line(aes(x = week, y = value, group = variable, color = variable), size = 2, stat = "identity")+
    labs(title = "Weekly missingness in 'Outcome'",
         x = "Week",
         y = "Proportion missing") + 
    scale_color_discrete(name = "", labels = c("Weekly proportion of missing outcomes"))+
    scale_y_continuous(breaks = c(seq(0,1,0.1)))+
  theme_minimal()+
  theme(
    legend.position = "bottom"
  )
```




<!-- ======================================================= -->
## `NA` in plots {.tabset .tabset-fade}
<h2> `NA` in plots </h2>





<!-- ======================================================= -->
## Imputation {.tabset .tabset-fade}
<h2> Imputation </h2>


Sometimes, when analyzing your data, it will be important to "fill in the gaps" and impute missing data While you can always simply analyze a dataset after removing all missing values, this can cause problems in many ways. Here are two examples: 

1) By removing all observations with missing values or variables with a large amount of missing data, you might reduce your power or ability to do some types of analysis. For example, as we discovered earlier, only 31.7% of the observations in our linelist dataset have no missing data across all of our variables. If we removed the majority of our dataset we'd be losing a lot of information! And, most of our variables have some amount of missing data--for most analysis it's probably not reasonable to drop every variable that has a lot of missing data either.

2) Depending on why your data is missing, analysis of only non-missing data might lead to biased or misleading results. For example, as we learned earlier we are missing data for some patients about whether they've had some important symptoms like fever or cough. But, as one possibility, maybe that information wasn't recorded for people that just obviously weren't very sick. In that case, if we just removed these observations we'd be excluding some of the healthiest people in our dataset and that might really bias any results.

It's important to think about why your data might be missing in addition to seeing how much is missing. Doing this can help you decide how important it might be to impute missing data, and also which method of imputing missing data might be best in your situation.

### Types of missing data

Here are three general types of missing data:

1) **Missing Completely at Random** (MCAR). This means that there is no relationship between the probability of data being missing and any of the other variables in your data. The probability of being missing is the same for all cases This is a rare situation. But, if you have strong reason to believe your data is MCAR analyzing only non-missing data without imputing won't bias your results (although you may lose some power). [TODO: consider discussing statistical tests for MCAR]

2) **Missing at Random** (MAR). This name is actually a bit misleading as MAR means that your data is missing in a systematic, predictable way based on the other information you have. For example, maybe every observation in our dataset with a missing value for fever was actually not recorded because every patient with chills and and aches was just assumed to have a fever so their temperature was never taken. If true, we could easily predict that every missing observation with chills and aches has a fever as well and use this information to impute our missing data. In practice, this is more of a spectrum. Maybe if a patient had both chills and aches they were more likely to have a fever as well if they didn't have their temperature taken, but not always. This is still predictable even if it isn't perfectly predictable. This is a common type of missing data 

3) **Missing not at Random** (MNAR). Sometimes, this is also called **Not Missing at Random** (NMAR). This assumes that the probability of a value being missing is NOT systematic or predictable using the other information we have but also isn't missing randomly. In this situation data is missing for unknown reasons or for reasons you don't have any information about. For example, in our dataset maybe information on age is missing because some very elderly patients either don't know or refuse to say how old they are. In this situation, missing data on age is related to the value itself (and thus isn't random) and isn't predictable based on the other information we have. MNAR is complex and often the best way of dealing with this is to try to collect more data or information about why the data is missing rather than attempt to impute it. 

In general, imputing MCAR data is often fairly simple, while MNAR is very challenging if not impossible. Many of the common data imputation methods assume MAR. 

### Useful packages

Some useful packages for imputing missing data are Mmisc, missForest (which uses random forests to impute missing data), and mice (Multivariate Imputation by Chained Equations). For this section we'll just use the mice package, which implements a variety of techniques. The maintainer of the mice package has published an online book about imputing missing data that goes into more detail here (https://stefvanbuuren.name/fimd/).  

Here is the code to load the mice package:

```{r}
pacman::p_load(mice)
```

### Mean Imputation

Sometimes if you are doing a simple analysis or you have strong reason to think you can assume MCAR, you can simply set missing numerical values to the mean of that variable. Perhaps we can assume that missing temperature measurements in our dataset were either MCAR or were just normal values. Here is the code to create a new variable that replaces missing temperature values with the mean temperature value in our dataset. However, in many situations replacing data with the mean can lead to bias, so be careful.

```{r}
linelist = linelist %>% mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))
```

You could also do a similar process for replacing categorical data with a specific value. For our dataset, imagine you knew that all observations with a missing value for their outcome (which can be "Death" or "Recover") were actually people that died (note: this is not actually true for this dataset):

```{r}
linelist = linelist %>% mutate(outcome_replace_na_with_death = 
                                 replace_na(outcome, "Death"))
```

### Regression imputation

A somewhat more advanced method is to use some sort of statistical model to predict what a missing value is likely to be and replace it with the predicted value. Here is an example of creating predicted values for all the observations where temperature is missing, but age and fever are not using simple linear regression using fever status, and age in years as predictors. In practice you'd want to use a better model than this sort of simple approach.
```{r}
simple_temperature_model_fit = lm(temp ~ fever + age_years, data = linelist)
predictions_for_missing_temps = predict(simple_temperature_model_fit,
                                        newdata = linelist %>% filter(is.na(temp))) #using our simple temperature model to predict values just for the observations where temp is missing
```

Or, using the same modeling approach through the mice package to create imputed values for the missing temperature observations:

```{r}
model_dataset = linelist %>%
  select(temp, fever, age_years)
temp_imputed_values = mice(model_dataset, method = "norm.predict", seed = 1, m = 1, print = F)$imp$temp
```


This is the same type of approach by some more advanced methods like using the missForest package to replace missing data with predicted values. In that case, the prediction model is a random forest instead of a linear regression. You can use other types of models to do this as well. However, while this approach works well under MCAR you should be a bit careful if you believe MAR or MNAR more accurately describes your situation. The quality of your imputation will depend on how good your prediction model is and even with a very good model the variability of your imputed data may be underestimated. 

### LOCF and BOCF

Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) are  imputation methods for time series/longitudinal data. The idea is to take the previous observed value as a replacement for the missing data. When multiple values are missing in succession, the method searches for the last observed value.

[TO BE COMPLETED]

### Multiple Imputation

The online book we mentioned earlier by the author of the mice package (https://stefvanbuuren.name/fimd/) contains a detailed explanation of multiple imputation and why you'd want to use it. But, here is a basic explanation of the method:

When you do multiple imputation, you create multiple datasets with the missing values imputed to plausible data values (depending on your research data you might want to create more or less of these imputed datasets, but the mice package sets the default number to 5). The difference is that rather than a single, specific value each imputed value is drawn from an estimated distribution (so it includes some randomness). As a result, each of these datasets will have slightly different different imputed values (however, the non-missing data will be the same in each of these imputed datasets). You still using some sort of predictive model to do the imputation in each of these new datasets (mice has many options for prediction methods including *Predictive Mean Matching*, *logistic regression*, and *random forest*) but the mice package can take care of many of the modeling details. 

Then, once you have created these new imputed datasets, you can apply then apply whatever statistical model or analysis you were planning to do for each of these new imputed datasets and pool the results of these models together. This works very well to reduce bias in both MCAR and many MAR settings and often results in more accurate standard error estimates.

Here is an example of applying the Multiple Imputation process to predict temperature in our linelist dataset using a age and fever status (our simplified model_dataset from above):
[Note from Daniel: this is not a very good model example and I'll change it later]

```{r}
multiple_imputation = mice(model_dataset, seed = 1, m = 10, print = FALSE) #imputing missing values for all variables in our model_dataset, and creating 10 new imputed datasets

model_fit <- with(multiple_imputation, lm(temp ~ age_years + fever))
summary(pool(model_fit))
```

Here we used the mice default method of imputation, which is Predictive Mean Matching. We then used these imputed datasets to seperately estimate and then pool results from simple linear regressions on each of these datasets. There are many details we've glossed over and many settings you can adjust during the Multiple Imputation process while using the mice package. For example, you won't always have numerical data and might need to use other imputation methods (you can still use the mice package for many other types of data and methods). But, for a more robust analysis when missing data is a significant concern, Multiple Imputation is good solution that isn't always much more work than doing a complete case analysis. 




<!-- ======================================================= -->
## Resources {.tabset .tabset-fade}
<h2> Resources </h2>





```{r, child= '_page_closeout.Rmd', eval = params$run_page_ind == F, include = F}
```
