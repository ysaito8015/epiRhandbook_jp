[{"path":"index.html","id":"welcome---this-is-a-draft","chapter":"Welcome - THIS IS A DRAFT","heading":"Welcome - THIS IS A DRAFT","text":"","code":""},{"path":"index.html","id":"about-this-handbook","chapter":"Welcome - THIS IS A DRAFT","heading":"About this handbook","text":"free open-access R reference book applied epidemiologists public health practitioners.book strives :Serve quick reference guide - textbookAddress common epidemiological problems via task-centered examplesBe accessible settings limited technical support low internet-connectivity (link downloadable version)Contain clear simple language, step--step instructions helpful code annotationBe living document, growing adapting new best practicesWhat gaps book address?Many epidemiologists lack formal R training transitioning SAS, STATA, statistical software.R universe changes frequently - place best practice code catered toward common epidemiologist user.Epidemiologists often search dozens online forums answers, epidemiology-oriented.epidemiologists work low internet-connectivity environments limited technical support.different R books?handbook written epidemiologists, epidemiologists. approved product specific organization. Examples techniques adapted authors lived experience local, national, academic, emergency settings.book offered download-able format settings unreliable internet.addition core R skills book uses epidemiology-centered examples cover tasks like epidemic curves, transmission chains epidemic modeling, age sex pyramids, age sex standardization, probabilistic matching records, outbreak detection methods, survey analysis, causal diagrams, survival analysis, GIS basics, phylogenetic trees, missing data imputation, automated routine reports Rmarkdown, etc…","code":""},{"path":"index.html","id":"how-to-read-this-handbook","chapter":"Welcome - THIS IS A DRAFT","heading":"How to read this handbook","text":"Search via search box Table ContentsClick “clipboard” “copy” icon copy codeSee “Resources” section page links trainingClick download offline versionIf use handbook suggestions, let us know SURVEY LINK!","code":""},{"path":"index.html","id":"edit-or-contribute","chapter":"Welcome - THIS IS A DRAFT","heading":"Edit or contribute","text":"suggestions want contribute content, please post issue submit pull request github repository.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Welcome - THIS IS A DRAFT","heading":"Acknowledgements","text":"","code":""},{"path":"index.html","id":"contributors","chapter":"Welcome - THIS IS A DRAFT","heading":"Contributors","text":"handbook collaborative team production. conceived, written, edited epidemiologists public health practitioners around world, drawn upon experiences within constellation organizations including local/state/provincial/national health departments ministries, World Health Organization (), MSF (Medecins sans frontiers / Doctors without Borders), UNHCR, WFP, hospital systems, academic institutions.team members:Editor--Chief: Neale BatraEditorial core team: Alex Spina, Amrish Baidjoe, Henry Laurenson-Schafer, Finlay Campbell, Pat KeatingAuthors (order contributions): Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, Isaac Florence, Natalie Fischer, Daniel Molling, Liza Coyer, Jonny Polonski, Yurie Izawa, Sara HollisReviewers: …(list)…Advisers …(list)…","code":""},{"path":"index.html","id":"funding-and-programmatic-support","chapter":"Welcome - THIS IS A DRAFT","heading":"Funding and programmatic support","text":"handbook received funding via COVID-19 emergency capacity-building grant Training Programs Epidemiology Public Health Interventions Network (TEPHINET).Programmatic support provided EPIET Alumni Network (EAN).","code":""},{"path":"index.html","id":"inspiration","chapter":"Welcome - THIS IS A DRAFT","heading":"Inspiration","text":"multitude tutorials vignettes provided foundational knowledge development handbook content credited within respective pages.generally, following sources provided inspiration laid groundwork handbook:“R4Epis” project (collaboration MSF RECON)R Epidemics Consortium (RECON)R Data Science book (R4DS)bookdown: Authoring Books Technical Documents R MarkdownNetlify hosts website","code":""},{"path":"index.html","id":"image-credits","chapter":"Welcome - THIS IS A DRAFT","heading":"Image credits","text":"Logo: CDC Public Health Image library, R Graph Gallery2013 Yemen looking mosquito breeding sitesEbola virusSurvey RajasthanNetwork","code":""},{"path":"index.html","id":"license-and-terms-of-use","chapter":"Welcome - THIS IS A DRAFT","heading":"License and Terms of Use","text":"handbook approved product specific organization.Although strive accuracy, provide guarantee content book.book licensed Creative Commons license TBD…","code":""},{"path":"time-series-analysis.html","id":"time-series-analysis","chapter":"1 Time series analysis","heading":"1 Time series analysis","text":"","code":""},{"path":"time-series-analysis.html","id":"overview","chapter":"1 Time series analysis","heading":"1.1 Overview","text":"tab demonstrates use several packages time series analysis.\r\nprimarily relies packages tidyverts\r\nfamily, also use RECON trending\r\npackage fit models appropriate infectious disease epidemiology.Time series dataDescriptive analysisFitting regressionsRelation two time seriesOutbreak detectionInterrupted time series","code":""},{"path":"time-series-analysis.html","id":"preparation","chapter":"1 Time series analysis","heading":"1.2 Preparation","text":"","code":""},{"path":"time-series-analysis.html","id":"packages","chapter":"1 Time series analysis","heading":"1.2.1 Packages","text":"code chunk shows loading packages required analyses.","code":"\npacman::p_load(rio,          # File import\n               here,         # File locator\n               tidyverse,    # data management + ggplot2 graphics\n               tsibble,      # handle time series datasets\n               slider,       # for calculating moving averages\n               imputeTS,     # for filling in missing values\n               feasts,       # for time series decomposition and autocorrelation\n               forecast,     # fit sin and cosin terms to data (note: must load after feasts)\n               trending,     # fit and assess models \n               tmaptools,    # for getting geocoordinates (lon/lat) based on place names\n               ecmwfr,       # for interacting with copernicus sateliate CDS API\n               stars,        # for reading in .nc (climate data) files\n               units,        # for defining units of measurement (climate data)\n               yardstick,    # for looking at model accuracy\n               surveillance  # for aberration detection\n               )"},{"path":"time-series-analysis.html","id":"load-data","chapter":"1 Time series analysis","heading":"1.2.2 Load data","text":"example dataset used section:Weekly counts campylobacter cases reported Germany 2001 2011.dataset reduced version dataset available surveillance package.\r\n(details load surveillance package see ?campyDE)dataset imported using import() function rio package. See page importing data various ways import data.first 10 rows counts displayed .","code":"\n# import the linelist\ncounts <- rio::import(\"campylobacter_germany.xlsx\")"},{"path":"time-series-analysis.html","id":"clean-data","chapter":"1 Time series analysis","heading":"1.2.3 Clean data","text":"makes sure date column appropriate format.\r\ntab using tsibble package yearweek\r\nfunction used create calendar week variable. several \r\nways (see handling dates page details), however \r\ntime series best keep within one framework.","code":"\n## ensure the date column is in the appropriate format\ncounts$date <- as.Date(counts$date)\n\n## create a calendar week variable \n## fitting ISO definitons of weeks starting on a monday\ncounts <- counts %>% \n     mutate(epiweek = yearweek(date, week_start = 1))"},{"path":"time-series-analysis.html","id":"download-climate-data","chapter":"1 Time series analysis","heading":"1.2.4 Download climate data","text":"relation two time series section tab, comparing\r\ncampylobacter case counts climate data.Climate data anywhere world can downloaded EU’s Copernicus\r\nSatellite. exact measurements, based model (similar \r\ninterpolation), however benefit global hourly coverage well forecasts.using ecmwfr package pull data Copernicus\r\nclimate data store. need create free account order \r\nwork. package website useful walkthrough\r\n. example code go , \r\nappropriate API keys. replace X’s account\r\nIDs.\r\nneed download one year data time otherwise server times-.sure coordinates location want download data\r\n, can use tmaptools package pull coordinates open street\r\nmaps. alternative option photon\r\npackage, however released CRAN yet; nice thing \r\nphoton provides contextual data several\r\nmatches search.","code":"\n## retrieve location coordinates\ncoords <- geocode_OSM(\"Germany\", geometry = \"point\")\n\n## pull together long/lats in format for ERA-5 querying (bounding box) \n## (as just want a single point can repeat coords)\nrequest_coords <- str_glue_data(coords$coords, \"{y}/{x}/{y}/{x}\")\n\n\n## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)\n## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app\n## https://github.com/bluegreen-labs/ecmwfr\n\n## set up key for weather data \nwf_set_key(user = \"XXXXX\",\n           key = \"XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX\",\n           service = \"cds\") \n\n## run for each year of interest (otherwise server times out)\nfor (i in 2002:2011) {\n  \n  ## pull together a query \n  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax\n  ## change request to a list using addin button above (python to list)\n  ## Target is the name of the output file!!\n  request <- request <- list(\n    product_type = \"reanalysis\",\n    format = \"netcdf\",\n    variable = c(\"2m_temperature\", \"total_precipitation\"),\n    year = c(i),\n    month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n    day = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n            \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\",\n            \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\"),\n    time = c(\"00:00\", \"01:00\", \"02:00\", \"03:00\", \"04:00\", \"05:00\", \"06:00\", \"07:00\",\n             \"08:00\", \"09:00\", \"10:00\", \"11:00\", \"12:00\", \"13:00\", \"14:00\", \"15:00\",\n             \"16:00\", \"17:00\", \"18:00\", \"19:00\", \"20:00\", \"21:00\", \"22:00\", \"23:00\"),\n    area = request_coords,\n    dataset_short_name = \"reanalysis-era5-single-levels\",\n    target = paste0(\"germany_weather\", i, \".nc\")\n  )\n  \n  ## download the file and store it in the current working directory\n  file <- wf_request(user     = \"XXXXX\",  # user ID (for authentication)\n                     request  = request,  # the request\n                     transfer = TRUE,     # download the file\n                     path     = here::here(\"data\", \"Weather\")) ## path to save the data\n  }"},{"path":"time-series-analysis.html","id":"load-climate-data","chapter":"1 Time series analysis","heading":"1.2.5 Load climate data","text":"","code":"\n## define path to weather folder \nfile_paths <- list.files(\n  here::here(\"data\", \"weather\"), \n  full.names = TRUE)\n\n## only keep those with the current name of interest \nfile_paths <- file_paths[str_detect(file_paths, \"germany\")]\n\n\n## read in as a stars object \ndata <- stars::read_stars(file_paths)## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp, \r\n## t2m, tp,\n## change to a data frame \ntemp_data <- as_tibble(data) %>% \n  ## add in variables and correct units\n  mutate(\n    ## create an calendar week variable \n    epiweek = tsibble::yearweek(time), \n    ## create a date variable (start of calendar week)\n    date = as.Date(epiweek),\n    ## change temperature from kelvin to celsius\n    t2m = set_units(t2m, celsius), \n    ## change precipitation from metres to millimetres \n    tp  = set_units(tp, mm)) %>% \n  ## group by week (keep the date too though)\n  group_by(epiweek, date) %>% \n  ## get the average per week\n  summarise(t2m = as.numeric(mean(t2m)), \n            tp = as.numeric(mean(tp)))"},{"path":"time-series-analysis.html","id":"time-series-data","chapter":"1 Time series analysis","heading":"1.3 Time series data","text":"number different packages structuring handling time series\r\ndata. said, focus tidyverts family packages \r\nuse tsibble package define time series object. data set\r\ndefined time series object means much easier structure analysis.use tsibble function specify “index”, .e. variable\r\nspecifying time unit interest. case epiweek variable.data set weekly counts province, example, also\r\nable specify grouping variable using “key” argument.\r\nallow us analysis group.Looking class(counts) tells top tidy data frame\r\n(“tbl_df”, “tbl”, “data.frame”), additional properties time series\r\ndata frame (“tbl_ts”).can take quick look data using ggplot2. see plot \r\nclear seasonal pattern, missings. However, \r\nseems issue reporting beginning year; cases drop\r\nlast week year increase first week next year.DANGER: datasets aren’t clean example.\r\nneed check duplicates missings . ","code":"\n## define time series object \ncounts <- tsibble(counts, index = epiweek)\n## plot a line graph of cases by week\nggplot(counts, aes(x = epiweek, y = case)) + \n     geom_line()"},{"path":"time-series-analysis.html","id":"duplicates","chapter":"1 Time series analysis","heading":"1.3.1 duplicates","text":"tsibble allow duplicate observations. row need \r\nunique, unique within group (key variable).\r\npackage functions help identify duplicates. include\r\nare_duplicated gives TRUE/FALSE vector whether row \r\nduplicate, duplicates gives data frame duplicated rows.See de-duplication page details select rows want.","code":"\n## get a vector of TRUE/FALSE whether rows are duplicates\nare_duplicated(counts, index = epiweek) \n\n## get a data frame of any duplicated rows \nduplicates(counts, index = epiweek) "},{"path":"time-series-analysis.html","id":"missings","chapter":"1 Time series analysis","heading":"1.3.2 Missings","text":"saw brief inspection missings, also\r\nsaw seems problem reporting delay around new year.\r\nOne way address problem set values missing \r\nimpute values. simplest form time series imputation draw\r\nstraight line last non-missing next non-missing value.\r\nuse imputeTS package function na_interpolation.\r\nSee missing data page options imputation.\r\nAnother alternative calculating moving average, try smooth\r\napparent reporting issues (see next section).","code":"\n## create a variable with missings instead of weeks with reporting issues\ncounts <- counts %>% \n     mutate(case_miss = if_else(\n          ## if epiweek contains 52, 53, 1 or 2\n          str_detect(epiweek, \"W51|W52|W53|W01|W02\"), \n          ## then set to missing \n          NA_real_, \n          ## otherwise keep the value in case\n          case\n     ))\n\n## alternatively interpolate missings by linear trend \n## between two nearest adjacent points\ncounts <- counts %>% \n  mutate(case_int = na_interpolation(case_miss)\n         )\n\n## to check what values have been imputed compared to the original\nggplot_na_imputations(counts$case_miss, counts$case_int) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()"},{"path":"time-series-analysis.html","id":"descriptive-analysis","chapter":"1 Time series analysis","heading":"1.4 Descriptive analysis","text":"","code":""},{"path":"time-series-analysis.html","id":"moving-averages","chapter":"1 Time series analysis","heading":"1.4.1 Moving averages","text":"data noisy (counts jumping ) can helpful \r\ncalculate moving average. example , week calculate \r\naverage number cases four previous weeks. smooths data, \r\nmake interpretable. case really add much, \r\nstick interpolated data analyis.\r\nSee moving averages page detail.","code":"\n## create a moving average variable (deals with missings)\ncounts <- counts %>% \n     ## create the ma_4w variable \n     ## slide over each row of the case variable\n     mutate(ma_4wk = slide_dbl(case, \n                               ## for each row calculate the name\n                               ~ mean(.x, na.rm = TRUE),\n                               ## use the four previous weeks\n                               .before = 4))\n\n## make a quick visualisation of the difference \nggplot(counts, aes(x = epiweek)) + \n     geom_line(aes(y = case)) + \n     geom_line(aes(y = ma_4wk), colour = \"red\")"},{"path":"time-series-analysis.html","id":"periodicity","chapter":"1 Time series analysis","heading":"1.4.2 Periodicity","text":"NOTE: possible use weeks add sin cosine terms, however use function generate terms (see regression section ) ","code":"\n## x is a dataset\n## counts is variable with count data or rates within x \n## start_week is the first week in your dataset\n## period is how many units in a year \n## output is whether you want return spectral periodogram or the peak weeks\n  ## \"periodogram\" or \"weeks\"\nperiodogram <- function(x, \n                        counts, \n                        start_week = c(2002, 1), \n                        period = 52, \n                        output = \"weeks\") {\n  \n\n    ## make sure is not a tsibble, filter to project and only keep columns of interest\n    prepare_data <- dplyr::as_tibble(x)\n    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]\n    prepare_data <- dplyr::select(prepare_data, {{counts}})\n    \n    ## create an intermediate \"zoo\" time series to be able to use with spec.pgram\n    zoo_cases <- zoo::zooreg(prepare_data, \n                             start = start_week, frequency = period)\n    \n    ## get a spectral periodogram not using fast fourier transform \n    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)\n    \n    ## return the peak weeks \n    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period\n    \n    if (output == \"weeks\") {\n      periodo_weeks\n    } else {\n      periodo\n    }\n    \n}\n\n## get spectral periodogram for extracting weeks with the highest frequencies \n## (checking of seasonality) \nperiodo <- periodogram(counts, \n                       case_int, \n                       start_week = c(2002, 1),\n                       output = \"periodogram\")\n\n## pull spectrum and frequence in to a dataframe for plotting\nperiodo <- data.frame(periodo$freq, periodo$spec)\n\n## plot a periodogram showing the most frequently occuring periodicity \nggplot(data = periodo, \n                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + \n  geom_line() + \n  labs(x = \"Period (Weeks)\", y = \"Log(density)\")\n## get a vector weeks in ascending order \npeak_weeks <- periodogram(counts, \n                          case_int, \n                          start_week = c(2002, 1), \n                          output = \"weeks\")"},{"path":"time-series-analysis.html","id":"decomposition","chapter":"1 Time series analysis","heading":"1.4.3 Decomposition","text":"Classical decomposition used break time series several parts, \r\ntaken together make pattern see.\r\ndifferent parts trend-cycle (long-term direction data),\r\nseasonality (repeating patterns) random (left removing\r\ntrend season).","code":"\n## decompose the counts dataset \ncounts %>% \n  # using an additive classical decomposition model\n  model(classical_decomposition(case_int, type = \"additive\")) %>% \n  ## extract the important information from the model\n  components() %>% \n  ## generate a plot \n  autoplot()## Warning: Removed 26 row(s) containing missing values (geom_path)."},{"path":"time-series-analysis.html","id":"autocorrelation","chapter":"1 Time series analysis","heading":"1.4.4 Autocorrelation","text":"Autocorrelation tells relation counts week\r\nweeks (called lags).\r\nUsing ACF function, can produce plot shows us number lines\r\nrelation different lags. lag 0 (x = 0), line \r\nalways 1 shows relation observation (shown ).\r\nfirst line shown (x = 1) shows relation observation\r\nobservation (lag 1), second shows relation \r\nobservation observation last (lag 2) lag \r\n52 shows relation observation observation 1\r\nyear (52 weeks ).\r\nUsing PACF function (partial autocorrelation shows type relation\r\nadjusted weeks . less informative determining\r\nperiodicity.can formally test null hypothesis independence time series (.e. \r\nautocorrelated) using Ljung-Box test (stats package).\r\nsignificant p-value suggests autocorrelation data.","code":"\n## using the counts dataset\ncounts %>% \n  ## calculate autocorrelation using a full years worth of lags\n  ACF(case_int, lag_max = 52) %>% \n  ## show a plot\n  autoplot()\n## using the counts data set \ncounts %>% \n  ## calculate the partial autocorrelation using a full years worth of lags\n  PACF(case_int, lag_max = 52) %>% \n  ## show a plot\n  autoplot()\n## test for independance \nBox.test(counts$case_int, type = \"Ljung-Box\")## \r\n##  Box-Ljung test\r\n## \r\n## data:  counts$case_int\r\n## X-squared = 472.95, df = 1, p-value < 2.2e-16"},{"path":"time-series-analysis.html","id":"fitting-regressions","chapter":"1 Time series analysis","heading":"1.5 Fitting regressions","text":"possible fit large number different regressions time series,\r\nhowever, demonstrate fit negative binomial regression - \r\noften appropriate counts data infectious diseases.","code":""},{"path":"time-series-analysis.html","id":"fourier-terms","chapter":"1 Time series analysis","heading":"1.5.1 Fourier terms","text":"Fourier terms equivalent sin cosin curves. difference \r\nfit based finding appropriate combination curves explain\r\ndata.\r\nfitting one fourier term, equivalent fitting sin\r\ncosin frequently occurring lag seen periodogram (\r\ncase 52 weeks). use fourier function forecast package.\r\ncode assign using $, fourier returns two columns (one\r\nsin one cosin) added dataset list, called\r\n“fourier” - list can used normal variable regression.","code":"\n## add in fourier terms using the epiweek and case_int variabless\ncounts$fourier <- select(counts, epiweek, case_int) %>% \n  fourier(K = 1)"},{"path":"time-series-analysis.html","id":"negative-binomial","chapter":"1 Time series analysis","heading":"1.5.2 Negative binomial","text":"possible fit regressions using base stats MASS\r\nfunctions (e.g. lm, glm glm.nb). However using \r\ntrending package, allows calculating appropriate confidence\r\nprediction intervals (otherwise available).\r\nsyntax , specify outcome variable tilda (~)\r\nadd various exposure variables interest separated plus (+).difference first define model fit \r\ndata. useful allows comparing multiple different models\r\nsyntax.TIP: wanted use rates, rather \r\ncounts include population variable logarithmic offset term, adding\r\noffset(log(population). need set population 1, \r\nusing predict order produce rate. TIP: fitting complex models \r\nARIMA prophet, see fable package.","code":"\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier)\n\n## fit your model using the counts dataset\nfitted_model <- fit(model, counts)\n\n## calculate confidence intervals and prediction intervals \nobserved <- predict(fitted_model)\n\n## plot your regression \nggplot(data = observed, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"Red\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()"},{"path":"time-series-analysis.html","id":"residuals","chapter":"1 Time series analysis","heading":"1.5.3 Residuals","text":"see well model fits observed data need look residuals.\r\nresiduals difference observed counts counts\r\nestimated model. calculate simply using case_int - estimate,\r\nresiduals function extracts directly regression us.see , explaining variation\r\nmodel. might fit fourier terms,\r\naddress amplitude. However example leave .\r\nplots show model worse peaks troughs (counts \r\nhighest lowest) might likely underestimate\r\nobserved counts.","code":"\n## calculate the residuals \nobserved <- observed %>% \n  mutate(resid = residuals(fitted_model$fitted_model, type = \"response\"))\n\n## are the residuals fairly constant over time (if not: outbreaks? change in practice?)\nobserved %>%\n  ggplot(aes(x = epiweek, y = resid)) +\n  geom_line() +\n  geom_point() + \n  labs(x = \"epiweek\", y = \"Residuals\")\n## is there autocorelation in the residuals (is there a pattern to the error?)  \nobserved %>% \n  as_tsibble(index = epiweek) %>% \n  ACF(resid, lag_max = 52) %>% \n  autoplot()\n## are residuals normally distributed (are under or over estimating?)  \nobserved %>%\n  ggplot(aes(x = resid)) +\n  geom_histogram(binwidth = 100) +\n  geom_rug() +\n  labs(y = \"count\") \n## compare observed counts to their residuals \n  ## should also be no pattern \nobserved %>%\n  ggplot(aes(x = estimate, y = resid)) +\n  geom_point() +\n  labs(x = \"Fitted\", y = \"Residuals\")\n## formally test autocorrelation of the residuals\n## H0 is that residuals are from a white-noise series (i.e. random)\n## test for independence \n## if p value significant then non-random\nBox.test(observed$resid, type = \"Ljung-Box\")## \r\n##  Box-Ljung test\r\n## \r\n## data:  observed$resid\r\n## X-squared = 356.55, df = 1, p-value < 2.2e-16"},{"path":"time-series-analysis.html","id":"relation-of-two-time-series","chapter":"1 Time series analysis","heading":"1.6 Relation of two time series","text":"look using weather data (specifically temperature) explain\r\ncampylobacter case counts.","code":""},{"path":"time-series-analysis.html","id":"merging-datasets","chapter":"1 Time series analysis","heading":"1.6.1 Merging datasets","text":"can join datasets using week variable. merging see \r\nhandbook section [joining].","code":"\n## left join so that we only have the rows already existing in counts\n## drop the date variable from temp_data (otherwise is duplicated)\ncounts <- left_join(counts, \n                    select(temp_data, -date),\n                    by = \"epiweek\")"},{"path":"time-series-analysis.html","id":"descriptive-analysis-1","chapter":"1 Time series analysis","heading":"1.6.2 Descriptive analysis","text":"First plot data see obvious relation.\r\nplot shows clear relation seasonality two\r\nvariables, temperature might peak weeks case number.\r\npivoting data, see handbook section [cleaning data].","code":"\ncounts %>% \n  ## keep the variables we are interested \n  select(epiweek, case_int, t2m) %>% \n  ## change your data in to long format\n  pivot_longer(\n    ## use epiweek as your key\n    !epiweek,\n    ## move column names to the new \"measure\" column\n    names_to = \"measure\", \n    ## move cell values to the new \"values\" column\n    values_to = \"value\") %>% \n  ## create a plot with the dataset above\n  ## plot epiweek on the x axis and values (counts/celsius) on the y \n  ggplot(aes(x = epiweek, y = value)) + \n    ## create a separate plot for temperate and case counts \n    ## let them set their own y-axes\n    facet_grid(measure ~ ., scales = \"free_y\") +\n    ## plot both as a line\n    geom_line()"},{"path":"time-series-analysis.html","id":"lags-and-cross-correlation","chapter":"1 Time series analysis","heading":"1.6.3 Lags and cross-correlation","text":"formally test weeks highly related cases temperature.\r\ncan use cross-correlation function (CCF) feasts package.\r\nalso visualise (rather using arrange) using autoplot function.see lag 4 weeks highly correlated,\r\nmake lagged temperature variable include regression.","code":"\ncounts %>% \n  ## calculate cross-correlation between interpolated counts and temperature\n  CCF(case_int, t2m,\n      ## set the maximum lag to be 52 weeks\n      lag_max = 52, \n      ## return the correlation coefficient \n      type = \"correlation\") %>% \n  ## arange in decending order of the correlation coefficient \n  ## show the most associated lags\n  arrange(-ccf) %>% \n  ## only show the top ten \n  slice_head(n = 10)## Warning: Current temporal ordering may yield unexpected results.\r\n## i Suggest to sort by ``, `lag` first.## # A tsibble: 10 x 2 [1W]\r\n##      lag   ccf\r\n##    <lag> <dbl>\r\n##  1    4W 0.750\r\n##  2    5W 0.746\r\n##  3    3W 0.736\r\n##  4    6W 0.731\r\n##  5    2W 0.727\r\n##  6    7W 0.705\r\n##  7    1W 0.694\r\n##  8    8W 0.671\r\n##  9    0W 0.647\r\n## 10  -47W 0.640\ncounts <- counts %>% \n  ## create a new variable for temperature lagged by four weeks\n  mutate(t2m_lag4 = lag(t2m, n = 4))"},{"path":"time-series-analysis.html","id":"negative-binomial-with-two-variables","chapter":"1 Time series analysis","heading":"1.6.4 Negative binomial with two variables","text":"fit negative binomial regression done previously. time add \r\ntemperature variable lagged four weeks.investigate individual terms, can pull original negative binomial\r\nregression trending format using get_model pass \r\nbroom package tidy function retrieve exponentiated estimates associated\r\nconfidence intervals.\r\nshows us lagged temperature, controlling trend seasonality,\r\nsimilar case counts (estimate ~ 1) significantly associated.\r\nsuggests might good variable use predicting future case\r\nnumbers (climate forecasts readily available).quick visual inspection model shows might better job \r\nestimating observed case counts.","code":"\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier + \n    ## use the temperature lagged by four weeks \n    t2m_lag4\n    )\n\n## fit your model using the counts dataset\nfitted_model <- fit(model, counts)\n\n## calculate confidence intervals and prediction intervals \nobserved <- predict(fitted_model)\nfitted_model %>% \n  ## extract original negative binomial regression\n  get_model() %>% \n  ## get a tidy dataframe of results\n  tidy(exponentiate = TRUE, \n       conf.int = TRUE)## Warning: Tidiers for objects of class negbin are not maintained by the broom team, and are only supported through the glmlm tidier method. Please be cautious in interpreting and\r\n## reporting broom output.## # A tibble: 5 x 7\r\n##   term         estimate  std.error statistic  p.value conf.low conf.high\r\n##   <chr>           <dbl>      <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\r\n## 1 (Intercept)   369.    0.105          56.4  0.        300.      453.   \r\n## 2 epiweek         1.00  0.00000747     10.5  9.57e-26    1.00      1.00 \r\n## 3 fourierS1-52    0.753 0.0213        -13.3  1.98e-40    0.723     0.785\r\n## 4 fourierC1-52    0.816 0.0198        -10.3  9.30e-25    0.786     0.848\r\n## 5 t2m_lag4        1.01  0.00267         2.36 1.81e- 2    1.00      1.01\n## plot your regression \nggplot(data = observed, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"Red\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()## Warning: Removed 4 row(s) containing missing values (geom_path)."},{"path":"time-series-analysis.html","id":"residuals-1","chapter":"1 Time series analysis","heading":"1.6.4.1 Residuals","text":"investigate residuals see well model fits observed data.\r\nresults interpretation similar previous regression,\r\nmay feasible stick simpler model without temperature.","code":"\n## calculate the residuals \nobserved <- observed %>% \n  mutate(resid = case_int - estimate)\n\n## are the residuals fairly constant over time (if not: outbreaks? change in practice?)\nobserved %>%\n  ggplot(aes(x = epiweek, y = resid)) +\n  geom_line() +\n  geom_point() + \n  labs(x = \"epiweek\", y = \"Residuals\")## Warning: Removed 4 row(s) containing missing values (geom_path).## Warning: Removed 4 rows containing missing values (geom_point).\n## is there autocorelation in the residuals (is there a pattern to the error?)  \nobserved %>% \n  as_tsibble(index = epiweek) %>% \n  ACF(resid, lag_max = 52) %>% \n  autoplot()\n## are residuals normally distributed (are under or over estimating?)  \nobserved %>%\n  ggplot(aes(x = resid)) +\n  geom_histogram(binwidth = 100) +\n  geom_rug() +\n  labs(y = \"count\") ## Warning: Removed 4 rows containing non-finite values (stat_bin).\n## compare observed counts to their residuals \n  ## should also be no pattern \nobserved %>%\n  ggplot(aes(x = estimate, y = resid)) +\n  geom_point() +\n  labs(x = \"Fitted\", y = \"Residuals\")## Warning: Removed 4 rows containing missing values (geom_point).\n## formally test autocorrelation of the residuals\n## H0 is that residuals are from a white-noise series (i.e. random)\n## test for independence \n## if p value significant then non-random\nBox.test(observed$resid, type = \"Ljung-Box\")## \r\n##  Box-Ljung test\r\n## \r\n## data:  observed$resid\r\n## X-squared = 349.57, df = 1, p-value < 2.2e-16"},{"path":"time-series-analysis.html","id":"outbreak-detection","chapter":"1 Time series analysis","heading":"1.7 Outbreak detection","text":"demonstrate two (similar) methods detecting outbreaks .\r\nfirst builds sections .\r\nuse trending package fit regressions previous years, \r\npredict expect see following year. observed counts \r\nexpect, suggest outbreak.\r\nsecond method based similar principles uses surveillance package,\r\nnumber different algorithms aberration detection.CAUTION: Normally, interested current year (know counts present week). example pretending week 52 2011.","code":""},{"path":"time-series-analysis.html","id":"trending","chapter":"1 Time series analysis","heading":"1.7.1 trending","text":"method define baseline (usually 5 years data).\r\nfit regression baseline data, use predict estimates\r\nnext year.","code":""},{"path":"time-series-analysis.html","id":"cut-off-date","chapter":"1 Time series analysis","heading":"1.7.1.1 Cut-off date","text":"easier define dates one place use throughout \r\nrest code.\r\ndefine start date (observations started) cut-date\r\n(end baseline period - period want predict starts).\r\nalso define many weeks year interest (one going \r\npredicting).","code":"\n## define start date (when observations began)\nstart_date <- min(counts$epiweek)\n\n## define a cut-off week (end of baseline, start of prediction period)\ncut_off <- yearweek(\"2010-12-31\")\n\n## get the year which want to predict for \n## add one week to cut_off (to push in to next year) and change to only have Year\npred_year <- format(cut_off + 1, format = \"%Y\") %>% \n  ## change to numeric\n  as.numeric()\n\n## find how many weeks in year of interest\nnum_weeks <- ifelse(\n  ## true/false of whether pred_year is a 53 week year\n  is_53weeks(pred_year),\n  ## if true then return 53 \n  53, \n  ## otherwise return 52\n  52)"},{"path":"time-series-analysis.html","id":"fourier-terms-1","chapter":"1 Time series analysis","heading":"1.7.1.2 Fourier terms","text":"need redefine fourier terms - want fit baseline\r\ndate predict (extrapolate) terms next year.\r\nneed combine two output lists fourier function together;\r\nfirst one baseline data, second one predicts \r\nyear interest (defining h argument).\r\nN.b. bind rows use rbind (rather tidyverse bind_rows) \r\nfourier columns list (named individually).","code":"\n## define fourier terms (sincos) \ncounts <- counts %>% \n  mutate(\n    ## combine fourier terms for weeks prior to  and after 2010 cut-off date\n    ## (nb. 2011 fourier terms are predicted)\n    fourier = rbind(\n      ## get fourier terms for previous years\n      fourier(\n        ## only keep the rows before 2011\n        filter(counts, \n               epiweek <= cut_off), \n        ## include one set of sin cos terms \n        K = 1\n        ), \n      ## predict the fourier terms for 2011 (using baseline data)\n      fourier(\n        ## only keep the rows before 2011\n        filter(counts, \n               epiweek <= cut_off),\n        ## include one set of sin cos terms \n        K = 1, \n        ## predict 52 weeks ahead\n        h = num_weeks\n        )\n      )\n    )"},{"path":"time-series-analysis.html","id":"split-data-and-fit-regression","chapter":"1 Time series analysis","heading":"1.7.1.3 Split data and fit regression","text":"now split dataset baseline period prediction\r\nperiod. done using dplyr group_split function group_by,\r\ncreate list two data frames, one cut-one\r\n.\r\nuse purrr package pluck function pull datasets \r\nlist (equivalent using square brackets, e.g. dat[[1]]), can fit\r\nmodel baseline data, use predict function data\r\ninterest cut-.previously, can visualise model ggplot. highlight alerts \r\nred dots observed counts 95% prediction interval.\r\ntime also add vertical line label forecast starts.","code":"\n# split data for fitting and prediction\ndat <- counts %>% \n  group_by(epiweek <= cut_off) %>%\n  group_split()\n\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier\n)\n\n# define which data to use for fitting and which for predicting\nfitting_data <- pluck(dat, 2)\npred_data <- pluck(dat, 1) %>% \n  select(case_int, epiweek, fourier)\n\n# fit model \nfitted_model <- fit(model, fitting_data)\n\n# get confint and estimates for fitted data\nobserved <- fitted_model %>% \n  predict()\n\n# forecast with data want to predict with \nforecasts <- fitted_model %>% \n  predict(pred_data)\n\n## combine baseline and predicted datasets\nobserved <- bind_rows(observed, forecasts)\n## plot your regression \nggplot(data = observed, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"grey\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## plot in points for the observed counts above expected\n  geom_point(\n    data = filter(observed, case_int > upper_pi), \n    aes(y = case_int), \n    colour = \"red\", \n    size = 2) + \n  ## add vertical line and label to show where forecasting started\n  geom_vline(\n           xintercept = as.Date(cut_off), \n           linetype = \"dashed\") + \n  annotate(geom = \"text\", \n           label = \"Forecast\", \n           x = cut_off, \n           y = max(observed$upper_pi), \n           angle = 90, \n           vjust = 1\n           ) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()"},{"path":"time-series-analysis.html","id":"prediction-validation","chapter":"1 Time series analysis","heading":"1.7.1.4 prediction validation","text":"Beyond inspecting residuals, important investigate good model \r\npredicting cases future. gives idea reliable \r\nthreshold alerts .\r\ntraditional way validating see well can predict latest\r\nyear present one (don’t yet know counts “current year”).\r\nexample data set use data 2002 2009 predict 2010,\r\nsee accurate predictions . refit model include\r\n2010 data use predict 2011 counts.\r\ncan seen figure Hyndman et al “Forecasting principles\r\npractice”.downside using data available , \r\nfinal model using prediction.alternative use method called cross-validation. scenario \r\nroll data available fit multiple models predict one year ahead.\r\nuse data model, seen figure \r\nHyndman et al text.\r\nexample, first model uses 2002 predict 2003, second uses 2002 \r\n2003 predict 2004, .\r\nuse purrr package map function loop dataset.\r\nput estimates one data set merge original case counts,\r\nuse yardstick package compute measures accuracy.\r\ncompute four measures including: Root mean squared error (RMSE), Mean absolute error\r\n(MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).","code":"\n## Cross validation: predicting week(s) ahead based on sliding window\n\n## expand your data by rolling over in 52 week windows (before + after) \n## to predict 52 week ahead\n## (creates longer and longer chains of observations - keeps older data)\n\n## define window want to roll over\nroll_window <- 52\n\n## define weeks ahead want to predict \nweeks_ahead <- 52\n\n## create a data set of repeating, increasingly long data\n## label each data set with a unique id\n## only use cases before year of interest (i.e. 2011)\ncase_roll <- counts %>% \n  filter(epiweek < cut_off) %>% \n  ## only keep the week and case counts variables\n  select(epiweek, case_int) %>% \n    ## drop the last x observations \n    ## depending on how many weeks ahead forecasting \n    ## (otherwise will be an actual forecast to \"unknown\")\n    slice(1:(n() - weeks_ahead)) %>%\n    as_tsibble(index = epiweek) %>% \n    ## roll over each week in x after windows to create grouping ID \n    ## depending on what rolling window specify\n    stretch_tsibble(.init = roll_window, .step = 1) %>% \n  ## drop the first couple - as have no \"before\" cases\n  filter(.id > roll_window)\n\n\n## for each of the unique data sets run the code below\nforecasts <- purrr::map(unique(case_roll$.id), \n                        function(i) {\n  \n  ## only keep the current fold being fit \n  mini_data <- filter(case_roll, .id == i) %>% \n    as_tibble()\n  \n  ## create an empty data set for forecasting on \n  forecast_data <- tibble(\n    epiweek = seq(max(mini_data$epiweek) + 1,\n                  max(mini_data$epiweek) + weeks_ahead,\n                  by = 1),\n    case_int = rep.int(NA, weeks_ahead),\n    .id = rep.int(i, weeks_ahead)\n  )\n  \n  ## add the forecast data to the original \n  mini_data <- bind_rows(mini_data, forecast_data)\n  \n  ## define the cut off based on latest non missing count data \n  cv_cut_off <- mini_data %>% \n    ## only keep non-missing rows\n    filter(!is.na(case_int)) %>% \n    ## get the latest week\n    summarise(max(epiweek)) %>% \n    ## extract so is not in a dataframe\n    pull()\n  \n  ## make mini_data back in to a tsibble\n  mini_data <- tsibble(mini_data, index = epiweek)\n  \n  ## define fourier terms (sincos) \n  mini_data <- mini_data %>% \n    mutate(\n    ## combine fourier terms for weeks prior to  and after cut-off date\n    fourier = rbind(\n      ## get fourier terms for previous years\n      forecast::fourier(\n        ## only keep the rows before cut-off\n        filter(mini_data, \n               epiweek <= cv_cut_off), \n        ## include one set of sin cos terms \n        K = 1\n        ), \n      ## predict the fourier terms for following year (using baseline data)\n      fourier(\n        ## only keep the rows before cut-off\n        filter(mini_data, \n               epiweek <= cv_cut_off),\n        ## include one set of sin cos terms \n        K = 1, \n        ## predict 52 weeks ahead\n        h = weeks_ahead\n        )\n      )\n    )\n  \n  \n  # split data for fitting and prediction\n  dat <- mini_data %>% \n    group_by(epiweek <= cv_cut_off) %>%\n    group_split()\n\n  ## define the model you want to fit (negative binomial) \n  model <- glm_nb_model(\n    ## set number of cases as outcome of interest\n    case_int ~\n      ## use epiweek to account for the trend\n      epiweek +\n      ## use the furier terms to account for seasonality\n      fourier\n  )\n\n  # define which data to use for fitting and which for predicting\n  fitting_data <- pluck(dat, 2)\n  pred_data <- pluck(dat, 1)\n  \n  # fit model \n  fitted_model <- fit(model, fitting_data)\n  \n  # forecast with data want to predict with \n  forecasts <- fitted_model %>% \n    predict(pred_data) %>% \n    ## only keep the week and the forecast estimate\n    select(epiweek, estimate)\n    \n  }\n  )\n\n## make the list in to a data frame with all the forecasts\nforecasts <- bind_rows(forecasts)\n\n## join the forecasts with the observed\nforecasts <- left_join(forecasts, \n                       select(counts, epiweek, case_int),\n                       by = \"epiweek\")\n\n## using {yardstick} compute metrics\n  ## RMSE: Root mean squared error\n  ## MAE:  Mean absolute error  \n  ## MASE: Mean absolute scaled error\n  ## MAPE: Mean absolute percent error\nmodel_metrics <- bind_rows(\n  ## in your forcasted dataset compare the observed to the predicted\n  rmse(forecasts, case_int, estimate), \n  mae( forecasts, case_int, estimate),\n  mase(forecasts, case_int, estimate),\n  mape(forecasts, case_int, estimate),\n  ) %>% \n  ## only keep the metric type and its output\n  select(Metric  = .metric, \n         Measure = .estimate) %>% \n  ## make in to wide format so can bind rows after\n  pivot_wider(names_from = Metric, values_from = Measure)\n\n## return model metrics \nmodel_metrics## # A tibble: 1 x 4\r\n##    rmse   mae  mase  mape\r\n##   <dbl> <dbl> <dbl> <dbl>\r\n## 1  252.  199.  1.96  17.3"},{"path":"time-series-analysis.html","id":"surveillance","chapter":"1 Time series analysis","heading":"1.7.2 surveillance","text":"section use surveillance package create alert thresholds\r\nbased outbreak detection algorithms. several different methods\r\navailable package, however focus two options .\r\ndetails, see papers application\r\ntheory\r\nalogirthms used.first option uses improved Farrington method. fits negative\r\nbinomial glm (including trend) -weights past outbreaks (outliers) \r\ncreate threshold level.second option use glrnb method. also fits negative binomial glm\r\nincludes trend fourier terms (favoured ). regression used\r\ncalculate “control mean” (~fitted values) - uses computed\r\ngeneralized likelihood ratio statistic assess shift mean\r\nweek. Note threshold week takes account previous\r\nweeks sustained shift alarm triggered.\r\n(Also note alarm algorithm reset)order work surveillance package, first need define \r\n“surveillance time series” object (using sts function) fit within \r\nframework.","code":"\n## define surveillance time series object\n## nb. you can include a denominator with the population object (see ?sts)\ncounts_sts <- sts(observed = counts$case_int,\n                  start = c(\n                    ## subset to only keep the year from start_date \n                    as.numeric(str_sub(start_date, 1, 4)), \n                    ## subset to only keep the week from start_date\n                    as.numeric(str_sub(start_date, 7, 8))), \n                  ## define the type of data (in this case weekly)\n                  freq = 52)\n\n## define the week range that you want to include (ie. prediction period)\n## nb. the sts object only counts observations without assigning a week or \n## year identifier to them - so we use our data to define the appropriate observations\nweekrange <- cut_off - start_date"},{"path":"time-series-analysis.html","id":"farrington-method","chapter":"1 Time series analysis","heading":"1.7.2.1 Farrington method","text":"define parameters Farrington method list.\r\nrun algorithm using farringtonFlexible can extract \r\nthreshold alert using farringtonmethod@upperboundto include \r\ndataset. also possible extract TRUE/FALSE week triggered\r\nalert (threshold) using farringtonmethod@alarm.can visualise results ggplot done previously.","code":"\n## define control\nctrl <- list(\n  ## define what time period that want threshold for (i.e. 2011)\n  range = which(counts_sts@epoch > weekrange),\n  b = 9, ## how many years backwards for baseline\n  w = 2, ## rolling window size in weeks\n  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)\n  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)\n  trend = TRUE,\n  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)\n  thresholdMethod = \"nbPlugin\",\n  populationOffset = TRUE\n  )\n\n## apply farrington flexible method\nfarringtonmethod <- farringtonFlexible(counts_sts, ctrl)\n\n## create a new variable in the original dataset called threshold\n## containing the upper bound from farrington \n## nb. this is only for the weeks in 2011 (so need to subset rows)\ncounts[which(counts$epiweek >= cut_off),\n              \"threshold\"] <- farringtonmethod@upperbound\nggplot(counts, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in upper bound of aberration algorithm\n  geom_line(aes(y = threshold, colour = \"Alert threshold\"), \n            linetype = \"dashed\", \n            size = 1.5) +\n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Alert threshold\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic() + \n  ## remove title of legend \n  theme(legend.title = element_blank())## Warning: Removed 469 row(s) containing missing values (geom_path)."},{"path":"time-series-analysis.html","id":"glrnb-method","chapter":"1 Time series analysis","heading":"1.7.2.2 GLRNB method","text":"Similarly GLRNB method define parameters list,\r\nfit algorithm extract upper bounds.CAUTION: method uses “brute force” (similar bootstrapping) calculating thresholds, can take long time!See GLRNB vignette\r\ndetails.Visualise outputs previously.","code":"\n## define control options\nctrl <- list(\n  ## define what time period that want threshold for (i.e. 2020)\n  range = which(counts_sts@epoch > weekrange),\n  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include\n  trend = TRUE,   ## whether to include trend or not\n  refit = FALSE), ## whether to refit model after each alarm\n  ## cARL = threshold for GLR statistic (arbitrary)\n     ## 3 ~ middle ground for minimising false positives\n     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)\n   c.ARL = 2,\n   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak\n   ret = \"cases\"     ## return threshold upperbound as case counts\n  )\n\n## apply the glrnb method\nglrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)## Return of cases is for the GLR detector based on the negative binomial distribution is currently\r\n##  only implemented brute force and hence might be very slow!\r\n## Return of cases is for the GLR detector based on the negative binomial distribution is currently\r\n##  only implemented brute force and hence might be very slow!\r\n## Return of cases is for the GLR detector based on the negative binomial distribution is currently\r\n##  only implemented brute force and hence might be very slow!\r\n## Return of cases is for the GLR detector based on the negative binomial distribution is currently\r\n##  only implemented brute force and hence might be very slow!\r\n## Return of cases is for the GLR detector based on the negative binomial distribution is currently\r\n##  only implemented brute force and hence might be very slow!\r\n## Return of cases is for the GLR detector based on the negative binomial distribution is currently\r\n##  only implemented brute force and hence might be very slow!\r\n## Return of cases is for the GLR detector based on the negative binomial distribution is currently\r\n##  only implemented brute force and hence might be very slow!\n## create a new variable in the original dataset called threshold\n## containing the upper bound from glrnb \n## nb. this is only for the weeks in 2011 (so need to subset rows)\ncounts[which(counts$epiweek >= cut_off),\n              \"threshold_glrnb\"] <- glrnbmethod@upperbound\nggplot(counts, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in upper bound of aberration algorithm\n  geom_line(aes(y = threshold_glrnb, colour = \"Alert threshold\"), \n            linetype = \"dashed\", \n            size = 1.5) +\n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Alert threshold\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic() + \n  ## remove title of legend \n  theme(legend.title = element_blank())## Warning: Removed 469 row(s) containing missing values (geom_path)."},{"path":"time-series-analysis.html","id":"interrupted-timeseries","chapter":"1 Time series analysis","heading":"1.8 Interrupted timeseries","text":"Interrupted timeseries (also called segmented regression intervention analysis),\r\noften used assessing impact vaccines incidence disease.\r\ncan used assessing impact wide range interventions introductions.\r\nexample changes hospital procedures introduction new disease\r\nstrain population.\r\nexample pretend new strain Campylobacter introduced\r\nGermany end 2008, see affects number cases.\r\nuse negative binomial regression . regression time \r\nsplit two parts, one intervention (introduction new strain )\r\none (pre post-periods). allows us calculate incidence rate ratio comparing \r\ntwo time periods. Explaining equation might make clearer (just\r\nignore!).negative binomial regression can defined follows:\\[\\log(Y_t)= β_0 + β_1 \\times t+ β_2 \\times δ(t-t_0) + β_3\\times(t-t_0 )^+ + log(pop_t) + e_t\\]:\r\n\\(Y_t\\)number cases observed time \\(t\\)\\(pop_t\\) population size 100,000s time \\(t\\) (used )\\(t_0\\) last year pre-period (including transition time )\\(δ(x\\) indicator function (0 x≤0 1 x>0)\\((x)^+\\) cut operator (x x>0 0 otherwise)\\(e_t\\) denotes residual\r\nAdditional terms trend season can added needed.\\(β_2 \\times δ(t-t_0) + β_3\\times(t-t_0 )^+\\) generalised linear\r\npart post-period zero pre-period.\r\nmeans \\(β_2\\) \\(β_3\\) estimates effects intervention.need re-calculate fourier terms without forecasting , use\r\ndata available us (.e. retrospectively). Additionally need calculate\r\nextra terms needed regression.use terms fit negative binomial regression, produce \r\ntable percentage change. example shows \r\nsignificant change.previously can visualise outputs regression.","code":"\n## add in fourier terms using the epiweek and case_int variabless\ncounts$fourier <- select(counts, epiweek, case_int) %>% \n  as_tsibble(index = epiweek) %>% \n  fourier(K = 1)\n\n## define intervention week \nintervention_week <- yearweek(\"2008-12-31\")\n\n## define variables for regression \ncounts <- counts %>% \n  mutate(\n    ## corresponds to t in the formula\n      ## count of weeks (could probably also just use straight epiweeks var)\n    # linear = row_number(epiweek), \n    ## corresponds to delta(t-t0) in the formula\n      ## pre or post intervention period\n    intervention = as.numeric(epiweek >= intervention_week), \n    ## corresponds to (t-t0)^+ in the formula\n      ## count of weeks post intervention\n      ## (choose the larger number between 0 and whatever comes from calculation)\n    time_post = pmax(0, epiweek - intervention_week + 1))\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier + \n    ## add in whether in the pre- or post-period \n    intervention + \n    ## add in the time post intervention \n    time_post\n    )\n\n## fit your model using the counts dataset\nfitted_model <- fit(model, counts)\n\n## calculate confidence intervals and prediction intervals \nobserved <- predict(fitted_model)\n\n\n\n## show estimates and percentage change in a table\nfitted_model %>% \n  ## extract original negative binomial regression\n  get_model() %>% \n  ## get a tidy dataframe of results\n  tidy(exponentiate = TRUE, \n       conf.int = TRUE) %>% \n  ## only keep the intervention value \n  filter(term == \"intervention\") %>% \n  ## change the IRR to percentage change for estimate and CIs \n  mutate(\n    ## for each of the columns of interest - create a new column\n    across(\n      all_of(c(\"estimate\", \"conf.low\", \"conf.high\")), \n      ## apply the formula to calculate percentage change\n            .f = function(i) 100 * (i - 1), \n      ## add a suffix to new column names with \"_perc\"\n      .names = \"{.col}_perc\")\n    ) %>% \n  ## only keep (and rename) certain columns \n  select(\"IRR\" = estimate, \n         \"95%CI low\" = conf.low, \n         \"95%CI high\" = conf.high,\n         \"Percentage change\" = estimate_perc, \n         \"95%CI low (perc)\" = conf.low_perc, \n         \"95%CI high (perc)\" = conf.high_perc,\n         \"p-value\" = p.value)## # A tibble: 1 x 7\r\n##     IRR `95%CI low` `95%CI high` `Percentage change` `95%CI low (perc)` `95%CI high (perc)` `p-value`\r\n##   <dbl>       <dbl>        <dbl>               <dbl>              <dbl>               <dbl>     <dbl>\r\n## 1 0.958       0.896         1.03               -4.17              -10.4                2.53     0.220\nggplot(observed, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate, col = \"Estimate\")) + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add vertical line and label to show where forecasting started\n  geom_vline(\n           xintercept = as.Date(intervention_week), \n           linetype = \"dashed\") + \n  annotate(geom = \"text\", \n           label = \"Intervention\", \n           x = intervention_week, \n           y = max(observed$upper_pi), \n           angle = 90, \n           vjust = 1\n           ) + \n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Estimate\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()"},{"path":"time-series-analysis.html","id":"resources","chapter":"1 Time series analysis","heading":"1.9 Resources","text":"forecasting: principles practice textbookEPIET timeseries analysis case studiesPenn State course\r\nSurveillance package manuscript","code":""}]
