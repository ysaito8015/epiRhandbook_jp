[{"path":"index.html","id":"welcome---this-is-a-draft","chapter":"Welcome - THIS IS A DRAFT","heading":"Welcome - THIS IS A DRAFT","text":"","code":""},{"path":"index.html","id":"about-this-handbook","chapter":"Welcome - THIS IS A DRAFT","heading":"About this handbook","text":"free open-access R reference book applied epidemiologists public health practitioners.book strives :Serve quick reference guide - textbookAddress common epidemiological problems via task-centered examplesBe accessible settings limited technical support low internet-connectivity (downloadable version)Contain clear simple language, step--step instructions helpful code annotationBe living document, growing adapting new best practicesWhat gaps book address?Many epidemiologists lack formal R training transitioning SAS, STATA, statistical software.R universe changes frequently - place best practice code catered toward common epidemiologist user.Epidemiologists often search dozens online forums answers, epidemiology-oriented.epidemiologists work low internet-connectivity environments limited technical support.different R books?handbook written epidemiologists, epidemiologists. approved product specific organization. Examples techniques adapted authors lived experience local, national, academic, emergency settings.book offered download-able format settings unreliable internet.addition core R skills book uses epidemiology-centered examples cover tasks like epidemic curves, transmission chains epidemic modeling, age sex pyramids, age sex standardization, probabilistic matching records, outbreak detection methods, survey analysis, causal diagrams, survival analysis, GIS basics, phylogenetic trees, missing data imputation, automated routine reports Rmarkdown, etc…","code":""},{"path":"index.html","id":"how-to-read-this-handbook","chapter":"Welcome - THIS IS A DRAFT","heading":"How to read this handbook","text":"Search via search box Table ContentsClick “clipboard” “copy” icon copy codeSee “Resources” section page links trainingClick download offline versionIf use handbook suggestions, let us know SURVEY LINK!","code":""},{"path":"index.html","id":"edit-or-contribute","chapter":"Welcome - THIS IS A DRAFT","heading":"Edit or contribute","text":"suggestions want contribute content, please post issue submit pull request github repository.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Welcome - THIS IS A DRAFT","heading":"Acknowledgements","text":"","code":""},{"path":"index.html","id":"contributors","chapter":"Welcome - THIS IS A DRAFT","heading":"Contributors","text":"handbook collaborative team production. conceived, written, edited epidemiologists public health practitioners around world, drawn upon experiences within constellation organizations including local/state/provincial/national health departments ministries, World Health Organization (), MSF (Medecins sans frontiers / Doctors without Borders), UNHCR, WFP, hospital systems, academic institutions.team members:Editor--Chief: Neale BatraEditorial core team: Alex Spina, Amrish Baidjoe, Henry Laurenson-Schafer, Finlay Campbell, Pat KeatingAuthors (order contributions): Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, Isaac Florence, Natalie Fischer, Daniel Molling, Liza Coyer, Jonny Polonski, Yurie Izawa, Sara HollisReviewers: …(list)…Advisers …(list)…","code":""},{"path":"index.html","id":"funding-and-programmatic-support","chapter":"Welcome - THIS IS A DRAFT","heading":"Funding and programmatic support","text":"handbook received funding via COVID-19 emergency capacity-building grant Training Programs Epidemiology Public Health Interventions Network (TEPHINET).Programmatic support provided EPIET Alumni Network (EAN).","code":""},{"path":"index.html","id":"inspiration","chapter":"Welcome - THIS IS A DRAFT","heading":"Inspiration","text":"multitude tutorials vignettes provided foundational knowledge development handbook content credited within respective pages.generally, following sources provided inspiration laid groundwork handbook:“R4Epis” project (collaboration MSF RECON)R Epidemics Consortium (RECON)R Data Science book (R4DS)bookdown: Authoring Books Technical Documents R MarkdownNetlify hosts website","code":""},{"path":"index.html","id":"image-credits","chapter":"Welcome - THIS IS A DRAFT","heading":"Image credits","text":"Logo: CDC Public Health Image library, R Graph Gallery2013 Yemen looking mosquito breeding sitesEbola virusSurvey RajasthanNetwork","code":""},{"path":"index.html","id":"license-and-terms-of-use","chapter":"Welcome - THIS IS A DRAFT","heading":"License and Terms of Use","text":"handbook approved product specific organization.Although strive accuracy, provide guarantee content book.book licensed Creative Commons license TBD…","code":""},{"path":"cleaning-data.html","id":"cleaning-data","chapter":"1 Cleaning data","heading":"1 Cleaning data","text":"","code":""},{"path":"cleaning-data.html","id":"overview","chapter":"1 Cleaning data","heading":"1.1 Overview","text":"page demonstrates common steps necessary clean dataset, starting importing raw data demonstrating “pipe chain” cleaning steps.page uses simulated Ebola case linelist, referenced throughout handbook.functions described page:%>% - pipe pass dataset one function nextmutate() - create, transform, re-define columnsselect() - select re-name columnsrename() - rename columnsacross() - transform multiple columns one timefilter() - keep certain rowsadd_row() - add row manuallyclean_names() - standardize syntax column namesas.characer(), .numeric(), .Date(), etc. - convert class columnrecode() - re-code values columncase_when() - re-code values column using complex logical criteriareplace_na(), na_if(), coalesce() - special functions re-codingclean_data() - re-code/clean using data dictionaryage_categories() cut() - create categorical groups numeric columndistinct() - de-duplicate rows","code":""},{"path":"cleaning-data.html","id":"cleaning-pipeline","chapter":"1 Cleaning data","heading":"1.2 Cleaning pipeline","text":"page proceeds typical cleaning steps, adding sequentially cleaning pipe chain.epidemiological analysis data processing, cleaning steps often performed linked together, sequentially. R often manifests cleaning “pipeline”, raw dataset passed “piped” one cleaning step another.chain often utilizes dplyr verb functions magrittr pipe operator %>%. pipe begins “raw” data (linelist_raw) ends “clean” dataset (linelist).cleaning pipeline order steps important. Cleaning steps might include:Importing dataColumn names cleaned changedDe-duplicationColumn creation transformation (e.g. re-coding cleaning values)Rows filtered added","code":""},{"path":"cleaning-data.html","id":"load-packages","chapter":"1 Cleaning data","heading":"1.3 Load packages","text":"packages used page clean data:","code":"\npacman::p_load(\n  rio,        # importing data  \n  here,       # relative file pathways  \n  janitor,    # data cleaning\n  lubridate,  # working with dates\n  epikit,     # age_categories() function\n  tidyverse   # data manipulation and visualization\n)"},{"path":"cleaning-data.html","id":"import-data","chapter":"1 Cleaning data","heading":"1.4 Import data","text":"","code":""},{"path":"cleaning-data.html","id":"import","chapter":"1 Cleaning data","heading":"Import","text":"import raw .xlsx dataset using import() function package rio, save dataframe linelist_raw. dataset large takes long time import, can useful import command separate pipe chain “raw” saved distinct file. also allows easy comparison original cleaned versions.See page [Importing exporting] data details unusual situations, including:Skipping import certain rowsDealing second row data dictionaryImporting Google sheetsYou can view first 50 rows original “raw” dataset :","code":"\nlinelist_raw <- import(\"linelist_raw.xlsx\")"},{"path":"cleaning-data.html","id":"review","chapter":"1 Cleaning data","heading":"Review","text":"can use package skimr function skim() get overview entire dataframe (see page [Descriptive analysis]).Scroll right see histograms numeric column included.Table 1.1: Data summaryVariable type: characterVariable type: numericVariable type: POSIXct","code":"\nskimr::skim(linelist_raw)"},{"path":"cleaning-data.html","id":"column-names","chapter":"1 Cleaning data","heading":"1.5 Column names","text":"Column names used often, must “clean” syntax. suggest following:Short namesNo spaces (replace underscores _ ,unusual characters (&, #, <, >, …)Similar style nomenclature (e.g. date columns named like date_onset, date_report, date_death…)columns names linelist_raw printed using names() base R. can see :names contain spacesDifferent naming patterns used dates (‘date onset’ vs. ‘infection date’)must merged header across two last columns .xlsx. know name two merged columns (“merged_header”) applied first one, second column assigned placeholder name “…28”, empty 28th column.Note: column name include spaces, surround name back-ticks, example: linelist$`infection date`. note keyboard, back-tick (`) different single quotation mark (’).","code":"\nnames(linelist_raw)##  [1] \"case_id\"         \"generation\"      \"infection date\"  \"date onset\"      \"hosp date\"       \"date_of_outcome\" \"outcome\"         \"gender\"         \r\n##  [9] \"hospital\"        \"lon\"             \"lat\"             \"infector\"        \"source\"          \"age\"             \"age_unit\"        \"row_num\"        \r\n## [17] \"wt_kg\"           \"ht_cm\"           \"ct_blood\"        \"fever\"           \"chills\"          \"cough\"           \"aches\"           \"vomit\"          \r\n## [25] \"temp\"            \"time_admission\"  \"merged_header\"   \"...28\""},{"path":"cleaning-data.html","id":"automatic-name-cleaning","chapter":"1 Cleaning data","heading":"Automatic name cleaning","text":"function clean_names() package janitor standardizes column names makes unique following:Converts names consist underscores, numbers, lettersAccented characters transliterated ASCII (e.g. german o umlaut becomes “o”, spanish “enye” becomes “n”)Capitalization preference can specified using case = argument (“snake” default, alternatives include “sentence”, “title”, “small_camel”…)can designate specific name replacements replace = argument (e.g. replace = c(onset = “date_of_onset”))online vignetteBelow, cleaning pipeline begins using clean_names() raw linelist.NOTE: last column name “…28” changed “x28”.","code":"\n# send the dataset through the function clean_names()\nlinelist <- linelist_raw %>% \n  janitor::clean_names()\n\n# see the new names\nnames(linelist)##  [1] \"case_id\"         \"generation\"      \"infection_date\"  \"date_onset\"      \"hosp_date\"       \"date_of_outcome\" \"outcome\"         \"gender\"         \r\n##  [9] \"hospital\"        \"lon\"             \"lat\"             \"infector\"        \"source\"          \"age\"             \"age_unit\"        \"row_num\"        \r\n## [17] \"wt_kg\"           \"ht_cm\"           \"ct_blood\"        \"fever\"           \"chills\"          \"cough\"           \"aches\"           \"vomit\"          \r\n## [25] \"temp\"            \"time_admission\"  \"merged_header\"   \"x28\""},{"path":"cleaning-data.html","id":"manual-name-cleaning","chapter":"1 Cleaning data","heading":"Manual name cleaning","text":"Re-naming columns manually often necessary. , re-naming performed using rename() function dplyr package, part pipe chain. rename() uses style “NEW = OLD”, new column name given old column name., re-name command added cleaning pipeline:Now can see columns names changed:","code":"\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\nlinelist <- linelist_raw %>%\n    \n    # standardize column name syntax\n    janitor::clean_names() %>% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome)##  [1] \"case_id\"              \"generation\"           \"date_infection\"       \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \r\n##  [7] \"outcome\"              \"gender\"               \"hospital\"             \"lon\"                  \"lat\"                  \"infector\"            \r\n## [13] \"source\"               \"age\"                  \"age_unit\"             \"row_num\"              \"wt_kg\"                \"ht_cm\"               \r\n## [19] \"ct_blood\"             \"fever\"                \"chills\"               \"cough\"                \"aches\"                \"vomit\"               \r\n## [25] \"temp\"                 \"time_admission\"       \"merged_header\"        \"x28\""},{"path":"cleaning-data.html","id":"rename-by-column-position","chapter":"1 Cleaning data","heading":"Rename by column position","text":"can also rename column position, instead column name, example:","code":"\nrename(newNameForFirstColumn  = 1,\n       newNameForSecondColumn = 2)"},{"path":"cleaning-data.html","id":"rename-via-select","chapter":"1 Cleaning data","heading":"Rename via select()","text":"can also rename columns within dplyr select() function, used retain certain columns (covered later page). approach also uses format new_name = old_name. example:","code":"\nlinelist_raw %>% \n  select(# NEW name             # OLD name\n         date_infection       = `infection date`,    # rename and KEEP ONLY these columns\n         date_hospitalisation = `hosp date`)"},{"path":"cleaning-data.html","id":"other-name-challenges","chapter":"1 Cleaning data","heading":"Other name challenges","text":"","code":""},{"path":"cleaning-data.html","id":"empty-excel-column-names","chapter":"1 Cleaning data","heading":"Empty Excel column names","text":"importing Excel sheet missing column name, depending import function used, R likely create column name value like “…1” “…2”. can clean names manually referencing position number (see example ), name (linelist_raw$...1).","code":""},{"path":"cleaning-data.html","id":"merged-excel-column-names-and-cells","chapter":"1 Cleaning data","heading":"Merged Excel column names and cells","text":"Merged cells Excel file common occurrence receiving data field level. Merged cells can nice human reading data, cause many problems machine reading data. R accommodate merged cells.Remind people data entry human-readable data machine-readable data. Strive train users principles tidy data. possible, try change procedures data arrive tidy format without merged cells.variable must column.observation must row.value must cell.using rio’s import() function, value merged cell assigned first cell subsequent cells empty.One solution deal merged cells import data function readWorkbook() package openxlsx. Set argument fillMergedCells = TRUE. gives value merged cell cells within merge range.DANGER: column names merged, end duplicate column names, need fix manually - R work well duplicate column names! can re-name referencing position (e.g. column 5), explained section manual column name cleaning..","code":"\nlinelist_raw <- openxlsx::readWorkbook(\"linelist_raw.xlsx\", fillMergedCells = TRUE)"},{"path":"cleaning-data.html","id":"select-or-re-order-columns","chapter":"1 Cleaning data","heading":"1.6 Select or re-order columns","text":"Use select() select columns want retain, order dataframe.CAUTION: examples , linelist modified select() -written. New column names displayed purpose example.column names linelist:","code":"\nnames(linelist)##  [1] \"case_id\"              \"generation\"           \"date_infection\"       \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \r\n##  [7] \"outcome\"              \"gender\"               \"hospital\"             \"lon\"                  \"lat\"                  \"infector\"            \r\n## [13] \"source\"               \"age\"                  \"age_unit\"             \"row_num\"              \"wt_kg\"                \"ht_cm\"               \r\n## [19] \"ct_blood\"             \"fever\"                \"chills\"               \"cough\"                \"aches\"                \"vomit\"               \r\n## [25] \"temp\"                 \"time_admission\"       \"merged_header\"        \"x28\""},{"path":"cleaning-data.html","id":"keep-columns","chapter":"1 Cleaning data","heading":"1.6.1 Keep columns","text":"Select columns want remainPut names select() command, quotation marks. appear order provide. Note include column exist, R return error (see any_of want error situation).","code":"\n# linelist dataset is piped through select() command, and names() prints just the column names\nlinelist %>% \n  select(case_id, date_onset, date_hospitalisation, fever) %>% \n  names()  # display the column names## [1] \"case_id\"              \"date_onset\"           \"date_hospitalisation\" \"fever\""},{"path":"cleaning-data.html","id":"helper-functions","chapter":"1 Cleaning data","heading":"1.6.2 Helper functions","text":"Helper functions operators exist make easy specify columns.example, want re-order columns, everything() useful signify columns yet mentioned. command pulls columns date_onset date_hospitalisation beginning:well everything() helpers functions work within select():everything() - columns mentionedlast_col() - last columnwhere() - applies function columns selects TRUEstarts_with() - matches specified prefix. Example: select(starts_with(\"date\"))ends_with() - matches specified suffix. Example: select(ends_with(\"_end\"))contains() - columns containing character string. Example: select(contains(\"time\"))matches() - apply regular expression (regex). Example: select(contains(\"[pt]al\"))num_range() - numerical range like x01, x02, x03any_of() - matches column named. Useful name might exist. Example: select(any_of(date_onset, date_death, cardiac_arrest))addition, use normal operators c() list several columns, : consecutive columns, ! opposite, & , | .Use () specify logical criteria columns. providing function inside (), include empty parentheses. selects columns class Numeric.Use contains() select columns column name contains string. ends_with() starts_with() provide nuance.function matches() works similarly contains() can provided regular expression (see page [Characters strings]), multiple strings separated bars within parentheses:CAUTION: column name specifically provide exist data, can return error stop code. Consider using any_of() cite columns may may exist, especially useful negative (remove) selections.one columns exists, error produced code continues.","code":"\n# move date_onset and date_hospitalisation to beginning\nlinelist %>% \n  select(date_onset, date_hospitalisation, everything()) %>% \n  names()##  [1] \"date_onset\"           \"date_hospitalisation\" \"case_id\"              \"generation\"           \"date_infection\"       \"date_outcome\"        \r\n##  [7] \"outcome\"              \"gender\"               \"hospital\"             \"lon\"                  \"lat\"                  \"infector\"            \r\n## [13] \"source\"               \"age\"                  \"age_unit\"             \"row_num\"              \"wt_kg\"                \"ht_cm\"               \r\n## [19] \"ct_blood\"             \"fever\"                \"chills\"               \"cough\"                \"aches\"                \"vomit\"               \r\n## [25] \"temp\"                 \"time_admission\"       \"merged_header\"        \"x28\"\n# select columns that are class Numeric\nlinelist %>% \n  select(where(is.numeric)) %>% \n  names()## [1] \"generation\" \"lon\"        \"lat\"        \"row_num\"    \"wt_kg\"      \"ht_cm\"      \"ct_blood\"   \"temp\"\n# select columns containing certain characters\nlinelist %>% \n  select(contains(\"date\")) %>% \n  names()## [1] \"date_infection\"       \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"\n# searched for multiple character matches\nlinelist %>% \n  select(matches(\"onset|hosp|fev\")) %>%   # note the OR symbol \"|\"\n  names()## [1] \"date_onset\"           \"date_hospitalisation\" \"hospital\"             \"fever\"\nlinelist %>% \n  select(any_of(c(\"date_onset\", \"village_origin\", \"village_detection\", \"village_residence\", \"village_travel\"))) %>% \n  names()## [1] \"date_onset\""},{"path":"cleaning-data.html","id":"remove-columns","chapter":"1 Cleaning data","heading":"1.6.3 Remove columns","text":"Indicate columns remove placing minus symbol “-” front column name (e.g. select(-outcome)), vector column names (). columns retained.","code":"\nlinelist %>% \n  select(-c(date_onset, fever:vomit)) %>% # remove onset and all from fever to vomit\n  names()##  [1] \"case_id\"              \"generation\"           \"date_infection\"       \"date_hospitalisation\" \"date_outcome\"         \"outcome\"             \r\n##  [7] \"gender\"               \"hospital\"             \"lon\"                  \"lat\"                  \"infector\"             \"source\"              \r\n## [13] \"age\"                  \"age_unit\"             \"row_num\"              \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \r\n## [19] \"temp\"                 \"time_admission\"       \"merged_header\"        \"x28\""},{"path":"cleaning-data.html","id":"standalone","chapter":"1 Cleaning data","heading":"1.6.4 Standalone","text":"select() can also used independent command (pipe chain). case, first argument original dataframe operated upon.Add pipe chainIn linelist_raw, columns need: row_num, merged_header, x28. Remove adding select() command cleaning pipe chain:","code":"\n# Create a new linelist with id and age-related columns\nlinelist_age <- select(linelist, case_id, contains(\"age\"))\n\n# display the column names\nnames(linelist_age)## [1] \"case_id\"  \"age\"      \"age_unit\"\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist <- linelist_raw %>%\n    \n    # standardize column name syntax\n    janitor::clean_names() %>% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %>% \n    \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    #####################################################\n\n    # remove column\n    select(-c(row_num, merged_header, x28))"},{"path":"cleaning-data.html","id":"deduplication","chapter":"1 Cleaning data","heading":"1.7 Deduplication","text":"See handbook page [De-duplication]. simple de-duplication example presented .package dplyr offers distinct() function reduce dataframe unique rows - removing rows 100% duplicates. just add simple command distinct() pipe chain:begin 6609 rows linelist.de-duplication 6479 rows. rows 100% duplicates rows., distinct() command added cleaning pipe chain:","code":"\nlinelist <- linelist %>% \n  distinct()\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist <- linelist_raw %>%\n    \n    # standardize column name syntax\n    janitor::clean_names() %>% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %>% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %>% \n  \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    #####################################################\n    \n    # de-duplicate\n    distinct()"},{"path":"cleaning-data.html","id":"column-creation-and-transformation","chapter":"1 Cleaning data","heading":"1.8 Column creation and transformation","text":"verb mutate() used add new column, modify existing one. example creating new columns mutate(). syntax : mutate(new_column_name = value transformation)","code":""},{"path":"cleaning-data.html","id":"new-columns","chapter":"1 Cleaning data","heading":"New columns","text":"basic mutate() command create new column might look like . creates new column new_col value every row 10.can also reference values columns, perform calculations. example Body Mass Index (BMI) calculated using formula BMI = kg/m^2, using column ht_cm column wt_kg.creating multiple new columns, separate comma new line. , examples ways new columns, including pasting together values columns using str_glue() stringr package:Scroll right see new columns (first 50 rows shown):TIP: verb transmute() adds new columns just like mutate() also drops/removes columns mention.","code":"\nlinelist <- linelist %>% \n  mutate(new_col = 10)\nlinelist <- linelist %>% \n  mutate(bmi = wt_kg / (ht_cm/100)^2)\nlinelist <- linelist %>%                       \n  mutate(\n    new_var_dup    = case_id,             # new column = duplicate/copy another existing column\n    new_var_static = 7,                   # new column = all values the same\n    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables\n    new_var_paste  = stringr::str_glue(\"{hospital} on ({date_hospitalisation})\") # new column = pasting together values from other columns\n    ) "},{"path":"cleaning-data.html","id":"convert-column-class","chapter":"1 Cleaning data","heading":"Convert column class","text":"Often need set correct class column. ways set column class import commands, often often cumbersome. See section object classes learn converting class objects, including columns.First, run checks important columns see correct class:Currently, class “age” column character. perform quantitative analyses, need numbers recognized numeric!class “date_onset” column also character! perform analyses, dates must recognized dates!case, use mutate() define column , converted different class. basic example, converting ensuring column age class Numeric:Examples converting functions:Dates can especially difficult! date values must format conversion work correctly (e.g “MM/DD/YYYY”, “DD Mmm YYYY”). See page Working Dates (LINK) details. Especially converting class date, check data visually cross-table confirm value converted correctly. .Date(), format = argument often source errors.","code":"\nclass(linelist$age)## [1] \"character\"\nclass(linelist$date_onset)## [1] \"character\"\nlinelist <- linelist %>% \n  mutate(age = as.numeric(age))\n# Examples of modifying class\nlinelist <- linelist %>% \n  mutate(date_var      = as.Date(date_var, format = \"MM/DD/YYYY\"),  # See page on Dates for details  \n         numeric_var   = as.numeric(numeric_var),\n         character_var = as.character(character_var),\n         factor_var    = factor(factor_var, levels = c(...), labels = c(...))  # See page on Factors for details  \n         )"},{"path":"cleaning-data.html","id":"using-base-r","chapter":"1 Cleaning data","heading":"Using base R","text":"define new column (re-define column) using base R, just use assignment operator .\r\nRemember using base R must specify dataframe writing column name (e.g. dataframe$column). two dummy examples:","code":"\nlinelist$old_var <- linelist$old_var + 7\nlinelist$new_var <- linelist$old_var + linelist$age"},{"path":"cleaning-data.html","id":"mutate-on-grouped-values","chapter":"1 Cleaning data","heading":"Mutate on grouped values","text":"dataframe already grouped (see page [Grouping data]), mutate() may behave differently dataframe grouped. summarizing functions, like mean(), median(), max(), etc. based grouped rows, rows.Read using mutate grouped dataframes tidyverse mutate documentation.","code":"\n# age normalized to mean of ALL rows\nlinelist %>% \n  select(case_id, age, hospital) %>% \n  mutate(age_norm = age / mean(age, na.rm=T))\n\n# age normalized to mean of hospital group\nlinelist %>% \n  select(case_id, age, hospital) %>% \n  group_by(hospital) %>% \n  mutate(age_norm = age / mean(age, na.rm=T))"},{"path":"cleaning-data.html","id":"add-to-pipe-chain","chapter":"1 Cleaning data","heading":"Add to pipe chain","text":", new column added pipe chain classes converted.","code":"\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist <- linelist_raw %>%\n    \n    # standardize column name syntax\n    janitor::clean_names() %>% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %>% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %>% \n  \n    # de-duplicate\n    distinct() %>% \n  \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    ###################################################\n    # add new column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %>% \n  \n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age)) "},{"path":"cleaning-data.html","id":"re-code-values","chapter":"1 Cleaning data","heading":"1.9 Re-code values","text":"scenarios need re-code (change) values:edit one specific value (e.g. one date incorrect year format)reconcile values spelled sameto create new column groupings categorical valuesto create new column grouping numeric values (e.g. age categories)","code":""},{"path":"cleaning-data.html","id":"re-code-specific-values","chapter":"1 Cleaning data","heading":"Re-code specific values","text":"change values manually can use recode() function within mutate() function.Imagine nonsensical date data (e.g. “2014-14-15”): fix date source data, , write change cleaning pipeline via mutate() recode().mutate() line can read : “mutate column date_onset equal column date_onset re-coded OLD VALUE changed NEW VALUE”. Note pattern (OLD = NEW) recode() opposite R patterns (new = old). R development community working revising .another example re-coding multiple values within one column.linelist values column “hospital” must cleaned. several different spellings many missing values.recode() command re-defines column “hospital” current column “hospital”, specified recode changes. Don’t forget commas !Now see spellings hospital column corrected consolidated:TIP: number spaces equals sign matter. Make code easier read aligning = rows. Also, consider adding hashed comment row clarify future readers side OLD side NEW. TIP: Sometimes blank character value exists dataset (recognized R’s value missing - NA). can reference value two quotation marks space inbetween (\"\").","code":"\n# fix incorrect values                   # old value       # new value\nlinelist <- linelist %>% \n  mutate(date_onset = recode(date_onset, \"2014-14-15\" = \"2014-04-15\"))\ntable(linelist$hospital, useNA = \"always\")## \r\n##                      Central Hopital                     Central Hospital                           Hospital A                           Hospital B \r\n##                                   11                                  443                                  289                                  289 \r\n##                     Military Hopital                    Military Hospital                     Mitylira Hopital                    Mitylira Hospital \r\n##                                   30                                  786                                    1                                   79 \r\n##                                Other                         Port Hopital                        Port Hospital St. Mark's Maternity Hospital (SMMH) \r\n##                                  885                                   47                                 1725                                  411 \r\n##   St. Marks Maternity Hopital (SMMH)                                 <NA> \r\n##                                   11                                 1472\nlinelist <- linelist %>% \n  mutate(hospital = recode(hospital,\n                      #    reference: OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      ))\ntable(linelist$hospital, useNA = \"always\")## \r\n##                     Central Hospital                           Hospital A                           Hospital B                    Military Hospital \r\n##                                  454                                  289                                  289                                  896 \r\n##                                Other                        Port Hospital St. Mark's Maternity Hospital (SMMH)                                 <NA> \r\n##                                  885                                 1772                                  422                                 1472"},{"path":"cleaning-data.html","id":"re-code-missing-values","chapter":"1 Cleaning data","heading":"Re-code missing values","text":"dplyr offers two special function handling missing values:replace_na()change missing values (NA) specific value, “Missing”, use function replace_na() within mutate(). Note used manner recode - name variable must repeated within replace_na().na_if()convert specific value NA, use na_if(). command performs opposite operation replace_na(). example , values “Missing” column hospital converted NA.Note: na_if() used logic criteria (e.g. “values > 99”) - use replace() case_when() :","code":"\nlinelist <- linelist %>% \n  mutate(hospital = replace_na(hospital, \"Missing\"))\nlinelist <- linelist %>% \n  mutate(hospital = na_if(hospital, \"Missing\"))\n# Convert temperatures above 40 to NA \nlinelist <- linelist %>% \n  mutate(temp = replace(temp, temp > 40, NA))\n\n# Convert onset dates earlier than 2000 to missing\nlinelist <- linelist %>% \n  mutate(temp = replace(date_onset, date_onset > as.Date(\"2000-01-01\"), NA))"},{"path":"cleaning-data.html","id":"re-code-by-logic","chapter":"1 Cleaning data","heading":"Re-code by logic","text":"demonstrated re-code values column using logic conditions:Using replace(), ifelse() if_else() simple logicUsing case_when() complex logic","code":""},{"path":"cleaning-data.html","id":"re-code-with-simple-logic","chapter":"1 Cleaning data","heading":"Re-code with simple logic","text":"","code":""},{"path":"cleaning-data.html","id":"replace","chapter":"1 Cleaning data","heading":"replace()","text":"re-code simple logical criteria, can use replace() within mutate(). replace() function base R. Use logic condition specify rows change . general syntax :mutate(col_to_change = replace(col_to_change, criteria rows, new value)).One common situation changing one value one row, using unique row identifier. , gender changed “Female” row column case_id “2195”.equivalent command using base R syntax indexing brackets [ ] . reads “Change value dataframe linelist‘s column gender (rows linelist’s column case_id value ’2195’) ‘Female’”.","code":"# Example: change gender of one specific observation to \"Female\" \r\nlinelist <- linelist %>% \r\n  mutate(gender = replace(gender, case_id == \"2195\", \"Female\")\nlinelist$gender[linelist$case_id == \"2195\"] <- \"Female\""},{"path":"cleaning-data.html","id":"ifelse-and-if_else","chapter":"1 Cleaning data","heading":"ifelse() and if_else()","text":"Another tool simple logical re-coding ifelse() partner if_else(). However, cases better use case_when() (clarity).commands simplified versions else programming statement (LINK). general syntax :ifelse(condition, value return condition evaluates TRUE, value return condition evaluates FALSE), column source_known defined (re-defined). value given row set “known” row’s value column source missing. value source missing, value source_known set “unknown”.if_else() special version dplyr handles dates. Note ‘true’ value date, ‘false’ value must also qualify date, hence using special character NA_real_ instead just NA.Avoid stringing together many ifelse commands… use case_when() instead! case_when() much easier read ’ll make fewer errors.Outside context dataframe, want object used code switch value based criteria, consider using switch() base R. See section using switch() page R interactive console.","code":"\nlinelist <- linelist %>% \n  mutate(source_known = ifelse(!is.na(source), \"known\", \"unknown\"))\n# Create a date of death column, which is NA if patient has not died.\nlinelist <- linelist %>% \n  mutate(date_death = if_else(outcome == \"Death\", date_outcome, NA_real_))"},{"path":"cleaning-data.html","id":"re-code-with-complex-logic","chapter":"1 Cleaning data","heading":"Re-code with complex logic","text":"Use dplyr’s case_when() need use complex logic statements re-code values. important differences recode() syntax logic order!case_when() commands Right-Hand Side (RHS) Left-Hand Side (LHS) separated “tilde” ~. logic criteria LHS pursuant value RHS. Statements separated commas. important note :Statements evaluated order written - top--bottom. Thus best write specific criteria first, general last.End TRUE LHS, signifies row value meet previous criteriaThe values RHS must class - either numeric, character, logical, etc.\r\nassign NA, may need use special values NA_character_, NA_real_ (numeric POSIX), .Date(NA)\r\nassign NA, may need use special values NA_character_, NA_real_ (numeric POSIX), .Date(NA)utilize columns age age_unit create column age_years:","code":"\nlinelist <- linelist %>% \n  mutate(age_years = case_when(\n            age_unit == \"years\"  ~ age,       # if age is given in years\n            age_unit == \"months\" ~ age/12,    # if age is given in months\n            is.na(age_unit)      ~ age,       # if age unit is missing, assume years\n            TRUE                 ~ NA_real_)) # any other circumstance assign missing"},{"path":"cleaning-data.html","id":"re-code-with-a-cleaning-dictionary","chapter":"1 Cleaning data","heading":"Re-code with a cleaning dictionary","text":"Use package linelist clean linelist cleaning dictionary.Import cleaning dictionary 3 columns:\r\n“” column (incorrect value)\r\n“” column (correct value)\r\ncolumn specifying column changes applied (“.global” apply columns)\r\n“” column (incorrect value)“” column (correct value)column specifying column changes applied (“.global” apply columns)Store names columns want “protect” changes. must provided clean_data() numeric logical vector, see use names(.) command (dot means dataframe).Run clean_data(), specifying cleaning dictionaryScroll see values changed - particularly gender (lowercase uppercase), symptoms columns transformed yes/1/0.CAUTION: clean_data() linelist package also clean values data unless columns protected - may encounter changes columns dashes “-” .Note column names cleaning dictionary must correspond names point cleaning script. clean_data() also implements column name cleaning function similar clean_names() janitor standardizes column names prior applying dictionary.See online reference linelist package details.","code":"\ncleaning_dict <- import(\"cleaning_dict.csv\")\nprotected_cols <- c(\"case_id\", \"source\")\nlinelist <- linelist %>% \n  linelist::clean_data(\n    wordlists = cleaning_dict,\n    spelling_vars = \"col\",       # dict column containing column names, defaults to 3rd column in dict\n    protect = names(.) %in% protected_cols\n  )"},{"path":"cleaning-data.html","id":"other-transformations","chapter":"1 Cleaning data","heading":"1.10 Other transformations","text":"","code":""},{"path":"cleaning-data.html","id":"coalesce","chapter":"1 Cleaning data","heading":"coalesce()","text":"dplyr function finds first non-missing value position.Say two vectors, one village detection another village residence. can use coalesce pick first non-missing value index:provide dataframe columns, row fill value first non-missing value columns provided.","code":"\nvillage_detection <- c(\"a\", \"b\", NA,  NA)\nvillage_residence <- c(\"a\", \"c\", \"a\", \"d\")\n\ncoalesce(village_detection, village_residence)## [1] \"a\" \"b\" \"a\" \"d\"\nlinelist <- linelist %>% \n  mutate(village = coalesce(village_detection, village_residence))"},{"path":"cleaning-data.html","id":"cumulative-mathematics","chapter":"1 Cleaning data","heading":"Cumulative mathematics","text":"column reflect cumulative sum/mean/min/max etc assessed rows dataframe, use following functions:cumsum() returns cumulative sum, shown :can used dataframe making new column. example, calculate cumulative number cases per day outbreak, consider code like :See page [Epidemic curves] plot cumulative incidence epicurve.See also:\r\ncumsum(), cummean(), cummin(), cummax(), cumany(), cumall()","code":"\nsum(c(2,4,15,10))     # returns only one number## [1] 31\ncumsum(c(2,4,15,10))  # returns the cumulative sum at each step## [1]  2  6 21 31\ncumulative_case_counts <- linelist %>% \n  count(date_onset) %>%           # count of rows per day   \n  mutate(                         \n    cumulative_cases = cumsum(n)  # new column of the cumulative sum at that row\n    )\n\ncumulative_case_counts##     date_onset  n cumulative_cases\r\n## 1   2012-04-21  1                1\r\n## 2   2012-05-07  1                2\r\n## 3   2012-05-20  1                3\r\n## 4   2012-05-21  1                4\r\n## 5   2012-05-25  1                5\r\n## 6   2012-06-05  1                6\r\n## 7   2012-06-06  1                7\r\n## 8   2012-06-07  2                9\r\n## 9   2012-06-12  1               10\r\n## 10  2012-07-03  1               11\r\n## 11  2012-07-08  1               12\r\n## 12  2012-07-09  1               13\r\n## 13  2012-07-15  2               15\r\n## 14  2012-07-16  1               16\r\n## 15  2012-07-18  1               17\r\n## 16  2012-07-20  1               18\r\n## 17  2012-07-22  1               19\r\n## 18  2012-07-24  1               20\r\n## 19  2012-07-26  2               22\r\n## 20  2012-07-29  2               24\r\n## 21  2012-07-31  1               25\r\n## 22  2012-08-02  1               26\r\n## 23  2012-08-05  1               27\r\n## 24  2012-08-07  1               28\r\n## 25  2012-08-08  1               29\r\n## 26  2012-08-12  1               30\r\n## 27  2012-08-13  2               32\r\n## 28  2012-08-14  1               33\r\n## 29  2012-08-16  4               37\r\n## 30  2012-08-17  1               38\r\n## 31  2012-08-18  1               39\r\n## 32  2012-08-19  2               41\r\n## 33  2012-08-23  1               42\r\n## 34  2012-08-24  3               45\r\n## 35  2012-08-25  2               47\r\n## 36  2012-08-28  1               48\r\n## 37  2012-08-29  1               49\r\n## 38  2012-08-30  2               51\r\n## 39  2012-09-01  2               53\r\n## 40  2012-09-02  1               54\r\n## 41  2012-09-03  2               56\r\n## 42  2012-09-05  2               58\r\n## 43  2012-09-06  1               59\r\n## 44  2012-09-07  2               61\r\n## 45  2012-09-08  2               63\r\n## 46  2012-09-09  3               66\r\n## 47  2012-09-11  6               72\r\n## 48  2012-09-12  2               74\r\n## 49  2012-09-13  4               78\r\n## 50  2012-09-14  1               79\r\n## 51  2012-09-15  2               81\r\n## 52  2012-09-16  2               83\r\n## 53  2012-09-17  1               84\r\n## 54  2012-09-18  2               86\r\n## 55  2012-09-19  2               88\r\n## 56  2012-09-21  2               90\r\n## 57  2012-09-23  6               96\r\n## 58  2012-09-24  7              103\r\n## 59  2012-09-25  3              106\r\n## 60  2012-09-26  1              107\r\n## 61  2012-09-27  3              110\r\n## 62  2012-09-28  4              114\r\n## 63  2012-09-29  7              121\r\n## 64  2012-09-30  6              127\r\n## 65  2012-10-01  3              130\r\n## 66  2012-10-02  9              139\r\n## 67  2012-10-03  2              141\r\n## 68  2012-10-04  5              146\r\n## 69  2012-10-05  5              151\r\n## 70  2012-10-06  5              156\r\n## 71  2012-10-07  1              157\r\n## 72  2012-10-08  6              163\r\n## 73  2012-10-09  5              168\r\n## 74  2012-10-10  2              170\r\n## 75  2012-10-11  3              173\r\n## 76  2012-10-12  3              176\r\n## 77  2012-10-13  6              182\r\n## 78  2012-10-14  5              187\r\n## 79  2012-10-15  4              191\r\n## 80  2012-10-16 10              201\r\n## 81  2012-10-17  5              206\r\n## 82  2012-10-18  5              211\r\n## 83  2012-10-19  6              217\r\n## 84  2012-10-20  4              221\r\n## 85  2012-10-21  8              229\r\n## 86  2012-10-22  3              232\r\n## 87  2012-10-23  7              239\r\n## 88  2012-10-24 10              249\r\n## 89  2012-10-25  4              253\r\n## 90  2012-10-26 10              263\r\n## 91  2012-10-27 13              276\r\n## 92  2012-10-28  6              282\r\n## 93  2012-10-29  4              286\r\n## 94  2012-10-30  3              289\r\n## 95  2012-10-31  7              296\r\n## 96  2012-11-01  5              301\r\n## 97  2012-11-02  7              308\r\n## 98  2012-11-03  9              317\r\n## 99  2012-11-04  6              323\r\n## 100 2012-11-05  1              324\r\n## 101 2012-11-06  1              325\r\n## 102 2012-11-07  3              328\r\n## 103 2012-11-08  8              336\r\n## 104 2012-11-09  3              339\r\n## 105 2012-11-10  8              347\r\n## 106 2012-11-11  5              352\r\n## 107 2012-11-12  2              354\r\n## 108 2012-11-13  6              360\r\n## 109 2012-11-14  1              361\r\n## 110 2012-11-15  4              365\r\n## 111 2012-11-16  3              368\r\n## 112 2012-11-17  3              371\r\n## 113 2012-11-18  4              375\r\n## 114 2012-11-19  1              376\r\n## 115 2012-11-20  4              380\r\n## 116 2012-11-21  6              386\r\n## 117 2012-11-22  2              388\r\n## 118 2012-11-23  6              394\r\n## 119 2012-11-24  4              398\r\n## 120 2012-11-25  5              403\r\n## 121 2012-11-26  1              404\r\n## 122 2012-11-27  3              407\r\n## 123 2012-11-28  5              412\r\n## 124 2012-11-29  6              418\r\n## 125 2012-11-30  3              421\r\n## 126 2012-12-01  4              425\r\n## 127 2012-12-02  3              428\r\n## 128 2012-12-04  6              434\r\n## 129 2012-12-05  3              437\r\n## 130 2012-12-06  5              442\r\n## 131 2012-12-07  3              445\r\n## 132 2012-12-08  1              446\r\n## 133 2012-12-09  3              449\r\n## 134 2012-12-10  6              455\r\n## 135 2012-12-11  2              457\r\n## 136 2012-12-12  1              458\r\n## 137 2012-12-13  4              462\r\n## 138 2012-12-14  7              469\r\n## 139 2012-12-15  2              471\r\n## 140 2012-12-16  4              475\r\n## 141 2012-12-17  2              477\r\n## 142 2012-12-18  1              478\r\n## 143 2012-12-19  3              481\r\n## 144 2012-12-20  3              484\r\n## 145 2012-12-21  3              487\r\n## 146 2012-12-22  3              490\r\n## 147 2012-12-24  2              492\r\n## 148 2012-12-25  3              495\r\n## 149 2012-12-26  2              497\r\n## 150 2012-12-27  1              498\r\n## 151 2012-12-28  1              499\r\n## 152 2012-12-29  3              502\r\n## 153 2012-12-30  1              503\r\n## 154 2012-12-31  1              504\r\n## 155 2013-01-01  3              507\r\n## 156 2013-01-03  2              509\r\n## 157 2013-01-05  4              513\r\n## 158 2013-01-06  2              515\r\n## 159 2013-01-07  1              516\r\n## 160 2013-01-08  1              517\r\n## 161 2013-01-11  4              521\r\n## 162 2013-01-12  4              525\r\n## 163 2013-01-14  1              526\r\n## 164 2013-01-15  1              527\r\n## 165 2013-01-16  2              529\r\n## 166 2013-01-17  1              530\r\n## 167 2013-01-19  3              533\r\n## 168 2013-01-20  1              534\r\n## 169 2013-01-21  1              535\r\n## 170 2013-01-23  1              536\r\n## 171 2013-01-25  2              538\r\n## 172 2013-01-26  1              539\r\n## 173 2013-01-28  1              540\r\n## 174 2013-01-29  2              542\r\n## 175 2013-02-01  2              544\r\n## 176 2013-02-02  2              546\r\n## 177 2013-02-04  1              547\r\n## 178 2013-02-05  1              548\r\n## 179 2013-02-06  1              549\r\n## 180 2013-02-09  1              550\r\n## 181 2013-02-10  2              552\r\n## 182 2013-02-15  1              553\r\n## 183 2013-02-19  1              554\r\n## 184 2013-02-20  1              555\r\n## 185 2013-02-21  2              557\r\n## 186 2013-02-22  1              558\r\n## 187 2013-02-24  1              559\r\n## 188 2013-03-03  3              562\r\n## 189 2013-03-05  2              564\r\n## 190 2013-03-08  1              565\r\n## 191 2013-03-09  1              566\r\n## 192 2013-03-11  1              567\r\n## 193 2013-03-12  1              568\r\n## 194 2013-03-13  1              569\r\n## 195 2013-03-18  1              570\r\n## 196 2013-03-19  2              572\r\n## 197 2013-03-20  1              573\r\n## 198 2013-03-22  1              574\r\n## 199 2013-03-23  1              575\r\n## 200 2013-03-27  1              576\r\n## 201 2013-03-29  1              577\r\n## 202 2013-04-01  2              579\r\n## 203 2013-04-05  1              580\r\n## 204 2013-04-13  1              581\r\n## 205 2013-04-14  1              582\r\n## 206 2013-04-16  1              583\r\n## 207 2013-04-24  1              584\r\n## 208 2013-04-27  2              586\r\n## 209 2014-04-07  1              587\r\n## 210 2014-04-15  1              588\r\n## 211 2014-04-21  2              590\r\n## 212 2014-04-25  1              591\r\n## 213 2014-04-26  1              592\r\n## 214 2014-04-27  1              593\r\n## 215 2014-05-01  2              595\r\n## 216 2014-05-03  1              596\r\n## 217 2014-05-04  1              597\r\n## 218 2014-05-05  1              598\r\n## 219 2014-05-06  3              601\r\n## 220 2014-05-07  2              603\r\n## 221 2014-05-08  2              605\r\n## 222 2014-05-09  2              607\r\n## 223 2014-05-10  1              608\r\n## 224 2014-05-11  1              609\r\n## 225 2014-05-12  3              612\r\n## 226 2014-05-13  3              615\r\n## 227 2014-05-14  3              618\r\n## 228 2014-05-16  2              620\r\n## 229 2014-05-17  3              623\r\n## 230 2014-05-18  4              627\r\n## 231 2014-05-19  3              630\r\n## 232 2014-05-20  1              631\r\n## 233 2014-05-21  3              634\r\n## 234 2014-05-22  1              635\r\n## 235 2014-05-23  2              637\r\n## 236 2014-05-24  2              639\r\n## 237 2014-05-25  3              642\r\n## 238 2014-05-26  4              646\r\n## 239 2014-05-27 11              657\r\n## 240 2014-05-28  1              658\r\n## 241 2014-05-29  1              659\r\n## 242 2014-05-30  1              660\r\n## 243 2014-05-31  2              662\r\n## 244 2014-06-02  5              667\r\n## 245 2014-06-03  3              670\r\n## 246 2014-06-05  4              674\r\n## 247 2014-06-06  5              679\r\n## 248 2014-06-07  5              684\r\n## 249 2014-06-08  1              685\r\n## 250 2014-06-09  5              690\r\n## 251 2014-06-10  5              695\r\n## 252 2014-06-11  3              698\r\n## 253 2014-06-12  1              699\r\n## 254 2014-06-13  1              700\r\n## 255 2014-06-14  2              702\r\n## 256 2014-06-15  5              707\r\n## 257 2014-06-16  6              713\r\n## 258 2014-06-17  4              717\r\n## 259 2014-06-18  2              719\r\n## 260 2014-06-19  3              722\r\n## 261 2014-06-20  7              729\r\n## 262 2014-06-21  3              732\r\n## 263 2014-06-22  5              737\r\n## 264 2014-06-23  2              739\r\n## 265 2014-06-24  6              745\r\n## 266 2014-06-25  5              750\r\n## 267 2014-06-26  3              753\r\n## 268 2014-06-27  2              755\r\n## 269 2014-06-28  2              757\r\n## 270 2014-06-29  2              759\r\n## 271 2014-06-30  6              765\r\n## 272 2014-07-01  4              769\r\n## 273 2014-07-02  4              773\r\n## 274 2014-07-03  5              778\r\n## 275 2014-07-04  7              785\r\n## 276 2014-07-05  5              790\r\n## 277 2014-07-06  3              793\r\n## 278 2014-07-07  3              796\r\n## 279 2014-07-08  7              803\r\n## 280 2014-07-09  8              811\r\n## 281 2014-07-10  5              816\r\n## 282 2014-07-11  4              820\r\n## 283 2014-07-12  6              826\r\n## 284 2014-07-13  6              832\r\n## 285 2014-07-14 11              843\r\n## 286 2014-07-15 10              853\r\n## 287 2014-07-16  6              859\r\n## 288 2014-07-17  6              865\r\n## 289 2014-07-18 10              875\r\n## 290 2014-07-19  7              882\r\n## 291 2014-07-20 12              894\r\n## 292 2014-07-21  4              898\r\n## 293 2014-07-22  6              904\r\n## 294 2014-07-23 11              915\r\n## 295 2014-07-24  9              924\r\n## 296 2014-07-25 10              934\r\n## 297 2014-07-26  7              941\r\n## 298 2014-07-27 12              953\r\n## 299 2014-07-28  6              959\r\n## 300 2014-07-29 15              974\r\n## 301 2014-07-30 14              988\r\n## 302 2014-07-31  8              996\r\n## 303 2014-08-01 10             1006\r\n## 304 2014-08-02 16             1022\r\n## 305 2014-08-03 13             1035\r\n## 306 2014-08-04 18             1053\r\n## 307 2014-08-05 18             1071\r\n## 308 2014-08-06  9             1080\r\n## 309 2014-08-07  9             1089\r\n## 310 2014-08-08  9             1098\r\n## 311 2014-08-09 10             1108\r\n## 312 2014-08-10 14             1122\r\n## 313 2014-08-11 17             1139\r\n## 314 2014-08-12  7             1146\r\n## 315 2014-08-13 17             1163\r\n## 316 2014-08-14 13             1176\r\n## 317 2014-08-15 19             1195\r\n## 318 2014-08-16 31             1226\r\n## 319 2014-08-17 13             1239\r\n## 320 2014-08-18 14             1253\r\n## 321 2014-08-19 15             1268\r\n## 322 2014-08-20 20             1288\r\n## 323 2014-08-21 20             1308\r\n## 324 2014-08-22 29             1337\r\n## 325 2014-08-23 19             1356\r\n## 326 2014-08-24 27             1383\r\n## 327 2014-08-25 23             1406\r\n## 328 2014-08-26 19             1425\r\n## 329 2014-08-27 25             1450\r\n## 330 2014-08-28 23             1473\r\n## 331 2014-08-29 26             1499\r\n## 332 2014-08-30 25             1524\r\n## 333 2014-08-31 22             1546\r\n##  [ reached 'max' / getOption(\"max.print\") -- omitted 243 rows ]"},{"path":"cleaning-data.html","id":"add-to-pipe-chain-1","chapter":"1 Cleaning data","heading":"Add to pipe chain","text":", new columns column transformations added pipe chain.","code":"\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist <- linelist_raw %>%\n    \n    # standardize column name syntax\n    janitor::clean_names() %>% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %>% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %>% \n  \n    # de-duplicate\n    distinct() %>% \n  \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    ###################################################\n    \n    # add column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     \n\n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age)) %>% \n    \n    # add column: delay to hospitalisation\n    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% \n    \n    # clean values of hospital column\n    mutate(hospital = recode(hospital,\n                      # OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      )) %>% \n    \n    mutate(hospital = replace_na(hospital, \"Missing\")) %>% \n\n    # create age_years column (from age and age_unit)\n    mutate(age_years = case_when(\n          age_unit == \"years\" ~ age,\n          age_unit == \"months\" ~ age/12,\n          is.na(age_unit) ~ age,\n          TRUE ~ NA_real_))"},{"path":"cleaning-data.html","id":"num_cats","chapter":"1 Cleaning data","heading":"1.11 Numeric categories","text":"describe special approaches creating numeric categories. Common examples include age categories, groups lab values, etc. discuss:age_categories(), epikit packagecut(), base Rcase_when()quantile breaks","code":""},{"path":"cleaning-data.html","id":"review-distribution","chapter":"1 Cleaning data","heading":"Review distribution","text":"example create age_cat column using age_years column.First, examine distribution data, make appropriate cut-points. See page [Plot continuous data].CAUTION: Sometimes, numeric variables import class “character”. occurs non-numeric characters values, example entry “2 months” age, (depending R locale settings) comma used decimals place (e.g. “4,5” mean four one half years)..","code":"\n#check the class of the linelist variable age\nclass(linelist$age_years)## [1] \"numeric\"\n# examine the distribution\nhist(linelist$age_years)\nsummary(linelist$age_years, na.rm=T)##           Min.        1st Qu.         Median           Mean        3rd Qu.           Max.           NA's \r\n##  0.00000000000  6.00000000000 13.00000000000 16.31495424837 24.00000000000 95.00000000000            104"},{"path":"cleaning-data.html","id":"age_categories","chapter":"1 Cleaning data","heading":"age_categories()","text":"epikit package, can use age_categories() function easily categorize label numeric columns (note: function can applied non-age numeric variables ). note: output ordered factor.required inputs:numeric vector (column)breakers = - numeric vector break points new groupsFirst, simple example:break values specify default included “higher” group - groups “open” lower/left side. shown , can add 1 break value achieve groups open top/right.can adjust labels displayed separator =. default “-”can adjust upper cut-values allowed included group. Use ceiling =, default FALSE. TRUE, highest break value “ceiling” category “XX+” included. values highest break value upper (defined) categorized NA. example ceiling = TRUE, category XX+ values 70 (highest break value) assigned NA.Alternatively, instead breakers =, can provide lower =, upper =, =:lower = lowest number want considered - default 0upper = highest number want consideredby = number years groupsSee function’s Help page details (enter ?age_categories R console).","code":"\n# Simple example\n################\npacman::p_load(epikit)\n\nlinelist <- linelist %>% \n  mutate(\n    age_cat = age_categories(\n      age_years,\n      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70)))\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")## \r\n##   0-4   5-9 10-14 15-19 20-29 30-39 40-49 50-59 60-69   70+  <NA> \r\n##  1159  1225  1045   839  1117   581   263   107    27    12   104\n# Include upper ends for the same categories\n############################################\nlinelist <- linelist %>% \n  mutate(\n    age_cat = age_categories(\n      age_years, \n      breakers = c(0, 6, 11, 16, 21, 31, 41, 51, 61, 71)))\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")## \r\n##   0-5  6-10 11-15 16-20 21-30 31-40 41-50 51-60 61-70   71+  <NA> \r\n##  1412  1214   999   776  1053   553   245    88    24    11   104\n# With ceiling set to TRUE\n##########################\nlinelist <- linelist %>% \n  mutate(\n    age_cat = age_categories(\n      age_years, \n      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70),\n      ceiling = TRUE)) # 70 is ceiling, all above become NA\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")## \r\n##   0-4   5-9 10-14 15-19 20-29 30-39 40-49 50-59 60-70  <NA> \r\n##  1159  1225  1045   839  1117   581   263   107    28   115\nlinelist <- linelist %>% \n  mutate(\n    age_cat = age_categories(\n      age_years, \n      lower = 0,\n      upper = 100,\n      by = 10))\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")## \r\n##   0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-79 80-89 90-99  100+  <NA> \r\n##  2384  1884  1117   581   263   107    27     9     0     3     0   104"},{"path":"cleaning-data.html","id":"cut","chapter":"1 Cleaning data","heading":"1.11.1 cut()","text":"can also use base R function cut(), creates categories numeric column. differences age_categories() :need install/load another packageYou can specify whether groups open/closed right/leftYou must provide accurate labels yourselfIf want 0 included lowest group must specify thisThe basic syntax within cut() first provide numeric variable cut (age_years), breaks argument, numeric vector (c()) break points. Using cut(), resulting column ordered factor. used within mutate() (dplyr verb) necessary specify dataframe column name (e.g. linelist$age_years).Create new column age categories (age_cat) cutting numeric age_year column specified break points.Specify numeric vector break pointsDefault behavior cut() lower break values excluded category, upper break values included. opposite behavior age_categories() function.Include 0 lowest category adding include.lowest = TRUEAdd vector customized labels using labels = argumentCheck work cross-tabulation numeric category columns - aware missing valuesBelow detailed description behavior using cut() make age_cat column. Key points:Inclusion/exclusion behavior break pointsCustom category labelsHandling missing valuesCheck work!simple example cut() applied age_years make new variable age_cat :default, categorization occurs right/upper side “open” inclusive (left/lower side “closed” exclusive). default labels use notation “(, B]”, means group include (lower break value), includes B (upper break value). Reverse behavior providing right = TRUE argument.default, categorization occurs right/upper side “open” inclusive (left/lower side “closed” exclusive). default labels use notation “(, B]”, means group include (lower break value), includes B (upper break value). Reverse behavior providing right = TRUE argument.Thus, default “0” values excluded lowest group, categorized NA. “0” values infants coded age 0. change add argument include.lowest = TRUE. , “0” values included lowest group. automatically-generated label lowest category change “(0,B]” “[0,B]”, signifies 0 values included.Thus, default “0” values excluded lowest group, categorized NA. “0” values infants coded age 0. change add argument include.lowest = TRUE. , “0” values included lowest group. automatically-generated label lowest category change “(0,B]” “[0,B]”, signifies 0 values included.Check work!!! Verify age value assigned correct category cross-tabulating numeric category columns. Examine assignment boundary values (e.g. 15, neighboring categories 10-15 15-20).Check work!!! Verify age value assigned correct category cross-tabulating numeric category columns. Examine assignment boundary values (e.g. 15, neighboring categories 10-15 15-20).Reverse break inclusion behavior cut()Lower break values included category (upper break values excluded) argument right = included set TRUE. applied - note values shifted among categories.NOTE: include include.lowest = TRUE argument right = TRUE, extreme inclusion now apply highest break point value category, lowest.Add labelsAs manually written, careful ensure accurate! Check work using cross-tabulation, described . code , manual labels added.Re-labeling NA values cut()cut() automatically label NA values, may want assign label “Missing”. requires extra steps cut() automatically classified new column age_cat class Factor (rigid class limited defined values).First, convert age_cut Factor Character class, flexibility add new character values (e.g. “Missing”). Otherwise encounter error. , use dplyr verb replace_na() replace NA values character value like “Missing”. steps can combined one step, shown .Note Missing added, order categories now wrong (alphabetical considering numbers characters).fix , re-convert age_cat factor, define order levels correctly.seems cumbersome, consider using age_categories() instead, described .Make breaks labelsFor fast way make breaks labels manually, use something like . See [R Basics] page references seq() rep().Read cut() Help page entering ?cut R console.","code":"\n# Create new variable, by cutting the numeric age variable\n# by default, upper break is excluded and lower break excluded from each category\nlinelist <- linelist %>% \n  mutate(\n    age_cat = cut(\n      age_years,\n      breaks = c(0, 5, 10, 15, 20,\n                 30, 50, 70, 100),\n      include.lowest = TRUE         # include 0 in lowest group\n      ))\n\n# tabulate the number of observations per group\ntable(linelist$age_cat, useNA = \"always\")## \r\n##    [0,5]   (5,10]  (10,15]  (15,20]  (20,30]  (30,50]  (50,70] (70,100]     <NA> \r\n##     1412     1214      999      776     1053      798      112       11      104\n# Cross tabulation of the numeric and category columns. \ntable(\"Numeric Values\" = linelist$age_years,   # names specified in table for clarity.\n      \"Categories\"     = linelist$age_cat,\n      useNA = \"always\")                        # don't forget to examine NA values##                    Categories\r\n## Numeric Values      [0,5] (5,10] (10,15] (15,20] (20,30] (30,50] (50,70] (70,100] <NA>\r\n##   0                   127      0       0       0       0       0       0        0    0\r\n##   0.166666666666667     1      0       0       0       0       0       0        0    0\r\n##   0.333333333333333     2      0       0       0       0       0       0        0    0\r\n##   0.5                   2      0       0       0       0       0       0        0    0\r\n##   0.666666666666667     8      0       0       0       0       0       0        0    0\r\n##   0.75                  6      0       0       0       0       0       0        0    0\r\n##   0.833333333333333     4      0       0       0       0       0       0        0    0\r\n##   0.916666666666667     2      0       0       0       0       0       0        0    0\r\n##   1                   264      0       0       0       0       0       0        0    0\r\n##   1.5                   2      0       0       0       0       0       0        0    0\r\n##   2                   247      0       0       0       0       0       0        0    0\r\n##   3                   248      0       0       0       0       0       0        0    0\r\n##   4                   246      0       0       0       0       0       0        0    0\r\n##   5                   253      0       0       0       0       0       0        0    0\r\n##   6                     0    274       0       0       0       0       0        0    0\r\n##   7                     0    220       0       0       0       0       0        0    0\r\n##   8                     0    240       0       0       0       0       0        0    0\r\n##   9                     0    238       0       0       0       0       0        0    0\r\n##   10                    0    242       0       0       0       0       0        0    0\r\n##   11                    0      0     229       0       0       0       0        0    0\r\n##   12                    0      0     233       0       0       0       0        0    0\r\n##   13                    0      0     181       0       0       0       0        0    0\r\n##   14                    0      0     160       0       0       0       0        0    0\r\n##   15                    0      0     196       0       0       0       0        0    0\r\n##   16                    0      0       0     186       0       0       0        0    0\r\n##   17                    0      0       0     150       0       0       0        0    0\r\n##   18                    0      0       0     157       0       0       0        0    0\r\n##   19                    0      0       0     150       0       0       0        0    0\r\n##   20                    0      0       0     133       0       0       0        0    0\r\n##   21                    0      0       0       0     134       0       0        0    0\r\n##   22                    0      0       0       0     126       0       0        0    0\r\n##   23                    0      0       0       0     118       0       0        0    0\r\n##   24                    0      0       0       0     118       0       0        0    0\r\n##   25                    0      0       0       0     119       0       0        0    0\r\n##   26                    0      0       0       0      93       0       0        0    0\r\n##   27                    0      0       0       0      88       0       0        0    0\r\n##   28                    0      0       0       0      96       0       0        0    0\r\n##   29                    0      0       0       0      92       0       0        0    0\r\n##   30                    0      0       0       0      69       0       0        0    0\r\n##   31                    0      0       0       0       0      66       0        0    0\r\n##   32                    0      0       0       0       0      67       0        0    0\r\n##   33                    0      0       0       0       0      84       0        0    0\r\n##   34                    0      0       0       0       0      59       0        0    0\r\n##   35                    0      0       0       0       0      59       0        0    0\r\n##   36                    0      0       0       0       0      43       0        0    0\r\n##   37                    0      0       0       0       0      38       0        0    0\r\n##   38                    0      0       0       0       0      53       0        0    0\r\n##   39                    0      0       0       0       0      43       0        0    0\r\n##   40                    0      0       0       0       0      41       0        0    0\r\n##   41                    0      0       0       0       0      42       0        0    0\r\n##   42                    0      0       0       0       0      31       0        0    0\r\n##   43                    0      0       0       0       0      20       0        0    0\r\n##   44                    0      0       0       0       0      32       0        0    0\r\n##   45                    0      0       0       0       0      19       0        0    0\r\n##   46                    0      0       0       0       0      19       0        0    0\r\n##   47                    0      0       0       0       0      18       0        0    0\r\n##   48                    0      0       0       0       0      25       0        0    0\r\n##   49                    0      0       0       0       0      16       0        0    0\r\n##   50                    0      0       0       0       0      23       0        0    0\r\n##   51                    0      0       0       0       0       0      18        0    0\r\n##   52                    0      0       0       0       0       0      13        0    0\r\n##   53                    0      0       0       0       0       0      11        0    0\r\n##   54                    0      0       0       0       0       0       5        0    0\r\n##   55                    0      0       0       0       0       0      10        0    0\r\n##   56                    0      0       0       0       0       0       7        0    0\r\n##   57                    0      0       0       0       0       0       6        0    0\r\n##   58                    0      0       0       0       0       0       8        0    0\r\n##   59                    0      0       0       0       0       0       6        0    0\r\n##   60                    0      0       0       0       0       0       4        0    0\r\n##   61                    0      0       0       0       0       0       3        0    0\r\n##   62                    0      0       0       0       0       0       1        0    0\r\n##   63                    0      0       0       0       0       0       6        0    0\r\n##   65                    0      0       0       0       0       0       4        0    0\r\n##   66                    0      0       0       0       0       0       3        0    0\r\n##   67                    0      0       0       0       0       0       3        0    0\r\n##   69                    0      0       0       0       0       0       3        0    0\r\n##   70                    0      0       0       0       0       0       1        0    0\r\n##   71                    0      0       0       0       0       0       0        3    0\r\n##   72                    0      0       0       0       0       0       0        2    0\r\n##   76                    0      0       0       0       0       0       0        1    0\r\n##   77                    0      0       0       0       0       0       0        2    0\r\n##   91                    0      0       0       0       0       0       0        1    0\r\n##   94                    0      0       0       0       0       0       0        1    0\r\n##   95                    0      0       0       0       0       0       0        1    0\r\n##   <NA>                  0      0       0       0       0       0       0        0  104\nlinelist <- linelist %>% \n  mutate(\n    age_cat = cut(\n      age_years,\n      breaks = c(0, 5, 10, 15, 20,\n                 30, 50, 70, 100),  # same breaks as above\n      right = FALSE,                # include each *lower* break point\n      include.lowest = TRUE         # include *highest* value *highest* group\n      ))                                                 \n\ntable(linelist$age_cat, useNA = \"always\")## \r\n##    [0,5)   [5,10)  [10,15)  [15,20)  [20,30)  [30,50)  [50,70) [70,100]     <NA> \r\n##     1159     1225     1045      839     1117      844      134       12      104\nlinelist <- linelist %>% \n  mutate(\n    age_cat = cut(\n      age_years,\n      breaks = c(0, 5, 10, 15, 20,\n                 30, 50, 70, 100),  # same breaks as above\n      right = FALSE,                # include each *lower* break point\n      include.lowest = TRUE,        # include *highest* value *highest* group\n      labels = c(\"0-4\", \"5-9\", \"10-14\",\n                 \"15-19\", \"20-29\", \"30-49\",\n                 \"50-69\", \"70-100\")\n      ))\n\ntable(linelist$age_cat, useNA = \"always\")## \r\n##    0-4    5-9  10-14  15-19  20-29  30-49  50-69 70-100   <NA> \r\n##   1159   1225   1045    839   1117    844    134     12    104\nlinelist <- linelist %>% \n  \n  # cut() creates age_cat, automatically of class Factor      \n  mutate(age_cat = cut(age_years,\n                          breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          \n                          right = FALSE,\n                          include.lowest = TRUE,        \n                          labels = c(\"0-4\", \"5-9\", \"10-14\", \"15-19\",\n                                     \"20-29\", \"30-49\", \"50-69\", \"70-100\")),\n         \n         # convert to class Character, and replace NA with \"Missing\"\n         age_cat = replace_na(as.character(age_cat), \"Missing\"))\n\n\ntable(linelist$age_cat, useNA = \"always\")## \r\n##     0-4   10-14   15-19   20-29   30-49     5-9   50-69  70-100 Missing    <NA> \r\n##    1159    1045     839    1117     844    1225     134      12     104       0\nlinelist <- linelist %>% \n  \n  # cut() creates age_cat, automatically of class Factor      \n  mutate(age_cat = cut(age_years,\n                          breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          \n                          right = FALSE,\n                          include.lowest = TRUE,        \n                          labels = c(\"0-4\", \"5-9\", \"10-14\", \"15-19\",\n                                     \"20-29\", \"30-49\", \"50-69\", \"70-100\")),\n         \n         # convert to class Character, and replace NA with \"Missing\"\n         age_cat = replace_na(as.character(age_cat), \"Missing\"),\n         \n         # re-classify age_cat as Factor, with correct level order and new \"Missing\" level\n         age_cat = factor(age_cat, levels = c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-29\",\n                                              \"30-49\", \"50-69\", \"70-100\", \"Missing\")))    \n  \n\ntable(linelist$age_cat, useNA = \"always\")## \r\n##     0-4     5-9   10-14   15-19   20-29   30-49   50-69  70-100 Missing    <NA> \r\n##    1159    1225    1045     839    1117     844     134      12     104       0\n# Make break points from 0 to 90 by 5\nage_seq = seq(from = 0, to = 90, by = 5)\nage_seq\n\n# Make labels for the above categories, assuming default cut() settings\nage_labels = paste0(age_seq+1, \"-\", age_seq + 5)\nage_labels\n\n# check that both vectors are the same length\nlength(age_seq) == length(age_labels)"},{"path":"cleaning-data.html","id":"quantile-breaks","chapter":"1 Cleaning data","heading":"Quantile breaks","text":"Make breaks quantile(). stats package comes base R.can use break points age_categories() cut().","code":"\nage_quantiles <- quantile(linelist$age_years, c(0, .25, .50, .75, .90, .95), na.rm=T)\nage_quantiles##  0% 25% 50% 75% 90% 95% \r\n##   0   6  13  24  35  42\n# to return only the numbers use unname()\nage_quantiles <- unname(age_quantiles)\nage_quantiles## [1]  0  6 13 24 35 42"},{"path":"cleaning-data.html","id":"case_when","chapter":"1 Cleaning data","heading":"case_when()","text":"dplyr function case_when() can also used create numeric categories.Allows explicit setting break point inclusion/exclusionAllows designation label NA values one stepMore complicated codeAllow flexibility include variables logicIf using case_when() please review proper use described earlier page, logic order assignment important understand avoid errors.CAUTION: case_when() right-hand side values must class. Thus, categories character values (e.g. “20-30 years”) designated outcome NA age values must also character (either “Missing”, special NA_character_ instead NA).need designate column factor (wrapping case_when() function factor()) provide ordering factor levels using levels = argument close case_when() function. using cut(), factor ordering levels done automatically.now view results table new column:","code":"\nlinelist <- linelist %>% \n  mutate(\n    age_cat = factor(case_when(\n      # provide the case_when logic and outcomes\n      age_years >= 0 & age_years < 5     ~ \"0-4\",          \n      age_years >= 5 & age_years < 10    ~ \"5-9\",\n      age_years >= 10 & age_years < 15   ~ \"10-14\",\n      age_years >= 15 & age_years < 20   ~ \"15-19\",\n      age_years >= 20 & age_years < 30   ~ \"20-29\",\n      age_years >= 30 & age_years < 50   ~ \"30-49\",\n      age_years >= 50 & age_years < 70   ~ \"50-69\",\n      age_years >= 45 & age_years <= 100 ~ \"70-100\",\n      is.na(age_years)                   ~ \"Missing\",      # if age_years is missing\n      TRUE                               ~ \"Check value\"), # trigger for review\n      \n      # define the levels order for factor()\n      levels = c(\"0-4\",\"5-9\", \"10-14\",\n                 \"15-19\", \"20-29\", \"30-49\",\n                 \"50-69\", \"70-100\", \"Missing\", \"Check value\")))\ntable(linelist$age_cat, useNA = \"always\")## \r\n##         0-4         5-9       10-14       15-19       20-29       30-49       50-69      70-100     Missing Check value        <NA> \r\n##        1159        1225        1045         839        1117         844         134          12         104           0           0"},{"path":"cleaning-data.html","id":"add-to-pipe-chain-2","chapter":"1 Cleaning data","heading":"Add to pipe chain","text":", code create two categorical age columns added cleaning pipe chain:","code":"\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist <- linelist_raw %>%\n    \n    # standardize column name syntax\n    janitor::clean_names() %>% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %>% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %>% \n  \n    # de-duplicate\n    distinct() %>% \n\n    # add column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     \n\n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age)) %>% \n    \n    # add column: delay to hospitalisation\n    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% \n    \n    # clean values of hospital column\n    mutate(hospital = recode(hospital,\n                      # OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      )) %>% \n    \n    mutate(hospital = replace_na(hospital, \"Missing\")) %>% \n\n    # create age_years column (from age and age_unit)\n    mutate(age_years = case_when(\n          age_unit == \"years\" ~ age,\n          age_unit == \"months\" ~ age/12,\n          is.na(age_unit) ~ age,\n          TRUE ~ NA_real_)) %>% \n  \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    ###################################################   \n    mutate(\n          # age categories: custom\n          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),\n        \n          # age categories: 0 to 85 by 5s\n          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5)))"},{"path":"cleaning-data.html","id":"add-rows","chapter":"1 Cleaning data","heading":"1.12 Add rows","text":"Remember column must contain values one class (either character, numeric, logical, etc.). adding row requires nuance maintain .Use ... place row want add. .= 3 put new row 3rd row. default behavior add row end. Columns specified left empty.new row number may look strange (“…23”) row numbers pre-existing rows changed. using command twice, examine/test insertion carefully.class provide see error like :(inserting row date value, remember wrap date function .Date() like .Date(\"2020-10-10\")).","code":"\nlinelist <- linelist %>% \n  add_row(row_num = 666,\n          case_id = \"abc\",\n          generation = 4,\n          `infection date` = as.Date(\"2020-10-10\"),\n          .before = 2)Error: Can't combine ..1$infection date <date> and ..2$infection date <character>."},{"path":"cleaning-data.html","id":"filter-rows","chapter":"1 Cleaning data","heading":"1.13 Filter rows","text":"typical early cleaning step filter dataframe specific rows using dplyr verb filter(). Within filter(), give logic must TRUE row dataset kept.shown filter rows based simple complex logical conditions, filter/subset rows stand-alone command base R","code":""},{"path":"cleaning-data.html","id":"a-simple-filter","chapter":"1 Cleaning data","heading":"A simple filter()","text":"simple example re-defines dataframe linelist , filtered rows meet logical condition. rows logical statement within parentheses TRUE kept.case, logical statement !.na(case_id), asking whether value column case_id missing (NA). Thus, rows case_id missing kept.filter applied, number rows linelist 6479.filter applied, number rows linelist 6474.","code":"\nlinelist <- linelist %>% \n  filter(!is.na(case_id))  # keep only rows where case_id is not missing"},{"path":"cleaning-data.html","id":"a-complex-filter","chapter":"1 Cleaning data","heading":"A complex filter()","text":"complex example using filter():","code":""},{"path":"cleaning-data.html","id":"examine-the-data","chapter":"1 Cleaning data","heading":"Examine the data","text":"simple one-line command create histogram onset dates. See second smaller outbreak 2012-2013 also included dataset. analyses, want remove entries earlier outbreak.","code":"\nhist(linelist$date_onset, breaks = 50)"},{"path":"cleaning-data.html","id":"how-filters-handle-missing-numeric-and-date-values","chapter":"1 Cleaning data","heading":"How filters handle missing numeric and date values","text":"Can just filter date_onset rows June 2013? Caution! Applying code filter(date_onset > .Date(\"2013-06-01\"))) remove rows later epidemic missing date onset!DANGER: Filtering greater (>) less (<) date number can remove rows missing values (NA)! NA treated infinitely large small.","code":""},{"path":"cleaning-data.html","id":"design-the-filter","chapter":"1 Cleaning data","heading":"Design the filter","text":"Examine cross-tabulation make sure exclude correct rows:criteria can filter remove first outbreak (2012 & 2013) dataset? see :first epidemic occurred Hospital , Hospital B, also 10 cases Port Hospital.Hospitals & B cases second epidemic, Port Hospital .want exclude:586 rows onset 2012 2013 either hospital , B, Port:\r\nExclude 586 rows onset 2012 2013\r\nExclude 0 rows Hospitals & B missing onset dates\r\nexclude 0 rows missing onset dates.\r\nExclude 586 rows onset 2012 2013Exclude 0 rows Hospitals & B missing onset datesDo exclude 0 rows missing onset dates.start linelist nrow(linelist). filter statement:re-make cross-tabulation, see Hospitals & B removed completely, 10 Port Hospital cases 2012 & 2013 removed, values - just wanted.Multiple statements can included within one filter command (separated commas), can always pipe separate filter() command clarity.Note: readers may notice easier just filter date_hospitalisation 100% complete missing values. true. date_onset used purposes demonstrating complex filter.","code":"\ntable(Hospital  = linelist$hospital,                     # hospital name\n      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset\n      useNA     = \"always\")                              # show missing values##                                       YearOnset\r\n## Hospital                               2012 2013 2014 2015 <NA>\r\n##   Central Hospital                        0    0  359   95    0\r\n##   Hospital A                            244   44    0    0    0\r\n##   Hospital B                            253   35    0    0    0\r\n##   Military Hospital                       0    0  698  198    0\r\n##   Missing                                 0    0 1159  310    0\r\n##   Other                                   0    0  713  172    0\r\n##   Port Hospital                           7    3 1423  339    0\r\n##   St. Mark's Maternity Hospital (SMMH)    0    0  331   91    0\r\n##   <NA>                                    0    0    0    0    0\nlinelist <- linelist %>% \n  # keep rows where onset is after 1 June 2013 OR where onset is missing and it was a hospital OTHER than Hospital A or B\n  filter(date_onset > as.Date(\"2013-06-01\") | (is.na(date_onset) & !hospital %in% c(\"Hospital A\", \"Hospital B\")))\n\nnrow(linelist)## [1] 5888\ntable(Hospital  = linelist$hospital,                     # hospital name\n      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset\n      useNA     = \"always\")                              # show missing values##                                       YearOnset\r\n## Hospital                               2014 2015 <NA>\r\n##   Central Hospital                      359   95    0\r\n##   Military Hospital                     698  198    0\r\n##   Missing                              1159  310    0\r\n##   Other                                 713  172    0\r\n##   Port Hospital                        1423  339    0\r\n##   St. Mark's Maternity Hospital (SMMH)  331   91    0\r\n##   <NA>                                    0    0    0"},{"path":"cleaning-data.html","id":"filter-stand-alone-command","chapter":"1 Cleaning data","heading":"1.13.1 Filter stand-alone command","text":"Filtering can also done stand-alone command (part pipe chain). Like dplyr verbs, case first argument must dataset .can also use base R subset using square brackets reflect [rows, columns] want retain.TIP: Use bracket-subset syntax View() quickly review records.","code":"\n# dataframe <- filter(dataframe, condition(s) for rows to keep)\n\nlinelist <- filter(linelist, !is.na(case_id))\n# dataframe <- dataframe[row conditions, column conditions] (blank means keep all)\n\nlinelist <- linelist[!is.na(case_id), ]"},{"path":"cleaning-data.html","id":"filter-to-quickly-review-observations","chapter":"1 Cleaning data","heading":"1.13.2 Filter to quickly review observations","text":"base R syntax can handy want quickly view subset rows columns. Use base R View() command (note capital “V”) around [] subset want see. result appear dataframe RStudio viewer panel. example, want review onset hospitalization dates 3 specific cases:View linelist viewer panel:View specific data three cases:Note: command can also written dplyr verbs filter() select() :","code":"\nView(linelist)\nView(linelist[linelist$case_id %in% c(\"11f8ea\", \"76b97a\", \"47a5f5\"), c(\"date_onset\", \"date_hospitalisation\")])\nView(linelist %>%\n       filter(case_id %in% c(\"11f8ea\", \"76b97a\", \"47a5f5\")) %>%\n       select(date_onset, date_hospitalisation))"},{"path":"cleaning-data.html","id":"add-to-pipe-chain-3","chapter":"1 Cleaning data","heading":"1.13.3 Add to pipe chain","text":"","code":"\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist <- linelist_raw %>%\n    \n    # standardize column name syntax\n    janitor::clean_names() %>% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %>% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %>% \n  \n    # de-duplicate\n    distinct() %>% \n\n    # add column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     \n\n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age)) %>% \n    \n    # add column: delay to hospitalisation\n    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% \n    \n    # clean values of hospital column\n    mutate(hospital = recode(hospital,\n                      # OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      )) %>% \n    \n    mutate(hospital = replace_na(hospital, \"Missing\")) %>% \n\n    # create age_years column (from age and age_unit)\n    mutate(age_years = case_when(\n          age_unit == \"years\" ~ age,\n          age_unit == \"months\" ~ age/12,\n          is.na(age_unit) ~ age,\n          TRUE ~ NA_real_)) %>% \n  \n    mutate(\n          # age categories: custom\n          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),\n        \n          # age categories: 0 to 85 by 5s\n          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5))) %>% \n    \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    ###################################################\n    filter(\n          # keep only rows where case_id is not missing\n          !is.na(case_id),  \n          \n          # also filter to keep only the second outbreak\n          date_onset > as.Date(\"2013-06-01\") | (is.na(date_onset) & !hospital %in% c(\"Hospital A\", \"Hospital B\")))"},{"path":"cleaning-data.html","id":"row-wise-calculations","chapter":"1 Cleaning data","heading":"1.14 Row-wise calculations","text":"want perform calculation within row, can use rowwise() dplyr. See vignette row-wise calculationsFor example, code applies rowwise() creates new column sums number symptoms per case:","code":"\nlinelist <- linelist %>%\n  rowwise() %>%\n  mutate(num_symptoms = sum(c(fever, chills, cough, aches, vomit) == \"yes\"))"},{"path":"cleaning-data.html","id":"transformations-across-multiple-columns","chapter":"1 Cleaning data","heading":"1.15 Transformations across multiple columns","text":"Often write concise code want apply transformation multiple columns . transformation can applied multiple variables using across() function package dplyr (contained within tidyverse package).across() can used dplyr verb, commonly mutate(), filter(), summarise().\r\nacross() allows specify columns want function apply . specify columns, can name indvidually, use helped functions.transformation .character() applied specific columns named within across(). Note functions across() written without parentheses ( )helpers available assist specifying columns:everything() - columns mentionedlast_col() - last columnwhere() - applies function columns selects TRUEstarts_with() - matches specified prefix. Example: select(starts_with(\"date\"))ends_with() - matches specified suffix. Example: select(ends_with(\"_end\"))contains() - columns containing character string. Example: select(contains(\"time\"))matches() - apply regular expression (regex). Example: select(contains(\"[pt]al\"))num_range() -any_of() - matches column named. Useful name might exist. Example: any_of(date_onset, date_death, cardiac_arrest)example one change columns character class:Columns name contains string “date” (note placement commas parentheses):, want mutate columns class POSIXct (datetime class shows timestamps) - function .POSIXct() evaluates TRUE. want apply function .Date() column convert class Date.Note within across() also use function ()Note .POSIXct package lubridate. similar functions (.character(), .numeric(), .logical()) base RHere online resources using across(): creator Hadley Wickham’s thoughts/rationale","code":"\nlinelist <- linelist %>% \n  mutate(across(c(temp, ht_cm, wt_kg), as.character))\n#to change all columns to character class\nlinelist <- linelist %>% \n  mutate(across(everything(), as.character))\n#to change all columns to character class\nlinelist <- linelist %>% \n  mutate(across(contains(\"date\"), as.character))\nlinelist <- linelist %>% \n  mutate(across(where(lubridate::is.POSIXct), as.Date))## [1] \"2014-04-17\"## [1] \"2014-04-19\""}]
