
# Time series analysis {.tabset .tabset-fade}  

<!-- ======================================================= -->
## Overview {.tabset .tabset-fade .tabset-pills}

This tab demonstrates the use of several packages for time series analysis. 
It primarily relies on packages from the [**tidyverts**](https://tidyverts.org/) 
family, but will also use the RECON [**trending**](https://github.com/reconhub/trending) 
package to fit models that are more appropriate for infectious disease epidemiology. 

1.  Time series data 
2.  Descriptive analysis 
3.  Fitting regressions
4.  Relation of two time series 
5.  Interrupted time series
6.  Outbreak detection


<!-- ======================================================= -->
## Preparation {.tabset .tabset-fade .tabset-pills}

### Packages

This code chunk shows the loading of packages required for the analyses.

```{r load_packages}
pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets
               slider,       # for calculating moving averages
               imputeTS,     # for filling in missing values
               feasts,        # for time series decomposition and autocorrelation
               forecast,     # fit sin and cosin terms to data (note: must load after fable)
               trending      # fit and assess models 
               )
``` 

### Load data

The example dataset used in this section:

-   Weekly counts of campylobacter cases reported in Germany between 2001 and 2011. 

This dataset is a reduced version of the dataset available in the [**surveillance**](https://cran.r-project.org/web/packages/surveillance/) package. 
(for details load the surveillance package and see `?campyDE`)

The dataset is imported using the `import()` function from the *rio* package. See the *page on importing data* for various ways to import data.

```{r read_data_hide, echo=F}
# import the counts into R
counts <- rio::import(here::here("data", "campylobacter_germany.xlsx"))

```

```{r read_data_show, eval=F}
# import the linelist
counts <- rio::import("campylobacter_germany.xlsx")
```

The first 10 rows of the counts are displayed below.

```{r inspect_data, message=FALSE, echo=F}
# display the counts data as a table
DT::datatable(head(counts, 10), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T) )
```

### Clean data

The below makes sure that the date column is in the appropriate format. 
For this tab we will be using the **tsibble** package and so the `yearweek` 
function will be used to create a calendar week variable. There are several other
ways of doing this (see **handling dates page** for details), however for 
time series its best to keep within one framework. 

```{r clean_data}

## ensure the date column is in the appropriate format
counts$date <- as.Date(counts$date)

## create a calendar week variable 
## fitting ISO definitons of weeks starting on a monday
counts <- counts %>% 
     mutate(epiweek = yearweek(date, week_start = 1))

```


<!-- ======================================================= -->
## Time series data {.tabset .tabset-fade .tabset-pills}

There are a number of different packages for structuring and handling time series
data. As said, we will focus on the **tidyverts** family of packages and so will
use the **tsibble** package to define our time series object. Having a data set
defined as a time series object means it is much easier to structure our analysis. 

To do this we use the `tsibble` function and specify the "index", i.e. the variable
specifying the time unit of interest. In our case this is the *epiweek* variable. 

If we had a data set with weekly counts by province, for example, we would also 
be able to specify the grouping variable using the "key" argument. 
This would allow us to do analysis for each group. 


```{r ts_object}

## define time series object 
counts <- tsibble(counts, index = epiweek)

```

Looking at `class(counts)` tells you that on top of being a tidy data frame 
("tbl_df", "tbl", "data.frame"), it has the additional properties of a time series
data frame ("tbl_ts"). 

You can take a quick look at your data by using ggplot2. We see from the plot that
there is a clear seasonal pattern, and that there are no missings. However, there
seems to be an issue with reporting at the beginning of each year; cases drop 
in the last week of the year and then increase for the first week of the next year. 

```{r basic_plot}

## plot a line graph of cases by week
ggplot(counts, aes(x = epiweek, y = case)) + 
     geom_line()

```


<span style="color: red;">**_DANGER:_** Most datasets aren't as clean as this example. 
You will need to check for duplicates and missings as below. </span>

<!-- ======================================================= -->
### duplicates {.tabset .tabset-fade .tabset-pills}

**tsibble** does not allow duplicate observations. So each row will need to be
unique, or unique within the group (`key` variable). 
The package has a few functions that help to identify duplicates. These include
`are_duplicated` which gives you a TRUE/FALSE vector of whether the row is a 
duplicate, and `duplicates` which gives you a data frame of the duplicated rows. 

See **de-duplication page** for details of how to select rows you want. 

```{r duplicates, eval = FALSE}

## get a vector of TRUE/FALSE whether rows are duplicates
are_duplicated(counts, index = epiweek) 

## get a data frame of any duplicated rows 
duplicates(counts, index = epiweek) 

```

<!-- ======================================================= -->
### Missings {.tabset .tabset-fade .tabset-pills}

We saw from our brief inspection above that there are no missings, but we also 
saw there seems to be a problem with reporting delay around new year. 
One way to address this problem could be to set these values to missing and then 
to impute values. The simplest form of time series imputation is to draw
a straight line between the last non-missing and the next non-missing value. 
To do this we will use the **imputeTS** package function `na_interpolation`. 
See **missing data page** for other options for imputation.  
Another alternative would be to calculating a moving average, to try and smooth
over these apparent reporting issues (see next section). 

```{r missings}

## create a variable with missings instead of weeks with reporting issues
counts <- counts %>% 
     mutate(case_miss = if_else(
          ## if epiweek contains 52, 53, 1 or 2
          str_detect(epiweek, "W51|W52|W53|W01|W02"), 
          ## then set to missing 
          NA_real_, 
          ## otherwise keep the value in case
          case
     ))

## alternatively interpolate missings by linear trend 
## between two nearest adjacent points
counts <- counts %>% 
  mutate(case_int = na_interpolation(case_miss)
         )

## to check what values have been imputed compared to the original
plotNA.imputations(counts$case_miss, counts$case_int)

```




<!-- ======================================================= -->
## Descriptive analysis {.tabset .tabset-fade .tabset-pills}



<!-- ======================================================= -->
### Moving averages {.tabset .tabset-fade .tabset-pills}

If data is very noisy (counts jumping up and down) then it can be helpful to 
calculate a moving average. In the example below, for each week we calculate the 
average number of cases from the four previous weeks. This smooths the data, to 
make it more interpretable. In our case this does not really add much, so we will
stick to the interpolated data for further analyis. 
See the **moving averages** page for more detail. 

```{r moving_averages}

## create a moving average variable (deals with missings)
counts <- counts %>% 
     ## create the ma_4w variable 
     ## slide over each row of the case variable
     mutate(ma_4wk = slide_dbl(case, 
                               ## for each row calculate the name
                               ~ mean(.x, na.rm = TRUE),
                               ## use the four previous weeks
                               .before = 4))

## make a quick visualisation of the difference 
ggplot(counts, aes(x = epiweek)) + 
     geom_line(aes(y = case)) + 
     geom_line(aes(y = ma_4wk), colour = "red")

```


<!-- ======================================================= -->
### Periodicity {.tabset .tabset-fade .tabset-pills}

```{r periodogram}

## x is a dataset
## counts is variable with count data or rates within x 
## start_week is the first week in your dataset
## period is how many units in a year 
## output is whether you want return spectral periodogram or the peak weeks
  ## "periodogram" or "weeks"
periodogram <- function(x, 
                        counts, 
                        start_week = c(2002, 1), 
                        period = 52, 
                        output = "weeks") {
  

    ## make sure is not a tsibble, filter to project and only keep columns of interest
    prepare_data <- dplyr::as_tibble(x)
    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
    prepare_data <- dplyr::select(prepare_data, {{counts}})
    
    ## create an intermediate "zoo" time series to be able to use with spec.pgram
    zoo_cases <- zoo::zooreg(prepare_data, 
                             start = start_week, frequency = period)
    
    ## get a spectral periodogram not using fast fourier transform 
    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)
    
    ## return the peak weeks 
    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period
    
    if (output == "weeks") {
      periodo_weeks
    } else {
      periodo
    }
    
}

## get spectral periodogram for extracting weeks with the highest frequencies 
## (checking of seasonality) 
periodo <- periodogram(counts, 
                       case_int, 
                       start_week = c(2002, 1),
                       output = "periodogram")

## pull spectrum and frequence in to a dataframe for plotting
periodo <- data.frame(periodo$freq, periodo$spec)

## plot a periodogram showing the most frequently occuring periodicity 
ggplot(data = periodo, 
                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + 
  geom_line() + 
  labs(x = "Period (Weeks)", y = "Log(density)")


## get a vector weeks in ascending order 
peak_weeks <- periodogram(counts, 
                          case_int, 
                          start_week = c(2002, 1), 
                          output = "weeks")

```

<span style="color: black;">**_NOTE:_** It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) </span>

<!-- ======================================================= -->
### Decomposition {.tabset .tabset-fade .tabset-pills}

Classical decomposition is used to break a time series down several parts, which
when taken together make up for the pattern you see. 
These different parts are the trend-cycle (the long-term direction of the data), 
the seasonality (repeating patterns) and the random (what is left after removing 
trend and season). 

```{r decomposition}

## decompose the counts dataset 
counts %>% 
  # using an additive classical decomposition model
  model(classical_decomposition(case_int, type = "additive")) %>% 
  ## extract the important information from the model
  components() %>% 
  ## generate a plot 
  autoplot()

```

<!-- ======================================================= -->
### Autocorrelation {.tabset .tabset-fade .tabset-pills}

Autocorrelation tells you about the relation between the counts of each week 
and the weeks before it (called lags). 
Using the `ACF` function, we can produce a plot which shows us a number of lines 
for the relation at different lags. Where the lag is 0 (x = 0), this line would 
always be 1 as it shows the relation between an observation and itself (not shown here). 
The first line shown here (x = 1) shows the relation between each observation 
and the observation before it (lag of 1), the second shows the relation between 
each observation and the observation before last (lag of 2) and so on until lag of
52 which shows the relation between each observation and the observation from 1 
year (52 weeks before). 
Using the `PACF` function (for partial autocorrelation shows the same type of relation 
but adjusted for all other weeks between. This is less informative for determining
periodicity. 

```{r autocorrelation}

## using the counts dataset
counts %>% 
  ## calculate autocorrelation using a full years worth of lags
  ACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

## using the counts data set 
counts %>% 
  ## calculate the partial autocorrelation using a full years worth of lags
  PACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

```

You can formally test the null hypothesis of independence in a time series (i.e. 
that it is not autocorrelated) using the Ljung-Box test (in the **stats** package). 
A significant p-value suggests that there is autocorrelation in the data.

```{r ljung_box}

## test for independance 
Box.test(counts$case_int, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Fitting regressions {.tabset .tabset-fade .tabset-pills}

It is possible to fit a large number of different regressions to a time series, 
however, here we will demonstrate how to fit a negative binomial regression - as 
this is often the most appropriate for counts data in infectious diseases. 

<!-- ======================================================= -->
### Fourier terms {.tabset .tabset-fade .tabset-pills}

Fourier terms are the equivalent of sin and cosin curves. The difference is that 
these are fit based on finding the most appropriate combination of curves to explain
your data. 
If only fitting one fourier term, this would be the equivalent of fitting a sin 
and a cosin for your most frequently occurring lag seen in your periodogram (in our 
case 52 weeks). We use the `fourier` function from the **forecast** package. 
In the below code we assign using the `$`, as `fourier` returns two columns (one 
for sin one for cosin) and so these are added to the dataset as a list, called 
"fourier" - but this list can then be used as a normal variable in regression. 

```{r fourier}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  fourier(K = 1)
```

<!-- ======================================================= -->
### Negative binomial {.tabset .tabset-fade .tabset-pills}



```{r nb_reg, warning = FALSE}

model <- glm_nb_model(case_int ~
               epiweek +
               fourier)

fitted_model <- fit(model, counts)

observed <- predict(fitted_model)

ggplot() + 
  geom_line(data = observed, aes(x = epiweek, y = estimate), col = "Red") + 
  geom_ribbon(data = observed, aes(x = epiweek, ymin = lower_pi, ymax = upper_pi), 
              alpha = 0.25) + 
  geom_line(data = counts, aes(x = epiweek, y = case_int), col = "black") + 
  theme_classic()
  



```

<!-- ======================================================= -->
### Residuals {.tabset .tabset-fade .tabset-pills}




<!-- ======================================================= -->
## Resources {.tabset .tabset-fade .tabset-pills}

This tab should stay with the name "Resources".
Links to other online tutorials or resources.





