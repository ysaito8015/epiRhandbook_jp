---
knit: "bookdown::render_book"
title: "The Epidemiologist R Handbook"
author: "the handbook team"
description: "This is a R reference manual for applied epidemiologists and public health practitioners."  
date: "`r Sys.Date()`"
#url: 'https://github.com/nsbatra/Epi_R_handbook'
github-repo: nsbatra/Epi_R_handbook
#twitter-handle: 
#cover-image: images/R_Handbook_Logo.png
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
---


#  {-}

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "R Handbook Logo.png"))
```

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<span style="color: red;">**THIS IS A DRAFT**.</span>

<span style="color: orange;">**IF YOU ARE REVIEWING THIS BOOK, PLEASE PROVIDE FEEDBACK FOR EACH PAGE AT THIS [LINK](https://forms.gle/4RNdRRLGx67xW9yq9)**</span>


<!-- ======================================================= -->
## About this handbook {-}

<span style="color: brown;">**This is a free open-access R reference manual for applied epidemiologists and public health practitioners.**</span>

**This book strives to:**  

* Serve as a quick reference manual - not as a textbook or comprehensive R training  
* Address common epidemiological problems via task-centered examples  
* Be accessible in settings with low internet-connectivity via an [offline version](https://github.com/nsbatra/Epi_R_handbook/tree/master/offline_long) (instructions below)  

**What challenges does this book try to address?**  

* Many epidemiologists are transitioning to R from SAS, STATA, SPSS, Excel, or other software  
* Epidemiologists should not need to spend hours searching online for code relevant to the common epi user  
* Epidemiologists sometimes work in low internet-connectivity environments and have limited support  

**How is this different than other R books?**  

* It is written by epidemiologists, for epidemiologists - leveraging experience in local, national, academic, and emergency settings  
* It provides examples of epidemic curves, transmission chains, epidemic modeling and projections, age and sex pyramids and standardization, record matching, outbreak detection, survey analysis, causal diagrams, survival analysis, GIS basics, phylogenetic trees, automated reports, etc...  



<!-- ======================================================= -->
## How to read this handbook {-} 

**Online version**  

* Search via the search box above the Table of Contents 
* Click the "copy" icons to copy code  
* See the "Resources" section of each page for further resources  
* To download data and "follow-along", see the [Download book and data] page  

**Offline version**  

To download the offline version, follow step-by-step instructions in the [Download book and data] page.  

**Languages**  

We are actively seeking to translate this book into languages other than English. If you can help, please contact us at **epiRhandbook@gmail.com**.  


<!-- ======================================================= -->
## Edit or contribute {-}

Want to share how you use this book? Want to offer a fix or addition?  
Email us at **epiRhandbook@gmail.com**. We welcome your comments and suggestions. 

You can also submit an issue or pull request at our [Github repository](https://github.com/nsbatra/R_epi_handbook), or provide structured feedback via this [Google survey](https://forms.gle/4RNdRRLGx67xW9yq9).  



<!-- ======================================================= -->
## Acknowledgements {-}  


### Contributors {-}  

This book is produced by a collaboration of epidemiologists from around the world, drawing upon experiences with organizations including local/state/provincial/national health departments and ministries, the World Health Organization (WHO), MSF (Médecins Sans Frontières / Doctors without Borders), hospital systems, and academic institutions.

**Editor-in-Chief:** Neale Batra 

**Core team:** Neale Batra, Alex Spina, Amrish Baidjoe, Pat Keating, Henry Laurenson-Schafer, Finlay Campbell  

**Authors**: Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, Isaac Florence, Natalie Fischer, Aminata Ndiaye, Liza Coyer, Jonny Polonski, Yurie Izawa, Daniel Molling, Sara Hollis, Isha Berry, Wen Lin  

**Reviewers**:  

**Advisers**:     


### Funding and programmatic support {-}  

This handbook is **not** an approved product of any specific organization. Although we strive for accuracy, we provide no guarantee of the content in this book.  

The handbook project received funding via a COVID-19 emergency capacity-building grant from Training Programs in Epidemiology and Public Health Interventions Network ([TEPHINET](https://www.tephinet.org/)). *This handbook was supported by Cooperative Agreement number NU2GGH001873, funded by the Centers for Disease Control and Prevention through TEPHINET, a program of The Task Force for Global Health. Its contents are solely the responsibility of the authors and do not necessarily represent the official views of the Centers for Disease Control and Prevention, the Department of Health and Human Services, The Task Force for Global Health, Inc. or TEPHINET.*

Programmatic support was provided by the EPIET Alumni Network ([EAN](https://epietalumni.net/)) and also MSF's Manson Unit.  



### Inspiration {-}  

The multitude of tutorials and vignettes that provided knowledge for development of handbook content are credited within their respective pages.  

More generally, the following sources provided inspiration and laid the groundwork for this handbook:  
[The "R4Epis" project](https://r4epis.netlify.app/) (a collaboration between MSF and RECON)  
[R Epidemics Consortium (RECON)](https://www.repidemicsconsortium.org/)  
[R for Data Science book (R4DS)](https://r4ds.had.co.nz/)  
[bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/)  
[Netlify](https://www.netlify.com) hosts this website  


### Image credits {-}  

Images in logo (from US CDC Public Health Image Library):  
[2013 Yemen looking for mosquito breeding sites](https://phil.cdc.gov/Details.aspx?pid=19623)  
[Ebola virus](https://phil.cdc.gov/Details.aspx?pid=23186)  
[Survey in Rajasthan](https://phil.cdc.gov/Details.aspx?pid=19838)  



### Terms of Use and License {-}  

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:index.Rmd-->

# (PART) About this book {-}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/cat_about_book.Rmd-->

# Editorial and technical notes { }

In this page we describe the philosophical approach, style, and specific editorial decisions made during the creation of this handbook.  


## Approach and style

The potential audience for this book is large. It will surely be used by people very new to R, and also by experienced R users looking for best practices and tips. So it must be both accessible and succinct. Therefore, our approach was to provide *just enough* text explanation that someone very new to R can apply the code and follow what is the code is doing.  

A few other points:  

* This is a code reference book accompanied by relatively brief examples - *not* a thorough textbook on R or data science  
* This is a *R handbook* for use within applied epidemiology - not a manual on the methods or science of applied epidemiology  



### Packages {-}

**So many choices**  

One of the most challenging aspects of learning R is knowing which R package to use for a given task. It is a common occurrence to struggle through a task only later to realize - hey, there's an R package that does all that in one command line!  

In this handbook, we try to offer you at least two ways to complete each task: one tried-and-true method (probably in **base** R or **tidyverse**) and one special R package that is custom-built for that purpose. We want you to have a couple options in case you can't download a given package or it otherwise does not work for you.  

In choosing which packages to use, we prioritized R packages and approaches that have been tested and vetted by the community, minimize the number of packages used in a typical work session, that are stable (not changing very often), and that accomplish the task simply and cleanly  

This handbook generally prioritizes R packages and functions from the **tidyverse**. Tidyverse is a collection of R packages designed for data science that share underlying grammar and data structures. All tidyverse packages can be installed or loaded via the **tidyverse** package. Read more at the [tidyverse website](https://www.tidyverse.org/).  

When applicable, we also offer code options using **base** R - the packages and functions that come with R at installation. This is because we recognize that some of this book's audience may not have reliable internet to download extra packages.  

**Linking functions to packages explicitly**

It is often frustrating in R tutorials when a function is shown in code, but you don't know which package it is from! We try to avoid this situation.  

In the narrative text, package names are written in bold (e.g. **dplyr**) and functions are written like this: `mutate()`. We strive to be explicit about which package a function comes from, either by referencing the package in nearby text or by specifying the package explicitly in the code like this: `dplyr::mutate()`. It may look redundant, but we are doing it on purpose.  

See the page on [R basics] to learn more about packages and functions.  


### Code style {-}

In the handbook, we frequently utilize "new lines", making our code appear "long". We do this for a few reasons:  

* We can write explanatory comments with `#` that are adjacent to each little part of the code  
* Generally, longer (vertical) code is easier to read  
* It is easier to read on a narrow screen (no sideways scrolling needed)  
* From the indentations, it can be easier to know which arguments belong to which function  

As a result, code that *could* be written like this:  

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>%  # group rows by hospital
  slice_max(date, n = 1, with_ties = F) # if there's a tie (of date), take the first row
```

...is written like this:  

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>% # group rows by hospital
  slice_max(
    date,                # keep row per group with maximum date value 
    n = 1,               # keep only the single highest row 
    with_ties = F)       # if there's a tie (of date), take the first row
```

R code is generally not affected by new lines or indentations. When writing code, if you initiate a new line after a comma it will apply automatic indentation patterns. 

We also use lots of spaces (e.g. `n = 1` instead of `n=1`) because it is easier to read. Be kind to the people reading your code!  



### Notes {-} 

Here are the types of notes you may encounter in the handbook:  

<span style="color: black;">**_NOTE:_** This is a note</span>  
<span style="color: darkgreen;">**_TIP:_** This is a tip.</span>  
<span style="color: orange;">**_CAUTION:_** This is a cautionary note.</span>  
<span style="color: red;">**_DANGER:_** This is a warning.</span>  



## Editorial decisions  

Below, we track significant editorial decisions around package and function choice. If you disagree or want to offer a new tool for consideration, please join/start a conversation on our Github page.    


**Table of package, function, and other editorial decisions**  



Subject           |     Considered      |   Outcome           |    Brief rationale   
----------------- | --------------------|---------------------|--------------------------------------------------    
Epiweeks          | aweek, lubridate    | lubridate           | consistency, package maintenance prospects  
ggplot labels     | labs(), ggtitle(), ylab(), xlab() | labs() | all labels in one place, simplicity |
factors           | factor() (base), as_factor() (forcats) | as_factor()       | all factor functions in one package  




## Session info (R, RStudio, packages)  

Below is the information on the versions of R, RStudio, and R packages used during this rendering of the Handbook.  


```{r}
sessioninfo::session_info()
```




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/editorial_style.Rmd-->

# Download book and data  


## Download offline handbook  

To download the offline version of this handbook please follow these steps. It will download as an HTML file. You can view the file in your web browser even if you do not have internet access.  

Note: The file is large (>40MB). When you open it, it may take several minutes for the images and the Table of Contents to load/appear. Also, the offline handbook displays as one long page, so please search for specific content using with Ctrl+f (Cmd-f).    

1) Click this link to go [the html file](https://github.com/nsbatra/Epi_R_handbook/tree/master/offline_long) in our Github repository's "offline_long" folder.  

```{r out.height = "50%", out.width= "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_offline.png"))
```

2) Click the "Download" button. A window will open with HTML source code.  

```{r out.height = "50%", out.width= "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_offline_button.png"))
```

3) "Save As" the webpage, via right-click (windows) or Cmd-s (mac) - if given the option, set the file type as "Webpage, Complete"  

```{r out.height = "50%", out.width= "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_offline_saveas.png"))
```





## Download data to follow along  

All the data in this handbook can be downloaded from the **["data" folder](https://github.com/nsbatra/Epi_R_handbook/tree/master/data)** of our Github repository. When you arrive at this folder, click the file you want to download.    

To "follow along" with any of the handbook examples that use the `linelist` dataset, download and load either the "linelist_raw.xlsx" (for the Cleaning page), or "linelist_cleaned.rds" (for any other page).  

Here are detailed instructions for downloading the data files from Github:  




### "Raw" linelist (.xlsx) {-}

Use the "linelist_raw.xlsx" to follow-along with the [Cleaning data and core functions] page. To download the *raw* linelist (uncleaned, with messy data), use the following appraoch.  
*Note: Use "linelist_cleaned.rds" to follow-along with any other page that uses the case linelist.*  

1) Go to the **["data" folder](https://github.com/nsbatra/Epi_R_handbook/tree/master/data)** of our Github repository  
2) Click on "linelist_raw.xlsx"  
3) Click the "Download" button as shown below  
4) Save the file on your computer, and import it into R with the `import()` function from **rio**, as described in the page on [Import and export].  


```{r out.height = "50%", out.width= "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_xlsx.png"))
```

### Clean linelist (.rds) {-}  

Use the "linelist_cleaned.rds" to follow-along with any page that uses the case linelist (other than the data cleaning page). A .rds file is an R-specific file type that preserves column classes. This ensures you will have minimal cleaning to do after importing.  

1) Go to the **["data" folder](https://github.com/nsbatra/Epi_R_handbook/tree/master/data)** of our Github repository  
2) Click on "linelist_cleaned.rds"  
3) Click the "Download" button as shown below  
4) Save the file on your computer, and import it into R with the `import()` function from **rio**, as described in the page on [Import and export].  



```{r out.height = "50%", out.width= "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_rds.png"))
```


### .csv files {-} 

It is easy to download a relatively small CSV file directly from Github into R with an R command.  

1) Go to the **["data" folder](https://github.com/nsbatra/Epi_R_handbook/tree/master/data)** of our Github repository  
2) Click on the .csv file of interest  
3) Click on the "Raw" button (you will then see the "raw" csv data, as shown below)  
4) Copy the URL (web address)  
5) Use the URL in the `import()` R command, as shown below  

```{r out.height = "50%", out.width= "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_csv_raw.png"))
```

Provide the URL to `import()` from **rio** as shown below. 

```{r, eval=F}
# Example of importing the country demographics csv file
country_demographics <- import("https://raw.githubusercontent.com/nsbatra/Epi_R_handbook/master/data/country_demographics.csv")

# Example of importing the likert-scale data csv file
likert_data <- import("https://raw.githubusercontent.com/nsbatra/Epi_R_handbook/master/data/likert_data.csv")
```

If this the above approach not work, you can click the "Raw" button, right-click on the webpage to "Save As" the data as a .txt file to your computer. This file can then be imported into R with `import()`.  


### Shapefiles {-}

Shapefiles have many sub-component files, each with a different file extention. One file will have the ".shp" extension, but others may have ".dbf", ".prj", etc.  

The [GIS basics] page provides links to the *Humanitarian Data Exchange* website where you can download the shapefiles directly. For example, the health facility points can be downloaded [here](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities). Download the points shapefile (shp) ".zip" folder. Once saved to your computer, "unzip" the folder. You will see several files with different extensions (e.g. ".shp", ".prj", ".shx") - all these must be saved in the same folder. As described in the [GIS basics] page, provide the filepath and name of the ".shp" file to `st_read()` from the **sf** package.  

Alternatively, you can download the shapefiles from the R Handbook Github "data" folder (see the "shp" sub-folder). However, be aware that you will need to download *each* sub-file individually to your computer. In Github, click on each file individually and download them by clicking on the "Download" button. Below, you can see how the shapefile "sle_adm3" consists of many files - each of which must be downloaded from Github.  

```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```


### NWK and other files {-}  

Click on the file in the Gitub "data" folder, and use the "Download" button, as described above for .xlsx files.  




## Background information  

Below are more details about some of the data: 

* The case linelist  
     * A *simulated* Ebola outbreak, expanded by the handbook team from the `ebola_sim` practice dataset in the **outbreaks** package  
     
* Aggregated counts  
     * A simulated dataset of malaria counts by age, day, and facility in a fictional region, which can be downloaded from the data folder noted above as "district_count_data.xlsx"  
     
* Time series and outbreak detection  
     * Campylobacter cases reported in Germany 2002-2011. Available from **surveillance** package
     * Climate data (temperature in degrees celsius and rain fail in millimetres) in Germany 2002-2011. Downloaded from the EU Copernicus satellite reanalysis dataset using the **ecmwfr** package as explained in the time series page.  

* GIS page shapefiles  
     * Downloaded from the Humanitarian Data Exchange (HDX) - see links in the GIS basics page  

* Phylogenetic tree data  
     * Newick file of phylogenetic tree constructed from whole genome sequencing of 299 Shigella sonnei samples and corresponding sample data.
     * The Belgian samples and resulting data are kindly provided by the Belgian NRC for Salmonella and Shigella in the scope of a project conducted by an ECDC EUPHEM Fellow, and will also be published in a manuscript.  
     * The international data are openly available on public databases (ncbi) and have been previously published.  
     

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/data_used.Rmd-->

# (PART) Basics {-}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/cat_basics.Rmd-->

# R Basics {}

<!-- ======================================================= -->
## Overview

**This page is not intended to be a comprehensive "learn R" tutorial**. However, it does cover some fundamentals that can be useful for reference or for refreshing your memory. See the section on recommended training for links to more comprehensive tutorials.  

Much of this page has been adapted with permission from the R Basics section of the [R4Epis project](https://r4epis.netlify.app/).  

See the page on [Transition to R] for tips on switching to R from STATA and SAS.  

```{r, echo=F}
# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
pacman::p_load(apyramid)
```




<!-- ======================================================= -->
## Why use R?

As stated on the [R project website](https://www.r-project.org/about.html), R is a programming language and environment for statistical computing and graphics. It is highly versatile, extensible, and community-driven.  

**Cost**

R is free to use! There is a strong ethic in the community of free and open-source material.  

**Reproducibility**  

Conducting your data management and analysis through a programming language (compared to Excel or another primarily point-click/manual tool) enhances **reproducibility**, makes **error-detection** easier, and eases your workload.  

**Community**  

The R community of users is enormous and collaborative. New packages and tools to address real-life problems are developed daily, and vetted by the community of users. As one example, [R-Ladies](https://rladies.org/) is a worldwide organization whose mission is to promote gender diversity in the R community, and is one of the largest organizations of R users. It likely has a chapter near you!  




## Recommended training  

### Resources within RStudio {-}  

**Help documentation**  

Search the RStudio "Help" tab for documentation on R packages and specific functions. This is within the pane that also contains Files, Plots, and Packages (typically in the lower-right pane). As a shortcut, you can also type the name of a package or function into the R console after a question-mark to open the relevant Help page. For example:  `?filter`  or `?diagrammeR`.  

**Interactive tutorials**  

RStudio has built-in interative tutorials via the **learnr** package. If this package is installed, you can go through these tutorials via the "Tutorial" tab in the upper-right RStudio pane (which also contains Environment and History tabs).  


### Cheatsheets {-}

This is an online R resource specifically for [Excel users](https://jules32.github.io/r-for-excel-users/)  

There are many cheatsheets available on the [RStudio website](https://rstudio.com/resources/cheatsheets/), for example:  

* Factors with **forcats** package  
* Dates and times with **lubridate** package  
* Strings with **stringr** package  
* "apply" functions with **purrr** package  
* Data import cheatsheet  
* Data transformation cheatsheet with **dplyr** package  
* R Markdown  
* Shiny  
* Data visualization with **ggplot2** package  
* Cartography  
* **leaflet** package (interactive maps)  
* Python with R (**reticulate** package)  


### Free online resources {-}  

A definitive text is the [R for Data Science](https://r4ds.had.co.nz/) book by Garrett Grolemund and Hadley Wickham

The [R4Epis](https://r4epis.netlify.app/) project website aims to "develop standardised data cleaning, analysis and reporting tools to cover common types of outbreaks and population-based surveys that would be conducted in an MSF emergency response setting." You can find R basics training materials, templates for RMarkdown reports on outbreaks and surveys, and tutorials to help you set them up.  






<!-- ======================================================= -->
## Installation  

**How to install R**  

Visit this website [https://www.r-project.org/](https://www.r-project.org/) and download the latest version of R suitable for your computer.  

**How to install RStudio**  

Visit this website [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/) and download the latest free Desktop version of RStudio suitable for your computer.

**Permissions**  
Note that you should install R and RStudio to a drive where you have read and write permissions. Otherwise, your ability to install R packages (a frequent occurrence) will be impacted. If you encounter problems, try opening RStudio by right-clicking the icon and selecting "Run as administrator". Other tips can be found in the page [R on network drives].  

**How to update R and RStudio**  

Your version of R is printed to the R Console at start-up. You can also run `sessionInfo()`.  

To update R, go to the website mentioned above and re-install R. Alternatively, you can use the **installr** package (on Windows) by running `installr::updateR()`. This will open dialog boxes to help you download the latest R version and update your packages to the new R version. More details can be found in the **installr** [documentation](https://www.r-project.org/nosvn/pandoc/installr.html).  

Be aware that the old R version will still exist in your computer. You can temporarily run an older version (older "installation") of R by clicking "Tools" -> "Global Options" in RStudio and choosing an R version. This can be useful if you want to use a package that has not been updated to work on the newest version of R.  

To update RStudio, you can go to the website above and re-download RStudio. Another option is to click "Help" -> "Check for Updates" within RStudio, but this may not show the very latest updates.  

To see which versions of R, RStudio, or packages were used when this Handbook as made, see the page on [Datasets and version used]. 


### Other software you *may* need to install {-} 

* TinyTeX (*for compiling an RMarkdown document to PDF*)  
* Pandoc  (*for compiling RMarkdown documents*)  
* RTools  (*for building packages for R*)  
* phantomjs (*for saving still images of animated networks, such as transmission chains*)  


#### TinyTex {-}  

TinyTex is a custom LaTeX distribution, useful when trying to produce PDFs from R.  
See [https://yihui.org/tinytex/](https://yihui.org/tinytex/) for more informaton.  

To install TinyTex from R:  

```{r, eval=F}
install.packages('tinytex')
tinytex::install_tinytex()
# to uninstall TinyTeX, run tinytex::uninstall_tinytex()
```


#### Pandoc {-}

Pandoc is a document converter, a separate software from R. **It comes bundled with RStudio and should not need to be downloaded.** It helps the process of converting Rmarkdown documents to formats like .pdf and adding complex functionality.  


#### RTools {-}  

RTools is a collection of software for building packages for R

Install from this website: [https://cran.r-project.org/bin/windows/Rtools/](https://cran.r-project.org/bin/windows/Rtools/)  


#### phantomjs {-}  

This is often used to take "screenshots" of webpages. For example when you make a transmission chain with **epicontacts** package, an HTML file is produced that is interactive and dynamic. If you want a static image, if can be useful to use the [**webshot**](https://wch.github.io/webshot/articles/intro.html) package to automate this process. This will require the external program "phantomjs". You can install phantomjs via the **webshot** package with the command `webshot::install_phantomjs()`.  




<!-- ======================================================= -->
## RStudio {#rstudio  }


### RStudio Orientation {-}  

This RStudio orientation is borrowed from the [R4Epis project](https://r4epis.netlify.app/).

**First, open RStudio.** As their icons can look very similar, be sure you are opening *RStudio* and not R.  

For RStudio to function you must also have R installed on the computer (see [this section](#install) for installation instructions).  

**RStudio** is an interface (GUI) for easier use of **R**. You can think of R as being the engine of a vehicle, doing the crucial work, and RStudio as the body of the vehicle (with seats, accessories, etc.) that helps you actually use the engine to move forward!  

By default RStudio displays four rectangle panes.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "RStudio_overview.png"))
```


<span style="color: black;">**_TIP:_** If your RStudio displays only one left pane it is because you have no scripts open yet.</span>


**The R Console Pane**  

The R Console, by default the left or lower-left pane in R Studio, is the home of the R "engine". This is where the commands are actually run and non-graphic outputs and error/warning messages appear. You can directly enter and run commands in the R Console, but realize that these commands are not saved as they are when running commands from a script.  

If you are familiar with Stata, the R Console is like the Command Window and also the Results Window.

**The Source Pane**  
This pane, by default in the upper-left, is space to edit, run, and save your scripts. This pane can also display datasets (data frames) for viewing.  

For Stata users, this pane is similar to your Do-file and Data Editor windows.


**The Environment Pane**  
This pane, by default the upper-right, is most often used to see brief summaries of objects in the R Environment in the current session. These [objects](#objects) could include imported, modified, or created datasets, parameters you have defined (e.g. a specific epi week for the analysis), or vectors or lists you have defined during analysis (e.g. names of regions). Click on the arrow next to a dataframe name to see its variables.  

In Stata, this is most similar to Variables Manager window.

This pane also contains *History* where can see commands that you can previously. It also has a "Tutorial" tab where you can complete interactive R tutorials if you have the **learnr** package installed. It also has a "Connections" pane for external connections, and can have a "Git" pane if you choose to interface with Github.  


**Plots, Viewer, Packages, and Help Pane**  
The lower-right pane includes several tabs. Typical plot graphics including maps will display in the Plot pane. Interactive or HTML outputs will display in the Viewer pane. The Help pane can display documentation and help file. The Files pane is a browser which can be used to open or delete files. The Packages pane allows you to see, install, update, delete, load/unload R packages, and see which version of the package you have.  

This pane contains the Stata equivalents of the Plots Manager and Project Manager windows.

### RStudio settings {-}  

Change RStudio settings and appearance in the *Tools* drop-down menu, by selecting *Global Options*. There you can change the default settings, including appearance/background color.  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "RStudio_tools_options_1.png"))

knitr::include_graphics(here::here("images", "RStudio_tools_options.png"))
```

**Restart**  

If your R freezes, you can re-start R by going to the Session menu and clicking "Restart R". The avoids the hassle of closing and opening RStudio. Everything in your R environment will be removed when you do this.  




### Keyboard shortcuts {-}  

UNDER CONSTRUCTION 




<!-- ======================================================= -->
## Scripts {#scripts}

Scripts are a fundamental part of programming. They act as a repository for your commands. Storing and running you code from a script (vs. typing in the R console "command line") has many advantages:  

* Portability - you can share your work with others by sending them your scripts  
* Reproducibility - so that you and others know exactly what you did  
* Version control - so you can track changes made by yourself or colleagues  
* Commenting/annotation - to explain to your colleagues what you have done  

Below is an example of a short R script. Remember, the better you succinctly explain your code in comments, the more your colleagues will like you!  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "example_script.png"))
```

<!-- ======================================================= -->
### R markdown {-}

R markdown is a type of script in which the script itself *becomes* a document (PDF, Word, HTML, Powerpoint, etc.). These are incredibly useful and versatile tools often used to create dynamic and automated reports. Even this website and handbook is produced with R markdown scripts!  

To learn more, see the handbook page on [R Markdown] documents.  


<!-- ======================================================= -->
### R notebooks {-}

There is no difference between writing in a Rmarkdown vs an R notebook. However the execution of the document differs slightly. See this [site](http://uc-r.github.io/r_notebook) for more details.

<!-- ======================================================= -->
### Shiny {-}

Shiny apps/websites are contained within one script, which must be named `app.R`. This file has three components:  

1) A user interface (ui)  
2) A server function  
3) A call to the `shinyApp` function  

See the handbook page on [Shiny and dashboards], or this online tutorial: [Shiny tutorial](https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/)

*In older times, the above file was split into two files (`ui.R` and `server.R`)*




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Working directory  

The working directory is the root folder location used by R for your work - where R looks for and saves files by default. By default, it will save new files and outputs to this location, and will look for files to import (e.g. datasets) here as well.  

The working directory appears in grey text at the top of the RStudio Console pane. You can also return the current working directory with `getwd()` (leave the parentheses empty).  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "working_directory_1.png"))
```

**See the page on [R projects] for details on our recommended approach to managing your working directory.** This common, efficient, and trouble-free way to use R is to combine these 3 elements: An R project to store all your files, the **here** package to locate files, and the **rio** package to import/export files.  


<!-- ======================================================= -->
### Set by command {-}

Although we do not recommend this approach in most circumstances, you can use the command `setwd()` with the desired folder file path in quotations, for example:  

```{r, eval=F}
setwd("C:/Documents/R Files/My analysis")
```


<!-- ======================================================= -->
### Set manually {-}
To set the working directory manually (point-and-click), click the Session drop-down menu and go to "Set Working Directory" and then "Choose Directory". This will set the working directory for that specific R session (if using this approach, you will have to do this each time you open RStudio).  


<!-- ======================================================= -->
### Within an R project {-}

If using an R project, the working directory will default to the R project root folder that contains the ".rproj" file. This will apply if you open RStudio by clicking open the R project (the file with ".rproj" extension))  

<!-- ======================================================= -->
### Working directory in an R markdown {-}

In an R markdown script, the default working directory is the folder the Rmarkdown file (`.Rmd`) is saved within. If using an R project and **here** package, this does not apply and the working directory will be `here()` as explained in the [R projects] page.  

If you want to change the working directory of a stand-alone R markdown (not in an R project), if you use `setwd()` this will only apply to that specific code chunk. To make the change for all code chunks in an R markdown, edit the setup chunk to add the `root.dir = ` parameter, such as below:

```{r, eval=F}
knitr::opts_knit$set(root.dir = 'desired/directorypath')
```

It is much easier to just use the R markdown within an R project and use the **here** package.  



 
<!-- ======================================================= -->
### Providing file paths {-}  

Perhaps the most common source of frustration for an R beginner (at least on a Windows machine) is typing in a filepath to import data. Note the following:  

**Slash direction** - *If typing in a filepath, beware the direction of the slashes.* Enter them using *forward slashes* to separate the components ("data/provincial.csv"). For Windows users, the default way that filepaths are displayed and copied is with *backslashes* ("\\") - so this means you will need to change the direction of each slash.  

If you use the **here** package as described in [R projects] the slash direction is not an issue.  

**Broken paths** - 

Below is an example of an "absolute" or "full address" filepath. These will likely break if used by another computer.  

```
C:/Users/Name/Document/Analytic Software/R/Projects/Analysis2019/data/March2019.csv  
```

In most situations, we recommend using "relative" filepaths instead - that is, the path *relative to* the root of an R project. You can do this using the **here** package as explained in the [R projects] page. 

One possible exception to this is if you need to load data from folder locations outside of an R project. In this case you can still use an R project and relative file paths for your scripts and outputs, but you may need to use an absolute file path to import these data.

 




<!-- ======================================================= -->
## Objects {#objects}

Everything in R is an object, and R is an "object-oriented" language. These sections will explain:  

* How to create objects (`<-`) 
* Types of objects (e.g. data frames, vectors..)  
* How to access subparts of objects (e.g. variables in a dataset)  
* Classes of objects (e.g. numeric, logical, integer, double, character, factor)  



<!-- ======================================================= -->
### Everything is an object {-} 

*This section is adapted from the [R4Epis project](https://r4epis.netlify.app/training/r_basics/objects/).*  
Everything you store in R - datasets, variables, a list of village names, a total population number, even outputs such as graphs - are **objects** which are **assigned a name** and **can be referenced** in later commands.  

An object exists when you have assigned it a value (see the assignment section below). When it is assigned a value, the object appears in the Environment (see the upper right pane of RStudio). It can then be operated upon, manipulated, changed, and re-defined.


<!-- ======================================================= -->
### Defining objects (`<-`) {-}

**Create objects *by assigning them a value* with the <- operator.**  
You can think of the assignment operator `<-` as the words "is defined as". Assignment commands generally follow a standard order:
 
**object_name**  <-  **value** (or process/calculation that produce a value)

For example, you may want to record the current epidemiological reporting week as an object for reference in later code. In this example, the object `current_week` is created when it is assigned the character value `"2018-W10"` (the quote marks make these a character value). The object `current_week` will then appear in the RStudio Environment pane (upper-right) and can be referenced in later commands.  

See the R commands and their output in the boxes below. 

```{r basics_objects_assignment}
current_week <- "2018-W10"   # this command creates the object current_week by assigning it a value
current_week                 # this command prints the current value of current_week object in the console
```

<span style="color: black;">**_NOTE:_** Note the `[1]` in the R console output is simply indicating that you are viewing the first item of the output</span>


<span style="color: orange;">**_CAUTION:_** **An object's value can be over-written** at any time by running an assignment command to re-define its value. Thus, the **order of the commands run is very important**.</span>

The following command will re-define the value of `current_week`: 

```{r basics_objects_reassignment}
current_week <- "2018-W51"   # assigns a NEW value to the object current_week
current_week                 # prints the current value of current_week in the console
```

**Equals signs `=`**  

You will also see equals signs in R code:  

* A double equals sign `==` between two objects or values asks a logical *question*: "is this equal to that?".  
* You will also see equals signs within functions used to specify values of function arguments (read about these in sections below), for example `max(age, na.rm = TRUE)`.  
* You *can* use a single equals sign `=` in place of `<-` to create and define objects, but this is discouraged. You can about why this is discouraged [here](https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/).  


**Datasets**  

Datasets are also objects (typically "dataframes") and must be assigned names when they are imported. In the code below, the object `linelist` is created and assigned the value of a CSV file imported with the **rio** package and its `import()` function.  

```{r basics_objects_dataframes, eval=FALSE}
# linelist is created and assigned the value of the imported CSV file
linelist <- rio::import("my_linelist.csv")
```

You can read more about importing and exporting datasets with the section on [Import and export].

<span style="color: orange;">**_CAUTION:_** A quick note on naming of objects:</span>

  * Object names must not contain spaces, but you should use underscore (_) or a period (.) instead of a space.  
  * Object names are case-sensitive (meaning that Dataset_A is different from dataset_A). 
  * Object names must begin with a letter (cannot begin with a number like 1, 2 or 3). 

**Outputs**  

Outputs like tables and plots provide an example of how outputs can be saved as objects, or just be printed without being saved. A cross-tabulation of gender and outcome using the **base** R function `table()` can be printed directly to the R console (*without* being saved).

```{r}
# printed to R console only
table(linelist$gender, linelist$outcome)
```

But the same table can be saved as a named object. Then, optionally, it can be printed.

```{r}
# save
gen_out_table <- table(linelist$gender, linelist$outcome)

# print
gen_out_table
```

**Columns**  

Columns in a dataset are also objects and can be defined, over-written, and created as described below in the section on Columns. 

You can use the assignment operator from **base** R to create a new column. Below, the new column `bmi` (Body Mass Index) is created, and for each row the new value is result of a mathematical operation on the row's value in the `wt_kg` and `ht_cm` columns.  

```{r, eval=F}
# create new "bmi" column using base R syntax
linelist$bmi <- linelist$wt_kg / (linelist$ht_cm/100)^2
```

However, in this handbook, we emphasize a different approach to defining columns, which uses the function `mutate()` from the **dplyr** package. The syntax is easier to read and there are other advantages explained in the page on [Cleaning data and core functions].  

```{r, eval=F} 
# create new "bmi" column using dplyr syntax
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```


<!-- ======================================================= -->
### Object structure {-}  

**Objects can be a single piece of data (e.g. `my_number <- 24`), or they can consist of structured data.**  

The graphic below is borrowed from [this online R tutorial](http://venus.ifca.unican.es/Rintro/dataStruct.html). It shows some common data structures and their names. Not included in this image is spatial data, which is discussed in the [GIS basics] page.  


```{r basics_objects_structures, echo=F, out.width = "75%", out.height="50%", fig.align = "center"}
knitr::include_graphics(here::here("images", "R_data_structures.png"))
```  

In epidemiology (and particularly field epidemiology), you will *most commonly* encounter data frames and vectors:  


Common structure | Explanation | Example
------------------- | ------------------------------------ | ------------------------  
Vectors | A container for a sequence of singular objects, all of the same class (e.g. numeric, character). | **"Variables" (columns) in data frames are vectors** (e.g. the column `age_years`).  
Data Frames | Vectors (e.g. columns) that are bound together that all have the same number of rows. | `linelist` is a data frame.

Note that to create a vector that "stands alone" (is not part of a data frame) the function `c()` is used to combine the different elements. For example, if creating a vector of colors plot's color scale: 
`list_of_names <- c("blue", "red2", "orange", "grey")`  


<!-- ======================================================= -->
### Object classes  {-}

All the objects stored in R have a *class* which tells R how to handle the object. There are many possible classes, but common ones include:

Class |	Explanation | Examples
------ | ------------------------------------------ |  -----------------------------
Character	| These are text/words/sentences **"within quotation marks"**. Math cannot be done on these objects.	| "Character objects are in quotation marks"  
Integer | Numbers that are **whole only** (no decimals) | -5, 14, or 2000  
Numeric	| These are numbers and **can include decimals**. If within quotation marks the will be considered character. | 23.1 or 14  
Factor | These are vectors that have a **specified order** or hierarchy of values | Variable `msf_involvement` with ordered values N, S, SUB, and U.  
Date | **Once R is told that certain data are Dates**, these data can be manipulated and displayed in special ways. See the page on [Working with dates] for more information. | 2018-04-12 or 15/3/1954 or Wed 4 Jan 1980  
Logical | Values must be one of the two special values TRUE or FALSE (note these are **not** "TRUE" and "FALSE" in quotation marks) | TRUE or FALSE  
data.frame | A data frame is how R stores a **typical dataset**. It consists of vectors (columns) of data bound together, that all have the same number of observations (rows). | The example AJS dataset named `linelist_raw` contains 68 variables with 300 observations (rows) each.  
tibble | tibbles are a variation on data frame, the main operational difference being that they print more nicely to the console (display first 10 rows and only columns that fit on the screen) | Any data frame, list, or matrix can be converted to a tibble with `as_tibble()`  
list | A list is like vector, but holds other objects that can be other different classes | A list could hold a single number, and a dataframe, and a vector, and even another list within it!  


**You can test the class of an object by providing its name to the function `class()`**. Note: you can reference a specific column within a dataset using the `$` notation to separate the name of the dataset and the name of the column.

```{r, echo=TRUE, eval=T}
class(linelist)         # class should be a data frame

class(linelist$age)     # class should be numeric

class(linelist$gender)  # class should be character
```

Sometimes, a column will be converted to a different class automatically by R. Watch out for this! For example, if you have a vector or column of numbers, but a character value is inserted... the entire column will change to class character.  

One common example of this is when manipulating a data frame in order to print a table - if you make a total row and try paste/glue together percents in the same cell as numbers (e.g. `23 (40%)`), the entire numeric column above will convert to character and can no longer be used for mathematical calculations.

```{r}
num_vector <- c(1,2,3,4,5) # define vector as all numbers
class(num_vector)          # vector is numeric class
num_vector[3] <- "three"   # convert the third element to a character
class(num_vector)          # vector is now character class
```


**Sometimes, you will need to convert objects or columns to another class.**

Function | Action  
----------------- | --------------------------------------------------------------    
`as.character()` | Converts to character class  
`as.numeric()` | Converts to numeric class  
`as.integer()` | Converts to integer class
`as.Date()` | Converts to Date class - Note: see section on [dates](#dates) for details  
`factor()` | Converts to factor - Note: re-defining order of value levels requires extra arguments

Likewise, there are **base** R functions to check whether an object IS of a specific class, such as `is.numeric()`, `is.character()`, `is.double()`, `is.factor()`, `is.integer()`

Here is [more online material on classes and data structures in R](https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/).


<!-- ======================================================= -->
### Columns/Variables (`$`) {-}  

**A column in a data frame is technically a "vector" (see table above)** - a series of values that must all be the same class (either character, numeric, logical, etc).  

A vector can exist independent of a data frame, for example a vector of column names that you want to include as explanatory variables in a model. To create a "stand alone" vector, use the `c()` function as below:  

```{r, warning=F, message=F}
# define the stand-alone vector of character values
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")

# print the values in this named vector
explanatory_vars
```

**Columns in a data frame are also vectors and can be called, referenced, extracted, or created using the `$` symbol.** The `$` symbol connects the name of the column to the name of its data frame. In this handbook, we try to use the word "column" instead of "variable".  


```{r basics_objects_call, eval=F}
# Retrieve the length of the vector age_years
length(linelist$age) # (age is a column in the linelist data frame)

```

By typing the name of the dataframe followed by `$` you will also see a drop-down menu of all columns in the data frame. You can scroll through them using your arrow key, select one with your Enter key, and avoid spelling mistakes!  

```{r echo=F, out.width = "100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Calling_Names.gif"))
```  



<span style="color: darkgreen;">**_ADVANCED TIP:_** Some more complex objects (e.g. a list, or an `epicontacts` object) may have multiple levels which can be accessed through multiple dollar signs. For example `epicontacts$linelist$date_onset`</span>



<!-- ======================================================= -->
### Access/index with brackets (`[ ]`) {-}  

You may need to view parts of objects, also called "indexing", which is often done using the square brackets `[ ]`. Using `$` on a dataframe to access a column is also a type of indexing.  

```{r}
my_vector <- c("a", "b", "c", "d", "e", "f")  # define the vector
my_vector[5]                                  # print the 5th element
```

Square brackets also work to return specific parts of an returned output, such as the output of a `summary()` function: 

```{r}
# All of the summary
summary(linelist$age)

# Just one part of the summary
summary(linelist$age)[2]

# One part without its "name"
summary(linelist$age)[[2]]
```

Brackets also work on data frames to view specific rows and columns. You can do this using the syntax `dataframe[rows, columns]`:  

```{r basics_objects_access, eval=F}
# View a specific row (2) from dataset, with all columns (don't forget the comma!)
linelist[2,]

# View all rows, but just one column
linelist[, "date_onset"]

# View values from row 2 and columns 5 through 10
linelist[2, 5:10] 

# View values from row 2 and columns 5 through 10 and 18
linelist[2, c(5:10, 18)] 

# View rows 2 through 20, and specific columns
linelist[2:20, c("date_onset", "outcome", "age")]

# View rows and columns based on criteria
# *** Note the dataframe must still be names in the criteria!
linelist[linelist$age > 25 , c("date_onset", "date_birth", "age")]

# Use View() to see the outputs in the RStudio Viewer pane (easier to read) 
# *** Note the capital "V" in View() function
View(linelist[2:20, "date_onset"])

# Save as a new object
new_table <- linelist[2:20, c("date_onset")] 
```

When indexing an object of class **list**, single brackets always return with class list, even if only a single object is returned. Double brackets, however, can be used to access a single element and return a different class than list.  
Brackets can also be written after one another, as demonstrated below.  

This [visual explanation of lists indexing, with pepper shakers](https://r4ds.had.co.nz/vectors.html#lists-of-condiments) is humorous and helpful.

```{r}
# define demo list
my_list <- list(
  # First element in the list is a character vector
  hospitals = c("Central", "Empire", "Santa Anna"),
  
  # second element in the list is a data frame of addresses
  addresses   = data.frame(
    street = c("145 Medical Way", "1048 Brown Ave", "999 El Camino"),
    city   = c("Andover", "Hamilton", "El Paso")
    )
  )
```

Here is how the list looks when printed to the console. See how there are two named elements:  

* `hospitals`, a character vector  
* `addresses`, a data frame of addresses

```{r}
my_list
```
Now we extract, using various methods:  

```{r}
my_list[1] # this returns the element in class "list" - the element name is still displayed

my_list[[1]] # this returns only the (unnamed) character vector

my_list[["hospitals"]] # you can also index by name of the list element

my_list[[1]][3] # this returns the third element of the "hospitals" character vector

my_list[[2]][1] # This returns the first column ("street") of the address data frame

```



<!-- ======================================================= -->
### Remove objects {-} 

You can remove individual objects from your R environment by putting the name in the `rm()` function (no quote marks):  

```{r, eval=F}
rm(object_name)
```

You can remove all objects (clear your workspace) by running:  

```{r, eval=F}
rm(list = ls(all = TRUE))
```


<!-- ======================================================= -->
## Functions  

This section on functions explains:  

* What a function is and how they work  
* What arguments are  
* How to get help understanding a function  


<!-- ======================================================= -->
### Simple functions {-}  

**A function is like a machine that receives inputs, does some action with those inputs, and produces an output.** What the output is depends on the function.    

**Functions typically operate upon some object placed within the function's parentheses**. For example, the function `sqrt()` calculates the square root of a number:  

```{r basics_function_sqrt}
sqrt(49)
```

The object provided to a function also can be a column in a dataset. For example, when the function `summary()` is applied to the numeric column `age` in the dataset `linelist`, the output is a summary of the columns's numeric and missing values.

```{r basics_functions_summary}
summary(linelist$age)
```

<span style="color: black;">**_NOTE:_** Behind the scenes, a function represents complex additional code that has been wrapped up for the user into one easy command.</span>



<!-- ======================================================= -->
### Functions with multiple arguments {-}  

Functions often ask for several inputs, called ***arguments***, located within the parentheses of the function, usually separated by commas. 

* Some arguments are required for the function to work correctly, others are optional  
* Optional arguments have default settings  
* Arguments can take character, numeric, logical (TRUE/FALSE), and other inputs  

Here is a fun fictional function, called `oven_bake()`, as an example of a typical function. It takes an input object (e.g. a dataset, or in this example "dough") and performs operations on it as specified by additional arguments (`minutes = ` and `temperature = `). The output can be printed to the console, or saved as an object using the assignment operator `<-`.  

```{r basics_functions_image, echo=F, out.width = "75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Function_Bread_Example.png"))
```


**In a more realistic example**, the `age_pyramid()` command below produces an age pyramid plot based on defined age groups and a binary split column, such as `gender`. The function is given three arguments within the parentheses, separated by commas. The values supplied to the arguments establish `linelist` as the dataframe to use, `age_cat5` as the column to count, and `gender` as the binary column to use for splitting the pyramid by color.

```{r basics_functions_arguments, include=FALSE, results='hide', message=FALSE, warning=FALSE, eval=T}
## create an age group variable by specifying categorical breaks
linelist$age_group <- cut(linelist$age, breaks = c(0, 5, 10, 15, 20, 30, 45, 60))
```

```{r message=FALSE, warning=FALSE, eval=T, out.width = "75%", out.height="75%"}
# Create an age pyramid
apyramid::age_pyramid(data = linelist, age_group = "age_cat5", split_by = "gender")
```

The above command can be equivalently written as below, with newlines. This can be easier to read and to write `# comments`. To run this command you can highlight the entire command, or just place your cursor in the first line and then press Ctrl and Enter keys simultaneously.  

```{r message=FALSE, warning=FALSE, eval=T, out.width = "75%", out.height="75%"}
# Create an age pyramid
apyramid::age_pyramid(
  data = linelist,        # case linelist
  age_group = "age_cat5", # age group column
  split_by = "gender"     # two sides to pyramid
  )
```

The first half of an argument assignment (e.g. `data = `) does not need to be specified if the arguments are written in a specific order (specified in the function's documentation). The below code produces the exact same pyramid as above, because the function expects the argument order: data frame, `age_group` variable, `split_by` variable.  

```{r, basics_functions_pyramid2, eval = FALSE, warning=FALSE, message=FALSE, , out.width = "75%", out.height="75%", eval=F}
# This command will produce the exact same graphic as above
apyramid::age_pyramid(linelist, "age_cat5", "gender")
```

**A more complex `age_pyramid()` command might include the *optional* arguments to:**  

* Show proportions instead of counts (set `proportional = TRUE` when the default is `FALSE`)  
* Specify the two colors to use (`pal = ` is short for "palette" and is supplied with a vector of two color names. See the [objects](#objectstructure) page for how the function `c()` makes a vector)  


<span style="color: black;">**_NOTE:_** For arguments that you specify with both parts of the argument (e.g. `proportional = TRUE`), their order among all the arguments does not matter.</span>


```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
apyramid::age_pyramid(
  linelist,                    # use case linelist
  "age_cat5",                  # age group column
  "gender",                    # split by gender
  proportional = TRUE,         # percents instead of counts
  pal = c("orange", "purple")  # colors
  )
```







<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Packages {}  

**Packages contain functions.**  

An R package is a shareable bundle of code and documentation that contains pre-defined functions. Users in the R community develop and share packages all the time, so chances are likely that a solution exists for you! You will install and use hundreds of packages in your use of R.  

On installation, R contains **"base"** packages and functions that perform common elementary tasks. But many R users create specialized functions, which are verified by the R community and which you can download as a **package** for your own use. In this handbook, package names are written in **bold**. One of the more challenging aspects of R is that there are often many functions or packages to choose from to complete a given task.  


### Install and load {-}  

*Functions* are contained within **packages** which can be downloaded ("installed") to your computer from the internet. Once a package is downloaded, you access its functions by loading the package with the `library()` command (from **base** R) at the beginning of each R session. Later in this section we advocate for use of `p_load()` instead of `library()`, it this is still a very common option. 

*Think of R as your personal library*: When you download a package, your library gains a new book of functions, but each time you want to use a function in that book, you must borrow that book from your library.  


**Your library**  

Your "library" is actually a folder on your computer, containing a folder for each package that has been installed. Find out where R is installed in your computer, and look for a folder called "win-library". For example: `R\win-library\4.0` (the 4.0 is the R version - you'll have a different library for each R version you've downloaded). As a last-case measure, you can remove a package by manually deleting it from here (but it is better to use `remove.packages("packagename")`).  


**CRAN**  

CRAN (Comprehensive R Archive Network) is a public warehouse of R packages that have been published by R community members. Most often, R users download packages from CRAN.  
 

**Install vs. Load**

To use a package, 2 steps must be implemented:  

1) The package must be **installed** (once), *and*  
2) The package must be **loaded** (each R session)  

The basic function for installing a package is `install.packages()`, where the name of the package is provided *in quotes*. This can also be accomplished point-and-click by going to the RStudio "Packages" pane and clicking "Install" and typing the package name. Note all this is case-sensitive. 

```{r, eval=F}
install.packages("tidyverse")
```

The basic function to **load** a package for use (after it has been installed) is `library()`, with the name of the package *NOT in quotes*.  

```{r, eval=F}
library(tidyverse)
```

To check whether a package in installed or loaded, you can view the Packages pane in RStudio. If the package is installed, it is shown there with version number. If the box is checked, it is loaded for the current session. 


**Using pacman**  

This handbook emphasizes use of the package **pacman** (abbreviation for "package manager"), which offers the useful function `p_load()`. This function combines the above two steps into one -  it *installs and/or loads packages*, depending on what is needed. If the package has not yet been installed, it will attempt to install from CRAN, and then load it.  

Below, we load three of the packages often used in this R basics page:  

```{r}
pacman::p_load(tidyverse, rio, here)
```


**Install from github**

Sometimes, you need to install a package that is not yet available from CRAN. Or perhaps the package is available on CRAN but you want the *development version* with new features. These are often hosted on with website Github in a public code "repository".  

To download these packages, you can use `p_load_gh()` from **pacman**. You may also see guidance top use the **remotes** or **devtools** packages. In the background, this **pacman** function utilizes `install_github()` from **devtools**).  

To install from github, you have to provide different information to the function. You must provide the Github ID of the repository owner, and the name of the code repository which contains the package.  

In the examples below, the first name listed in the quotation marks is the Github ID of the repository owner, and after the slash is the name of the repository. If you want to install from a branch other than the main/master branch, add the branch name after an "@" after the repository name.  

As usual, to use any package (including **pacman**) you must load it first, or specify the package name before the function with two colons `pacman::p_load_gh()`. This syntax loads the package in order to execute that function.  


```{r, eval=F}
# install/load package from github repository
p_load_gh("reconhub/epicontacts")

# load development version of package which you had downloaded from github repository
p_load_gh("reconhub/epicontacts")

# install development version of package, but not the main branch
p_install_gh("reconhub/epicontacts@timeline")
```

Read more about **pacman** in this [online vignette](http://trinker.github.io/pacman/vignettes/Introduction_to_pacman.html)


**Install from ZIP or TAR**

You could install the package from a URL:  

```{r, eval=F}
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

Or, download it to your computer in a zipped file:  

Option 1: using `install_local()` from the **remotes** package  

```{r, eval=F}
remotes::install_local("~/Downloads/dplyr-master.zip")
```

Option 2: using `install.packages()` from **base** R, providing the file path to the ZIP file and setting `type = "source` and `repos = NULL`.    

```{r, eval=F}
install.packages("~/Downloads/dplyr-master.zip", repos=NULL, type="source")
```


### Code syntax {-}  

For clarity in this handbook, functions are sometimes preceded by the name of their package using the `::` symbol in the following way: `package_name::function_name()`  

Once a package is loaded for a session, this explicit style is not necessary. One can just use `function_name()`. However writing the package name is useful when a function name is common and may exist in multiple packages (e.g. `plot()`). Writing the package name will also load the package if it is not already loaded. 

```{r eval=FALSE}
# This command uses the package "rio" and its function "import()" to import a dataset
linelist <- rio::import("linelist.xlsx", which = "Sheet1")
```



### Function help {-}  

To read more about a function, you can search for it in the Help tab of the lower-right RStudio. You can also run a command like  `?thefunctionname` (put the name of the function after a question mark) and the Help page will appear in the Help pane. Finally, try searching online for resources.  



### Update packages {-}  

You can update packages by re-installing them. You can also click the green "Update" button in your RStudio Packages pane to see which packages have new versions to install. Be aware that your old code may need to be updated if there is a major revision to how a function works!  


### Delete packages {-}
Use `p_delete()` from **pacman**, or `remove.packages()` from **base** R. Alternatively, go find the folder which contains your library and manually delete the folder.



### Dependencies {-}  

Packages often depend on other packages to work. These are called dependencies. If a dependency fails to install, then the package depending on it may also fail to install.  

See the dependencies of a package with `p_depends()`, and see which packages depend on it with `p_depends_reverse()`  



### Masked functions {-}  

It is not uncommon that two or more packages contain the same function name. For example, the package **dplyr** has a `filter()` function, but so does the package **stats**. The default `filter()` function depends on the order these packages are first loaded in the R session - the later one will be the default for the command `filter()`. 

You can check the order in your Environment pane of R Studio - click the drop-down for "Global Environment" and see the order of the packages. Functions from packages *lower* on that drop-down list will mask functions of the same name in packages that appear higher in the drop-down list. When first loading a package, R will warn you in the console if masking is occurring, but this can be easy to miss.  

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "masking_functions.png"))
```

Here are ways you can fix masking:  

1) Specify the package name in the command. For example, use `dplyr::filter()`  
2) Re-arrange the order in which the packages are loaded (e.g. within `p_load()`), and **start a new R session**  



### Detach / unload {-}  

To detach (unload) a package, use this command, with the correct package name and only one colon. Note that this may not resolve masking.  

```{r, eval=F}
detach(package:PACKAGE_NAME_HERE, unload=TRUE)
```


### Install older version {-}  

See this [guide](https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages) to install an older version of a particular package.  


### Suggested packages {-}  

See the page on [Suggested packages] for a listing of packages we recommend for everyday epidemiology.  






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Piping (`%>%`)  

**Two general approaches to working with objects are:**  

1) **Pipes/tidyverse** - pipes send an object from function to function - emphasis is on the *action*, not the object  
2) **Define intermediate objects** - an object is re-defined again and again - emphasis is on the object  


<!-- ======================================================= -->
### **Pipes** {-}

**Simply explained, the pipe operator (`%>%`) passes an intermediate output from one function to the next.**  
You can think of it as saying "then". Many functions can be linked together with `%>%`.  

* **Piping emphasizes a sequence of actions, not the object the actions are being performed on**  
* Pipes are best when a sequence of actions must be performed on one object  
* Pipes come from the package **magrittr**, which is automatically included in packages **dplyr** and **tidyverse**
* Pipes can make code more clean and easier to read, more intuitive

Read more on this approach in the tidyverse [style guide](https://style.tidyverse.org/pipes.html)  

Here is a fake example for comparison, using fictional functions to "bake a cake". First, the pipe method:  

```{r piping_example_pipe, eval=F}
# A fake example of how to bake a care using piping syntax

cake <- flour %>%       # to define cake, start with flour, and then...
  left_join(eggs) %>%   # add eggs
  left_join(oil) %>%    # add oil
  left_join(water) %>%  # add water
  mix_together(         # mix together
    utensil = spoon,
    minutes = 2) %>%    
  bake(degrees = 350,   # bake
       system = "fahrenheit",
       minutes = 35) %>%  
  let_cool()            # let it cool down
```

Here is another [link](https://cfss.uchicago.edu/notes/pipes/#:~:text=Pipes%20are%20an%20extremely%20useful,code%20and%20combine%20multiple%20operations) describing the utility of pipes.  

Piping is not a **base** function. To use piping, the **magrittr** package must be installed and loaded (this is typically done by loading **tidyverse** or **dplyr** package). You can [read more about piping in the magrittr documentation](https://magrittr.tidyverse.org/).

Note that just like other R commands, pipes can be used to just display the result, or to save/re-save an object, depending on whether the assignment operator `<-` is involved. See both below:  

```{r, eval=F}
# Create or overwrite object, defining as aggregate counts by age category (not printed)
linelist_summary <- linelist %>% 
  count(age_cat)
```

```{R, eval=F}
# Print the table of counts in the console, but don't save it
linelist %>% 
  count(age_cat)
```

<span style="color: orange;">**_CAUTION:_** Remember that even when using piping to link functions, if the assignment operator (`<-`) is present, the object to the left will still be over-written (re-defined) by the right side.</span>


**`%<>%`**  
This is an "assignment pipe" from the **magritter** package, which pipes an object forward and also re-defines the object. It must be the first pipe operator in the chain. It is shorthand, so `object %<>% function() %>% function()` is the same as `object <- object %>% function() %>% function()`.  


<!-- ======================================================= -->
### Define intermediate objects {-}

This approach to changing objects/dataframes may be better if:  

* You need to manipulate multiple objects  
* There are intermediate steps that are meaningful and deserve separate object names


**Risks:**  

* Creating new objects for each step means creating lots of objects. If you use the wrong one you might not realize it!  
* Naming all the objects can be confusing  
* Errors may not be easily detectable  

Either name each intermediate object, or overwrite the original, or combine all the functions together. All come with their own risks.  

Below is the same fake "cake" example as above, but using this style:  

```{r piping_example_redefine, eval=F}
# a fake example of how to bake a cake using this method (defining intermediate objects)
batter_1 <- left_join(flour, eggs)
batter_2 <- left_join(batter_1, oil)
batter_3 <- left_join(batter_2, water)

batter_4 <- mix_together(object = batter_3, utensil = spoon, minutes = 2)

cake <- bake(batter_4, degrees = 350, system = "fahrenheit", minutes = 35)

cake <- let_cool(cake)
```

Combine all functions together - this is difficult to read:  

```{r eval=F}
# an example of combining/nesting mutliple functions together - difficult to read
cake <- let_cool(bake(mix_together(batter_3, utensil = spoon, minutes = 2), degrees = 350, system = "fahrenheit", minutes = 35))
```


<!-- ======================================================= -->
## Key operators and functions {#operators}

This section details operators in R, such as:  

* Definitional operators  
* Relational operators (less than, equal too..)  
* Logical operators (and, or...)  
* Handling missing values  
* Mathematical operators and functions (+/-, >, sum(), median(), ...)  
* The `%in%` operator  



<!-- ======================================================= -->
### Assignment operators {-}  

**`<-`**  

The basic assignment operator in R is `<-`. Such that `object_name <- value`.  
This assignment operator can also be written as `=`. We advise use of `<-` for general R use.  
We also advise surrounding such operators with spaces, for readability.  


**`<<-`**  

If [Writing functions], or using R in an interactive way with sourced scripts, then you may need to use this assignment operator `<<-` (from **base** R). This operator is used to define an object in a higher 'parent' R Environment. See this [online reference](https://stat.ethz.ch/R-manual/R-devel/library/base/html/assignOps.html).


**`%<>%`**  

This is an "assignment pipe" from the **magrittr** package, which pipes an object forward and *also re-defines the object*. It must be the first pipe operator in the chain. It is shorthand, as shown below in two equivalent examples:  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age_months = age_years * 12)
```  

The above is equivalent to the below:  

```{r, eval=F}
linelist %<>% mutate(age_months = age_years * 12)
```

**`%<+%`**

This is used to add data to phylogenetic trees with the **ggtree** package. See the page on [Phylogenetic trees] or this online [resource book](https://yulab-smu.top/treedata-book/).  


 

<!-- ======================================================= -->
### Relational and logical operators {-}  

Relational operators compare values and are often used when defining new variables and subsets of datasets. Here are the common relational operators in R:  

Function                |Operator     |Example       |Example Result
------------------------|-------------|--------------|---------------------------
Equal to                |`==`         |`"A" == "a"`  |`FALSE` (because R is case sensitive) *Note that == (double equals) is different from = (single equals), which acts like the assignment operator `<-`*
Not equal to            |`!=`         |`2 != 0`      |`TRUE`
Greater than            |`>`          |`4 > 2`       |`TRUE`
Less than               |`<`          |`4 < 2`       |`FALSE`
Greater than or equal to|`>=`         |`6 >= 4`      |`TRUE`
Less than or equal to   |`<=`         |`6 <= 4`      |`FALSE`
Value is missing        |`is.na()`    |`is.na(7)`    |`FALSE` (see page on [Missing data])
Value is not missing    |`!is.na()`   |`!is.na(7)`   |`TRUE`

Logical operators, such as AND and OR, are often used to connect relational operators and create more complicated criteria. Complex statements might require parentheses ( ) for grouping and order of application.  

Function   |Operator
-----------|------------------------
AND        |`&`
OR         |`|` (vertical bar)
Parentheses|`( )` Used to group criteria together and clarify order  of operations


For example, below, we have a linelist with two variables we want to use to create our case definition, `hep_e_rdt`, a test result and `other_cases_in_hh`, which will tell us if there are other cases in the household. The command below uses the function `case_when()` to create the new variable `case_def` such that:

```{r eval=FALSE}
linelist_cleaned <- linelist %>%
  mutate(case_def = case_when(
    is.na(rdt_result) & is.na(other_case_in_home)            ~ NA_character_,
    rdt_result == "Positive"                                 ~ "Confirmed",
    rdt_result != "Positive" & other_cases_in_home == "Yes"  ~ "Probable",
    TRUE                                                     ~ "Suspected"
  ))
```

Criteria in example above               | Resulting value in new variable "case_def"
----------------------------------------|-------------------------------------
If the value for variables `rdt_result` and `other_cases_in_home` are missing | `NA` (missing)  
If the value in `rdt_result` is "Positive" | "Confirmed"  
If the value in `rdt_result` is NOT "Positive" AND the value in `other_cases_in_home` is "Yes" | "Probable"  
If one of the above criteria are not met | "Suspected"  


*Note that R is case-sensitive, so "Positive" is different than "positive"...*  

 
<!-- ======================================================= -->
### Missing values {-}

In R, missing values are represented by the special value `NA` (a "reserved" value) (capital letters N and A - not in quotation marks). If you import data that records missing data in another way (e.g. 99, "Missing", or .), you may want to re-code those values to `NA`. How to do this is addressed in the [Import and export] page.  

**To test whether a value is `NA`, use the special function `is.na()`**, which returns `TRUE` or `FALSE`.

```{r basics_operators_missing}
rdt_result <- c("Positive", "Suspected", "Positive", NA)   # two positive cases, one suspected, and one unknown
is.na(rdt_result)  # Tests whether the value of rdt_result is NA
```

Read more about missing, infinite, `NULL`, and impossible values in the page on [Missing data]. Learn how to convert missing values when importing data in the page on [Import and export].  



<!-- ======================================================= -->
### Mathematics and statistics {-}  

All the operators and functions in this page is automatically available using **base** R.  

#### Mathematical operators {-} 

These are often used to perform addition, division, to create new columns, etc. Below are common mathematical operators in R. Whether you put spaces around the operators is not important.  


Objective          |Example in R
-------------------|-------------
addition           | 2 + 3
subtraction        | 2 - 3
multiplication     | 2 * 3
division           | 30 / 5
exponent           | 2^3
order of operations| ( )



#### Mathematical functions {-}

Objective          |Function
-------------------|-------------
rounding           | round(x, digits = n)  
rounding           | janitor::round_half_up(x, digits = n)
ceiling (round up) | ceiling(x)
floor (round down) | floor(x)
absolute value     | abs(x)
square root        | sqrt(x)
exponent           | exponent(x)
natural logarithm  | log(x)
log base 10        | log10(x)
log base 2         | log2(x)



#### Scientific notation {-}

To turn off scientific notation in your R session, run this command:  

```{r, eval=F}
options(scipen=999)
```



#### Rounding {-}  

<span style="color: red;">**_DANGER:_** `round()` uses "banker's rounding" which rounds up from a .5 only if the upper number is even. Use `round_half_up()` from **janitor** to consistently round halves up to the nearest whole number. See [this explanation](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes) </span>  

```{r}
# use the appropriate rounding function for your work
round(c(2.5, 3.5))

janitor::round_half_up(c(2.5, 3.5))
```


#### Statistical functions:  

<span style="color: orange;">**_CAUTION:_** The functions below will by default include missing values in calculations. Missing values will result in an output of NA, unless the argument `na.rm=TRUE` is specified</span>


Objective                |Function
-------------------------|----------------------
mean (average)           | mean(x, na.rm=T)
median                   | median(x, na.rm=T)
standard deviation       | sd(x, na.rm=T)
quantiles*               | quantile(x, probs)
sum                      | sum(x, na.rm=T)
minimum value            | min(x, na.rm=T)
maximum value            | max(x, na.rm=T)
range of numeric values  | range(x, na.rm=T)
summmary**               | summary(x)

Notes:  

* `quantile()`: `x` is the numeric vector to examine, and `probs = ` is a numeric vector with probabilities within 0 and 1.0, e.g `c(0.5, 0.8, 0.85)`
* `summary()`: gives a summary on a numeric vector including mean, median, and common percentiles  

<span style="color: red;">**_DANGER:_** If providing a vector of numbers to one of the above functions, be sure to wrap the numbers within `c()` .</span>

```{r}
# If supplying raw numbers to a function, wrap them in c()
mean(1, 6, 12, 10, 5, 0)    # !!! INCORRECT !!!  

mean(c(1, 6, 12, 10, 5, 0)) # CORRECT
```





#### Other useful functions:  


Objective                   |Function            |Example
----------------------------|--------------------|-----------------------------------------------
create a sequence           | seq(from, to, by)  |`seq(1, 10, 2)`
repeat x, n times           | rep(x, ntimes)     |`rep(1:3, 2)` or `rep(c("a", "b", "c"), 3)` 
subdivide a numeric vector  | cut(x, n)          |`cut(linelist$age, 5)`
take a random sample        | sample(x, size)    |`sample(linelist$id, size = 5, replace = TRUE)`




<!-- ======================================================= -->
### `%in%` {-}  

A very useful operator for matching values, and for quickly assessing if a value is within a vector or dataframe.   

```{r}
my_vector <- c("a", "b", "c", "d")
```

```{r}
"a" %in% my_vector
"h" %in% my_vector
```

To ask if a value is **not** `%in%` a vector, put an exclamation mark (!) **in front** of the logic statement:  

```{r}
# to negate, put an exclamation in front
!"a" %in% my_vector
!"h" %in% my_vector
```

`%in%` is very useful when using the **dplyr** function `case_when()`. You can define a vector previously, and then reference it later. For example:  

```{r eval=F}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")

linelist <- linelist %>% 
  mutate(child_hospitaled = case_when(
    hospitalized %in% affirmative & age < 18 ~ "Hospitalized Child",
    TRUE                                      ~ "Not"))
```

Note: If you want to detect a partial string, perhaps using `str_detect()` from **stringr**, it will not accept a character vector like `c("1", "Yes", "yes", "y")`. Instead, it must be given a *regular expression* - one condensed string with OR bars, such as "1|Yes|yes|y". For example, `str_detect(hospitalized, "1|Yes|yes|y")`. See the page on [Characters and strings] for more information.  

You can convert a character vector to a named regular expression with this command:  

```{r}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")
affirmative

# condense to 
affirmative_str_search <- paste0(affirmative, collapse = "|")  # option with base R
affirmative_str_search <- str_c(affirmative, collapse = "|")   # option with stringr package

affirmative_str_search
```









<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Errors & warnings  

This section explains:  

* The difference between errors and warnings  
* General syntax tips for writing R code  
* Code assists  

Common errors and warnings and troubleshooting tips can be found in the page on [Errors and warnings].  



<!-- ======================================================= -->
### Error versus Warning {-}

When a command is run, the R Console may show you warning or error messages in red text.  

* A **warning** means that R has completed your command, but had to take additional steps or produced unusual output that you should be aware of.  

* An **error** means that R was not able to complete your command.  

Look for clues: 

* The error/warning message will often include a line number for the problem.  

* If an object "is unknown" or "not found", perhaps you spelled it incorrectly, forgot to call a package with library(), or forgot to re-run your script after making changes.  

If all else fails, copy the error message into Google along with some key terms - chances are that someone else has worked through this already!


<!-- ======================================================= -->
### General syntax tips {-}

A few things to remember when writing commands in R, to avoid errors and warnings:  

* Always close parentheses - tip: count the number of opening "(" and closing parentheses ")" for each code chunk
* Avoid spaces in column and object names. Use underscore ( _ ) or periods ( . ) instead
* Keep track of and remember to separate a function's arguments with commas
* R is case-sensitive, meaning `Variable_A` is *different* from `variable_A`


<!-- ======================================================= -->
### Code assists {-}  

Any script (RMarkdown or otherwise) will give clues when you have made a mistake. For example, if you forgot to write a comma where it is needed, or to close a parentheses, RStudio will raise a flag on that line, on the right side of the script, to warn you.  








```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/basics.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Transition to R { }  

Below, we provide some advice and resources if you are transitioning to R.


## From Excel  

Transitioning from Excel directly to R can seem daunting, but is a very achievable goal.  

You will find that using R (or another coding language) offers immense benefits in time saved, more consistent and accurate analysis, reproducibility, shareability, and faster error-correction. Like any new software there is a learning "curve" of time you must invest to become familiar. The dividends will be significant and immense scope of new possibilities will open to you with R.  

Excel is a well-known software that can be easy for a beginner to use to produce simple analysis and visualizations with "point-and-click". In comparison, it can take a couple weeks to become comfortable with R functions and interface. However, R has evolved in recent years to become much more friendly to beginners.  

Many Excel workflows rely on memory and on repetition - thus, there is much opportunity for error. Furthermore, in general Excel use the data cleaning, analysis methodology, and equations used are hidden from view. It can require substantial time for a new colleague to learn what an Excel workbook is doing and how to troubleshoot it. With R, all the steps are explicitly written in the script and can be easily viewed, edited, corrected, and applied to other datasets.   




### Mindset changes {-}  

To begin your transition from Excel to R you must adjust your mindset in a few important ways:  

1) Instead of clicking buttons and dragging cells you will be writing *every* step and procedure into a "script". *The script is step-by-step instructions.* This allows any colleague to read the script and easily see the steps you took. This also helps de-bug errors or inaccurate calculations. See the [R Basics] section on scripts for examples.  

Here is an example of an R script:  

```{r, echo=F, out.width = "75%", out.height="50%", fig.align = "center"}
knitr::include_graphics(here::here("images", "example_script.png"))
```



2) Use machine-readable "tidy" data over messy "human-readable" data. These are the three main requirements for "tidy" data, as explained in this tutorial on ["tidy" data in R](https://r4ds.had.co.nz/tidy-data.html):  

* Each variable must have its own column  
* Each observation must have its own row  
* Each value must have its own cell  

*The main reason one encounters non-tidy data is because many Excel spreadsheets are designed to prioritize easy reading by humans, not easy reading by machines/software.*  

To help you see the difference, below are some fictional examples of **non-tidy data** that prioritize *human*-readability over *machine*-readability:  

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_1.png"))
```


*Problems:* In the spreadsheet above, there are *merged cells* which are not easily digested by R. Which row should be considered the "header" is not clear. A color-based dictionary is to the right side and cell values are represented by colors - which is also not easily interpreted by R. Furthermore, different piece of information are combined into one cell (multiple partner organizations working in one area, or the status "TBC" in the same cell as "Partner D").  


```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_2.png"))
```


*Problems:* In the spreadsheet above, there are numerous extra empty rows and columns within the dataset - this will cause cleaning headaches in R. Furthermore, the GPS coordinates are spread across two rows for a given treatment center. As a side note - the GPS coordinates are in two different formats!  

"Tidy" datasets may not be as readable to a human eye, but they make data cleaning and analysis much easier!



### Excel-to-R resources {-}

Here are some links to tutorials to help you transition to R from Excel:  

* [R vs. Excel](https://www.northeastern.edu/graduate/blog/r-vs-excel/)  
* [RStudio course in R for Excel users](https://rstudio-conf-2020.github.io/r-for-excel/)  





## From Stata  
<!-- ======================================================= -->

**Coming to R from Stata**  

Many epidemiologists are first taught how to use Stata, and it can seem daunting to move into R. However, if you are a comfortable Stata user then the jump into R is certainly more manageable than you might think. While there are some key differences between Stata and R in how data can be created and modified, as well as how analysis functions are implemented – after learning these key differences you will be able to translate your skills.

Below are some key translations between Stata and R, which may be handy as your review this guide.


**General notes**

**STATA**                    | **R**  
---------------------------- | ---------------------------------------------    
You can only view and manipulate one dataset at a time | You can view and manipulate multiple datasets at the same time, therefore you will frequently have to specify your dataset within the code
Online community available through [https://www.statalist.org/](https://www.statalist.org/) | Online community available through [RStudio](https://community.rstudio.com/), [StackOverFlow](https://stackoverflow.com/questions/tagged/r), and [R-bloggers](https://www.r-bloggers.com/)
Point and click functionality as an option | Minimal point and click functionality
Help for commands available by `help [command]` | Help available by `[function]?` or search in the Help pane
Comment code using * or /// or  /* TEXT */ | Comment code using #
Almost all commands are built-in to Stata. New/user-written functions can be installed as **ado** files using **ssc install** [package] | R installs with **base** functions, but typical use involves installing other packages from CRAN (see page on [R basics])
Analysis is usually written in a **do** file | Analysis written in an R script in the RStudio source pane. R markdown scripts are an alternative.


**Working directory**  

**STATA**                        | **R**  
-------------------------------- | ---------------------------------------------
Working directories involve absolute filepaths (e.g. "C:/usename/documents/projects/data/")| Working directories can be either absolute, or relative to a project root folder by using the **here** package (see [Import and export]) 
See current working directory with **pwd** | Use `getwd()` or `here()` (if using the **here** package), with empty parentheses 
Set working directory with **cd** “folder location” | Use `setwd(“folder location”)`, or `set_here("folder location)` (if using **here** package)

**Importing and viewing data**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Specific commands per file type | Use `import()` from **rio** package for almost all filetypes. Specific functions exist as alternatives (see [Import and export])
Reading in csv files is done by **import delimited** “filename.csv” | Use `import("filename.csv")`
Reading in xslx files is done by **import excel** “filename.xlsx” | Use `import("filename.xlsx")`
Browse your data in a new window using the command **browse** | View a dataset in the RStudio source pane using `View(dataset)`. *You need to specify your dataset name to the function in R because multiple datasets can be held at the same time. Note capital "V" in this function*
Get a high-level overview of your dataset using **summarize**, which provides the variable names and basic information | Get a high-level overview of your dataset using `summary(dataset)`

**Basic data manipulation**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Dataset columns are often referred to as "variables" | More often referred to as "columns" or sometimes as "vectors" or "variables"
No need to specify the dataset | In each of the below commands, you need to specify the dataset - see the page on [Cleaning data and core functions] for examples
New variables are created using the command **generate** *varname* =  | Generate new variables using the function `mutate(varname = )`. See page on [Cleaning data and core functions] for details on all the below **dplyr** functions.
Variables are renamed using **rename** *old_name new_name* | Columns can be renamed using the function `rename(new_name = old_name)`
Variables are dropped using **drop** *varname* | Columns can be removed using the function `select()` with the column name in the parentheses following a minus sign
Factor variables can be labeled using a series of commands such as **label define** | Labeling values can done by converting the column to Factor class and specifying levels. See page on [Factors]. Column names are not typically labeled as they are in Stata.

**Descriptive analysis**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Tabulate counts of a variable using **tab** *varname* | Provide the dataset and column name to `table()` such as `table(dataset$colname)`. Alternatively, use `count(varname)` from the **dplyr** package, as explained in [Grouping data]
Cross-tabulaton of two variables in a 2x2 table is done with **tab** *varname1 varname2* | Use `table(dataset$varname1, dataset$varname2` or `count(varname1, varname2)`


While this list gives an overview of the basics in translating Stata commands into R, it is not exhaustive. There are many other great resources for Stata users transitioning to R that could be of interest:  

* https://dss.princeton.edu/training/RStata.pdf  
* https://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html  
* http://r4stats.com/books/r4stata/  




## From SAS  
<!-- ======================================================= -->

**Coming from SAS to R**  

SAS is commonly used at public health agencies and academic research fields. Although transitioning to a new language is rarely a simple process, understanding key differences between SAS and R may help you start to navigate the new language using your native language. 
Below outlines the key translations in data management and descriptive analysis between SAS and R.   

**General notes**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Online community available through [SAS Customer Support](https://support.sas.com/en/support-home.html)|Online community available through RStudio, StackOverFlow, and R-bloggers
Help for commands available by `help [command]`|Help available by [function]? or search in the Help pane
Comment code using `* TEXT ;` or `/* TEXT */`|Comment code using #
Almost all commands are built-in.  Users can write new functions using SAS macro, SAS/IML, SAS Component Language (SCL), and most recently, procedures `Proc Fcmp` and `Proc Proto`|R installs with **base** functions, but typical use involves installing other packages from CRAN (see page on [R basics])
Analysis is usually conducted by writing a SAS program in the Editor window.|Analysis written in an R script in the RStudio source pane. R markdown scripts are an alternative.

**Working directory**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Working directories can be either absolute, or relative to a project root folder by defining the root folder using `%let rootdir=/root path; %include “&rootdir/subfoldername/filename”`|Working directories can be either absolute, or relative to a project root folder by using the **here** package (see [Import and export])
See current working directory with `%put %sysfunc(getoption(work));`|Use `getwd()` or `here()` (if using the **here** package), with empty parentheses
Set working directory with `libname “folder location”`|Use `setwd(“folder location”)`, or `set_here("folder location)` if using **here** package


**Importing and viewing data**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Use `Proc Import` procedure or using `Data Step Infile` statement.|Use `import()` from **rio** package for almost all filetypes. Specific functions exist as alternatives (see [Import and export])
Reading in csv files is done by using `Proc Import datafile=”filename.csv” out=work.filename dbms=CSV; run;` OR using [Data Step Infile statement](http://support.sas.com/techsup/technote/ts673.pdf)|Use `import("filename.csv")`
Reading in xslx files is done by using Proc Import datafile=”filename.xlsx” out=work.filename dbms=xlsx; run;
OR using [Data Step Infile statement](http://support.sas.com/techsup/technote/ts673.pdf)|Use import("filename.xlsx")
Browse your data in a new window by opening the Explorer window and select desired library and the dataset|View a dataset in the RStudio source pane using View(dataset). You need to specify your dataset name to the function in R because multiple datasets can be held at the same time. Note capital “V” in this function

**Basic data manipulation**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Dataset columns are often referred to as “variables”|More often referred to as “columns” or sometimes as “vectors” or “variables”
No special procedures are needed to create a variable. New variables are created simply by typing the new variable name, followed by an equal sign, and then an expression for the value|Generate new variables using the function `mutate()`. See page on [Cleaning data and core functions] for details on all the below **dplyr** functions.
Variables are renamed using `rename *old_name=new_name*`|Columns can be renamed using the function `rename(new_name = old_name)`
Variables are kept using `**keep**=varname`|Columns can be selected using the function `select()` with the column name in the parentheses
Variables are dropped using `**drop**=varname`|Columns can be removed using the function `select()` with the column name in the parentheses following a minus sign
Factor variables can be labeled in the Data Step using `Label` statement|Labeling values can done by converting the column to Factor class and specifying levels. See page on [Factors]. Column names are not typically labeled.
Records are selected using `Where` or `If` statement in the Data Step. Multiple selection conditions are separated using “and” command.|Records are selected using the function `filter()` with multiple selection conditions separated either by an AND operator (&) or a comma  
Datasets are combined using `Merge` statement in the Data Step. The datasets to be merged need to be sorted first using `Proc Sort` procedure.|**dplyr** package offers a few functions for merging datasets. See page [Joining Data] for details.

**Descriptive analysis**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Get a high-level overview of your dataset using `Proc Summary` procedure, which provides the variable names and descriptive statistics|Get a high-level overview of your dataset using `summary(dataset)` or `skim(dataset)` from the **skimr** package
Tabulate counts of a variable using `proc freq data=Dataset; Tables varname; Run;`|See the page on [Descriptive tables]. Options include `table()` from **base** R, and `tabyl()` from **janitor** package, among others. Note you will need to specify the dataset and column name as R holds multiple datasets.
Cross-tabulation of two variables in a 2x2 table is done with `proc freq data=Dataset; Tables rowvar*colvar; Run;`|Again, you can use `table()`, `tabyl()` or other options as described in the [Descriptive tables] page.  

**Some useful resources:**  

[R for SAS and SPSS Users (2011)](https://www.amazon.com/SAS-SPSS-Users-Statistics-Computing/dp/1461406846/ref=sr_1_1?dchild=1&gclid=EAIaIQobChMIoqLOvf6u7wIVAhLnCh1c9w_DEAMYASAAEgJLIfD_BwE&hvadid=241675955927&hvdev=c&hvlocphy=9032185&hvnetw=g&hvqmt=e&hvrand=16854847287059617468&hvtargid=kwd-44746119007&hydadcr=16374_10302157&keywords=r+for+sas+users&qid=1615698213&sr=8-1)

[SAS and R, Second Edition (2014)](https://www.amazon.com/SAS-Management-Statistical-Analysis-Graphics-dp-1466584491/dp/1466584491/ref=dp_ob_title_bk)



## Data interoperability  
<!-- ======================================================= -->

See the [Import and export] page for details on how the R package **rio** can import and export files such as STATA .dta files, SAS .xpt and.sas7bdat files, SPSS .por and.sav files, and many others.  



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/switch_to_R.Rmd-->

# Import and export {}

In this page we describe ways to locate, import, and export files:  

* Using the **rio** package to `import()` and `export()` data  
* The **here** package to locate files in your computer or R project  
* Specific import scenarios, such as  
  * Excel sheets  
  * Google sheets 
  * Websites  
  * Skipping rows  
* Exporting/saving data files  


<!-- ======================================================= -->
## Overview

When you import a "dataset" into R, you are generally creating a new data frame object in your R environment and defining it as the imported flat file (Excel, CSV, etc.). To learn more about objects and the assignment operator, see the page on [R Basics].  


<!-- ======================================================= -->
## The **rio** package {}  

The R package we recommend is: **rio**. The `import()` function from **rio** utilizes the file extension (e.g. .xlsx, .csv, .rds, .tsv) to correctly import or export the file. The name "rio" is an abbreviation of "R I/O" (input/output).  

The alternative to using **rio** is to use functions from many other packages, each of which is specific to a type of file. For example, `read.csv()` (**base** R), `read.xlsx()` (**openxlsx** package), and `write_csv()` (**readr** pacakge), etc. These alternatives can be difficult to remember, whereas using `import()` and `export()` from **rio** is easy.  

**rio**'s functions `import()` and `export()` use the appropriate package and function for a given file, based on its file extension. See the end of this page for a complete table of which packages/functions **rio** uses in the background. It can also be used to import STATA, SAS, and SPSS files, among dozens of others.  

Import/export of shapefiles requires other packages, as detailed in the page on [GIS basics].    


<!-- ======================================================= -->
## File paths  

When importing or exporting data, you must provide a file path. You can do this one of three ways:  

1) Provide the "full" / "absolute" file path  
2) Provide a "relative" file path - the location *relative to* an R project root directory  
3) Manual file selection  



### "Absolute" file paths {-}  

This is an example of an absolute file path, placed within quotes and provided to `import()`, to be saved in R as the data frame object named `linelist`.  
In the filepath below, "epiproject" is the home/root folder of the analysis. There is a sub-folder "data" and within that a subfolder "linelists", in which there is the xlsx file.  

```{r, eval=F}
linelist <- import("C:/Users/Pierre/My Documents/epiproject/data/linelists/ebola_linelist.xlsx")
```

A few things to note about absolute file paths:  

* **Avoid using absolute file paths** as they will usually break if the script is run on a different computer
* They can be used if you must load data from a *shared drive* folder distant from where your R script is saved  
* Use *forward* slashes (`/`) as in the example above (this is *NOT* the default for Windows file paths)  
* File paths that begin with double slashes (e.g. "//...") will likely **not be recognized by R** and will produce an error. Consider moving to a "named" or "lettered" drive that begins with a letter (e.g. "J:" or "C:"). See the page on [Directory interactions] for more details on this issue.  

<span style="color: darkgreen;">**_TIP:_** To quickly convert all `\` to `/`, highlight the code of interest, use Ctrl+f (in Windows), check the option box for "In selection", and then use the replace functionality to convert them.</span>  


### "Relative" file paths {-}

"Relative" file paths consist of the file path *relative to* a particular root folder. They allow for more simple file paths that can work on different computers (e.g. if root folder is on a shared drive or is sent by email).  

Below, we assume the folder "epiproject" is the root folder of an R project and that we are using the package **here**. The same file path as above is written as *relative to* the R project root folder "epiproject". The **here** syntax is explained below.  

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```

The package **here** and its function `here()` locate files on your computer in relation to the root directory of an R project, and provide a shortcut syntax for providing a file path to a function like `import()`. This is how `here()` works:  

* Ensure you are working within an R project (read more on the [R projects] page  
* When the **here** package is first loaded within the R project, it places a small file called "here" in the root-level folder of your R project as a "benchmark" or "anchor" for all other files in the project.  
* In your script, if you want to reference a file saved in your project’s folders, you use the function `here()` to tell R where the file is located *in relation to that anchor*.  
* `here()` can be used for both importing and exporting.  

If you are unsure where “here” root is set to, run the function `here()` with empty parentheses:

```{r, eval=T}
# load the package
pacman::p_load(here)

# return the folder path that "here" is set to 
here()
```

You can build onto that anchor by specifying further folders, within quotes, separated by commas, finally ending with the filename and extension. This approach also removes slash direction as a source of error. 

Running the `here()` command with folder names and file name returns the extended file path, which can then processed by the `import()` function.

```{r}
# the filepath
here("data", "linelist.xlsx")
```



<!-- ======================================================= -->
### Select file manually {-}

You can import data manually via one of these methods:  

1) Environment RStudio Pane, click "Import Dataset", and select the type of data 
2) Click File / Import Dataset / (select the type of data)  
3) To hard-code manual selection, use the *base R* command `file.choose()` (leaving the parentheses empty) to trigger appearance of a **pop-up window** that allows the user to manually select the file from their computer. For example:  

```{r import_choose, eval=F}
# Manual selection of a file. When this command is run, a POP-UP window will appear. 
# The file path selected will be supplied to the import() command.

my_data <- import(file.choose())
```

<span style="color: darkgreen;">**_TIP:_** The **pop-up window** may appear BEHIND your RStudio window.</span>





<!-- ======================================================= -->
## Excel sheets {}

If you want to import a specific **sheet** from an Excel workbook, include the sheet name to the `which = ` argument. For example:  

```{r eval=F}
my_data <- import("my_excel_file.xlsx", which = "Sheetname")
```

If using the `here()` method to provide a relative pathway to `import()`, you can still indicate a specific sheet by adding the `which = ` argument after the closing parentheses of the `here()` function.  

```{r import_sheet_here, eval=F}
# Demonstration: importing a specific Excel sheet when using relative pathways with the 'here' package
linelist_raw <- import(here("data", "linelist.xlsx"), which = "Sheet1")`  
```

To *export* a dataframe from R to a specific Excel sheet and have the rest of the Excel workbook remain unchanged, you will have to import, edit, and export with an alternative package catered to this purpose such as **openxlsx**. See more information in the page on [Directory interactions] or [at this github page](https://ycphs.github.io/openxlsx/).

If your Excel workbook is .xlsb (binary format Excel workbook) you may not be able to import it using **rio**. Consider re-saving it as .xlsx, or using a package like **readxlsb** which is built for [this purpose](https://cran.r-project.org/web/packages/readxlsb/vignettes/read-xlsb-workbook.html).  






<!-- ======================================================= -->
## Missing values  

You may want to designate which value(s) in your dataset should be considered as missing. As explained in the page on [Missing data], the value in R for missing data is `NA`, but perhaps the dataset you want to import uses 99, "Missing", or just empty character space "" instead.  

Use the `na = ` argument for `import()` and provide the value(s) within quotes (even if they are numbers). You can specify multiple values by including them within a vector, using `c()` as shown below.  

Here, the value "99" in the imported dataset is considered missing and converted to `NA` in R.  

```{r, eval=F}
linelist <- import(here("data", "linelist_raw.xlsx"), na ="99")
```

Here, any of the values "Missing", "" (empty cell), or " " (single space) in the imported dataset are converted to `NA` in R.  

```{r, eval=F}
linelist <- import(here("data", "cleaning_dict.csv"), na = c("Missing", "", " "))
```



<!-- ======================================================= -->
## Google sheets {}

You can import data from an online Google spreadsheet with the **googlesheet4** package and by authenticating your access to the spreadsheet.  


```{r, eval=F}
pacman::p_load("googlesheets4")
```

Below, a demo Google sheet is imported and saved. This command may prompt confirmation of authentification of your Google account. Follow prompts and pop-ups in your internet browser to grant Tidyverse API packages permissions to edit, create, and delete your spreadsheets in Google Drive.  


The sheet below is "viewable for anyone with the link" and you can try to import it.  

```{r, eval=F}
Gsheets_demo <- read_sheet("https://docs.google.com/spreadsheets/d/1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY/edit#gid=0")
```

The sheet can also be imported using only the sheet ID, a shorter part of the URL:  

```{r, eval=F}
Gsheets_demo <- read_sheet("1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY")
```


Another package, **googledrive** offers useful functions for writing, editing, and deleting Google sheets. For example, using the  `gs4_create()` and `sheet_write()` functions found in this package. 

Here are some other helpful online tutorials: [basic importing tutorial](https://arbor-analytics.com/post/getting-your-data-into-r-from-google-sheets/) [more detail](https://googlesheets4.tidyverse.org/articles/googlesheets4.html) [interaction between the two packages](https://googlesheets4.tidyverse.org/articles/articles/drive-and-sheets.html)





<!-- ======================================================= -->
## Scraping websites {}

Scraping data from a website - TBD - Under construction




<!-- ======================================================= -->
## Skip rows  

Sometimes, you may want to avoid importing a row of data. You can do this with the argument `skip = ` if using `import()` from **rio** on a .xlsx or .csv file. Provide the number of rows you want to skip. 


```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx", skip = 1)  # does not import header row
```

Unfortunately `skip = ` only accepts one integer value, *not* a range (e.g. "2:10" does not work). To skip import of specific rows that are not consecutive from the top, consider importing multiple times and using `bind_rows()` from **dplyr**. See the example below of skipping only row 2.  



### Remove second header row {-}  

Sometimes, your data may have a *second* row that you want to remove, for example if it is a "data dictionary" row as shown below. This situation can be problematic because it can result in all columns being imported as class "character".  

```{r, echo=F}
# HIDDEN FROM READER
####################
# Create second header row of "data dictionary" and insert into row 2. Save as new dataframe.
linelist_2headers <- rio::import(here::here("data", "linelist_cleaned.rds")) %>%         
        mutate(across(everything(), as.character)) %>% 
        add_row(.before = 1,
                #row_num = "000",
                case_id = "case identification number assigned by MOH",
                generation = "transmission chain generation number",
                date_infection = "estimated date of infection, mm/dd/yyyy",
                date_onset = "date of symptom onset, YYYY-MM-DD",
                date_hospitalisation = "date of initial hospitalization, mm/dd/yyyy",
                date_outcome = "date of outcome status determination",
                outcome = "either 'Death' or 'Recovered' or 'Unknown'",
                gender = "either 'm' or 'f' or 'unknown'",
                hospital = "Name of hospital of first admission",
                lon = "longitude of residence, approx",
                lat = "latitude of residence, approx",
                infector = "case_id of infector",
                source = "context of known transmission event",
                age = "age number",
                age_unit = "age unit, either 'years' or 'months' or 'days'",
                fever = "presence of fever on admission, either 'yes' or 'no'",
                chills = "presence of chills on admission, either 'yes' or 'no'",
                cough = "presence of cough on admission, either 'yes' or 'no'",
                aches = "presence of aches on admission, either 'yes' or 'no'",
                vomit = "presence of vomiting on admission, either 'yes' or 'no'",
                time_admission = "time of hospital admission HH:MM")
```


```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_2headers, 5), rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```

To solve this, you will likely need to import the data twice.  

1) Import the data in order to store the correct column names  
2) Import the data again, skipping the first *two* rows (header and second rows)  
3) Bind the correct names onto the reduced dataframe

The exact argument used to bind the correct column names depends on the type of data file (.csv, .tsv, .xlsx, etc.). This is because **rio** is using a different function for the different file types (see table above).  

**For Excel files:** (`col_names = `)  

```{r, eval=F}
# import first time; store the column names
linelist_raw_names <- import("linelist_raw.xlsx") %>% names()  # save true column names

# import second time; skip row 2, and assign column names to argument col_names =
linelist_raw <- import("linelist_raw.xlsx",
                       skip = 2,
                       col_names = linelist_raw_names
                       ) 
```

**For CSV files:** (`col.names = `)  

```{r, eval=F}
# import first time; sotre column names
linelist_raw_names <- import("linelist_raw.csv") %>% names() # save true column names

# note argument for csv files is 'col.names = '
linelist_raw <- import("linelist_raw.csv",
                       skip = 2,
                       col.names = linelist_raw_names
                       ) 
```

**Backup option** - changing column names as a separate command

```{r, eval=F}
# assign/overwrite headers using the base 'colnames()' function
colnames(linelist_raw) <- linelist_raw_names
```




### Make a data dictionary {-}  

Bonus! If you do have a second row that is a data dictionary, you can easily create a proper data dictionary from it. This tip is adapted from this [post](https://alison.rbind.io/post/2018-02-23-read-multiple-header-rows/).  


```{r}
dict <- linelist_2headers %>%             # begin: linelist with dictionary as first row
  head(1) %>%                             # keep only column names and first dictionary row                
  pivot_longer(cols = everything(),       # pivot all columns to long format
               names_to = "Column",       # assign new column names
               values_to = "Description")
```


```{r message=FALSE, echo=F}
DT::datatable(dict, rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```



### Combine two header rows {-}  

In some cases, you may want to combine two header rows into one. This command will define the column names as the combination (pasting together) of the existing column names with the value underneath in the first row. Replace "df" with the name of your dataset.  

```{r, eval=F}
names(df) <- paste(names(df), df[1, ], sep = "_")
```


## Multiple files - import, export, split, combine  

See the page on [Iteration] for examples of how to import and combine multiple files, or multiple Excel workbook files. This page also has examples on how to split a data frame into parts and export each one separately, or as named sheets in an Excel workbook.  




<!-- ======================================================= -->
## Import from Github {}

Importing data directly from Github into R can be easy or annoying, depending on the file type or file size. Below are some approaches:  

### CSV files  

It can be easy to import a .csv file directly from Github into R with an R command.  

1) Go to the Github repo, locate the file of interest, and click on it  
3) Click on the "Raw" button (you will then see the "raw" csv data, as shown below)  
4) Copy the URL (web address)  
5) Use the URL in the `import()` R command, as shown below  

```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_csv_raw.png"))
```

### XLSX files  

You may not be able to view the "Raw" data for some files (e.g. .xlsx, .rds, .nwk)
1) Go to the **["data" folder](https://github.com/nsbatra/Epi_R_handbook/tree/master/data)** of our Github repository  
2) Click on "linelist_raw.xlsx"  
3) Click the "Download" button as shown below  
4) Save the file on your computer, and import it into R with the `import()` function from **rio**, as described in the page on [Import and export].  


```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_xlsx.png"))
```

### Shapefiles {-} 

Shapefiles have many sub-component files, each with a different file extention. One file will have the ".shp" extension, but others may have ".dbf", ".prj", etc.  To download a shapefile from Github, you will need to download each of the sub-component files individually, and save them in the *same* folder on your computer. In Github, click on each file individually and download them by clicking on the "Download" button.  

Once saved to your computer you can import the shapefile as shown in the [GIS basics] page using `st_read()` from the **sf** package. You only need to provide the filepath and name of the ".shp" file - as long as the other related files are within the same folder on your computer.  

Below, you can see how the shapefile "sle_adm3" consists of many files - each of which must be downloaded from Github.  

```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```



<!-- ======================================================= -->
## Manual data entry {}

### Entry by rows {-}  

Use the `tribble` function from the **tibble** package from the tidverse ([onlinetibble reference](https://tibble.tidyverse.org/reference/tribble.html)).  
  
Note how column headers start with a *tilde* (`~`).  Also note that each column must contain only one class of data (character, numeric, etc.). You can use tabs, spacing, and new rows to make the data entry more intuitive and readable. Spaces do not matter between values, but each row is represented by a new line of code. For example:  

```{r import_manual_row}
# create the dataset manually by row
manual_entry_rows <- tibble::tribble(
  ~colA, ~colB,
  "a",   1,
  "b",   2,
  "c",   3
  )
```

And now we display the new dataset:  

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_rows)
```


### Entry by columns {-}  

Since a data frame consists of vectors (vertical columns), the **base** approach to manual dataframe creation in R expects you to define each column and then bind them together. This can be counter-intuitive in epidemiology, as we usually think about our data in rows (as above). 

```{r import_manual_col}
# define each vector (vertical column) separately, each with its own name
PatientID <- c(235, 452, 778, 111)
Treatment <- c("Yes", "No", "Yes", "Yes")
Death     <- c(1, 0, 1, 0)
```

<span style="color: orange;">**_CAUTION:_** All vectors must be the same length (same number of values).</span>

The vectors can then be bound together using the function `data.frame()`:  

```{r}
# combine the columns into a data frame, by referencing the vector names
manual_entry_cols <- data.frame(PatientID, Treatment, Death)
```

And now we display the new dataset:  

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_cols)
```




### Pasting from clipboard {-}  


If you copy data from elsewhere and have it on your clipboard, you can try the following function from **base** R to convert those data into an R data frame:  

```{r, eval=F}
df_from_clipboard <- read.table(
  file = "clipboard",  # specify this as "clipboard"
  sep = "t",           # separator could be tab, or commas, etc.
  header=TRUE)         # if there is a header row
```




<!-- ======================================================= -->
## Export {}  

With **rio**, you can use the `export()` function in a very similar way to `import()`. First give the name of the R object you want to save (e.g. `linelist`) and then in quote the filepath including name and file extension. For example:  

```{r, eval=F}
export(linelist, "my_linelist.xlsx") # will save to working directory
```

You could save the same dataframe as a .csv, and to a folder specified by **here** relative pathway:  

```{r, eval=F}
export(linelist, here("data","clean", "my_linelist.csv")
```



## RDS files {}

Along with .csv, .xlsx, etc, you can also export/save R data frames as .rds files. This is a file format specific to R, and is very useful if you know you will work with the exported data again in R. 

The classes of columns are stored, so you don't have do to cleaning again when it is imported (with an Excel or even a CSV file this can be a headache!).  

For example, if you work in an Epi team and need to send files to a GIS team for mapping, and they use R as well, just send them the .rds file! Then all the column classes are retained and they have less work to do.  

```{r, eval=F}
export(linelist, here("data","clean", "my_linelist.rds")
```



<!-- ======================================================= -->
## Rdata files {}

`.Rdata` files store R objects, and can actually store multiple R objects within one file, for example multiple dataframes, model results, lists, etc. This can be very useful to consolidate or share a lot of your data for a given project.  

In the below example, multiple R objects are stored within the exported file "my_objects.Rdata":  

```{r, eval=F}
rio::export(my_list, my_dataframe, my_vector, "my_objects.Rdata")
```

Note: if you are trying to *import* a list, use `import_list()` from **rio** to import it with the complete original structure and contents.  

```{r, eval=F}
rio::import_list("my_list.Rdata")
```





<!-- ======================================================= -->
## Saving plots {} 

How to save plots, such as those created by `ggplot()` is discussed in depth in the [ggplot tips] page.  

In brief, run `ggsave("my_plot_filepath_and_name.png")` after printing your plot. You can either provide a saved plot object to the `plot = ` argument, or only specify the destination file path (with file extension) to save the most recently-displayed plot. You can also control the `width = `, `height = `, `units = `, and `dpi = `.  

How to save a network graph, such as a transmission tree, is addressed in the page on [Transmission chains]. 


<!-- ======================================================= -->
## Resources {} 

The [R Data Import/Export Manual](https://cran.r-project.org/doc/manuals/r-release/R-data.html)  
[R 4 Data Science chapter](https://r4ds.had.co.nz/data-import.html#data-import)  
[ggsave](https://ggplot2.tidyverse.org/reference/ggsave.html)  


Below is a table, taken from the **rio** online [vignette](https://cran.r-project.org/web/packages/rio/vignettes/rio.html). For each type of data it shows: the expected file extension, the package **rio** uses to import or export the data, and whether this functionality is included in the default installed version of **rio**.  



Format                     | Typical Extension | Import Package    | Export Package     | Installed by Default
---------------------------|-------------------|-------------------|--------------------|---------------------
Comma-separated data | .csv | data.table `fread()` | data.table |	Yes
Pipe-separated data |	.psv | data.table `fread()` | data.table | Yes
Tab-separated data| .tsv | data.table `fread()` | data.table | Yes
SAS | .sas7bdat | haven | haven | Yes
SPSS | .sav | haven | haven | Yes
Stata | .dta | haven | haven | Yes
SAS | XPORT | .xpt | haven | haven | Yes
SPSS Portable | .por | haven | | Yes
Excel | .xls | readxl | | Yes
Excel | .xlsx | readxl | openxlsx | Yes
R syntax | .R	| base | base | Yes
Saved R objects | .RData, .rda | base | base | Yes
Serialized R objects | .rds | base | base | Yes
Epiinfo | .rec | foreign | | Yes
Minitab | .mtp | foreign | | Yes
Systat | .syd |	foreign | | Yes
“XBASE” | database files | .dbf | foreign | foreign | Yes
Weka Attribute-Relation File Format | .arff | foreign | foreign | Yes
Data Interchange Format | .dif | utils | | Yes
Fortran data | no recognized extension | utils | | Yes
Fixed-width format data | .fwf | utils | utils | Yes
gzip comma-separated data | .csv.gz | utils | utils | Yes
CSVY (CSV + YAML metadata header) | .csvy | csvy | csvy | No
EViews | .wf1 |hexView | | No
Feather R/Python interchange format | .feather | feather | feather | No
Fast Storage | .fst | fst |	fst | No
JSON | .json | jsonlite | jsonlite | No
Matlab | .mat | rmatio | rmatio | No
OpenDocument Spreadsheet | .ods | readODS | readODS | No
HTML Tables | .html | xml2 | xml2 | No
Shallow XML documents | .xml | xml2 | xml2 | No
YAML | .yml | yaml | yaml	| No
Clipboard	default is tsv | |  clipr | clipr | No



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/importing.Rmd-->


# R projects {}  


An R project enables your work to be bundled in a self-contained folder. Within the project, all the relevant scripts, data files, figures/outputs, and history are be stored in sub-folders and importantly - the *working directory* is the project's root folder.  

## Suggested use  

A common, efficient, and trouble-free way to use R is to combine these 3 elements. Each is described in the sections below.  

1) An **R project**  
     - A self-contained working environment with folders for data, scripts, outputs, etc.  
2) The **here** package for relative filepaths  
     - Filepaths are written relative to the root folder of the R project - see [Import and export] for more information  
3) The **rio** package for importing/exporting  
     - `import()` and `export()` handle any file type by by its extension (e.g. .csv, .xlsx, .png)  
     
     


<!-- ======================================================= -->
## Creating an R project {}

To create an R project, select "New Project" from the File menu.

* If you want to create a new folder for the project, select "New directory" and indicate where you want it to be created.  
* If you want to create the project within an existing folder, click "Existing directory" and indicate the folder.  
* If you want to clone a Github repository, select the third option "Version Control" and then "Git". See the page on [Collaboration with Github] for further details.  


```{r out.width = "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "create_project.png"))
```


The R project you create will come in the form of a folder containing a *.Rproj* file. This file is a shortcut and likely the primary way you will open your project. You can also open a project by selecting "Open Project" from the File menu. Alternatively on the far upper right side of RStudio you will see an R project icon and a drop-down menu of available R projects. 

To exit from an R project, either open a new project, or close the project (File - Close Project).  


### Switch projects {-}

To switch between projects, click the R project icon and drop-down menu at the very top-right of RStudio. You will see options to Close Project, Open Project, and a list of recent projects.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Rproject_dropdown.png"))
```


### Settings {-}  

It is generally advised that you start RStudio each time with a "clean slate" - that is, with your workspace **not** preserved from your previous session. This will mean that your objects and results will not persist session-to-session (you must re-create them by running your scripts). This is good, because it will force you to write better scripts and avoid errors in the long run.  

To set RStudio to have a "clean slate" each time at start-up:  

* Select "Project Options" from the Tools menu.  
* In the "General" tab, set RStudio to **not** restore .RData into workspace at startup, and to **not** save workspace to .RData on exit.  



### Organization {-}  

It is common to have subfolders in your project. Consider having folders such as "data", "scripts", "figures", "presentations".  

### Version control {-}  

Consider a version control system. It could be something as simple as having dates on the names of scripts (e.g. "transmission_analysis_2020-10-03.R") and an "archive" folder. Consider also having commented header text at the top of each script with a description, tags, authors, and change log.  

A more complicated method would involve using Github or a similar platform for version control. See the page on [Collaboration with Github].  

One tip is that you can search across an entire project or folder using the "Find in Files" tool (Edit menu). It can search and even replace strings across multiple files.  






## Examples  

Below are some examples of import/export/saving using `here()` from within an R projct. Read more about using the **here** package in the [Import and export] page.  


*Importing `linelist.xlsx` from the "data" folder in your R project*  

```{r eval=F}
linelist <- import(here("data", "linelist.xlsx"))
```

*Exporting the R object `linelist` as "my_linelist.rds" to the "clean" folder within the "data" folder in your R project.*   

```{r, eval=F}
export(linelist, here("data","clean", "my_linelist.rds")
```

*Saving the most recently printed plot as "epicurve_2021-02-15.png" within the "epicurves" folder in "outputs" folder in your R project.*  

```{r, eval=F}
ggsave(here::here("outputs", "epicurves", "epicurve_2021-02-15.png"))
```




<!-- ======================================================= -->
## Resources {}

RStudio webpage on [using R projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/r_projects.Rmd-->

# Suggested packages

Below is a long list of suggested packages for common epidemiological work in R. You can copy this code, run it, and all of these packages will install from CRAN and load for use in the current R session. If a package is already installed, it will be loaded for use only.  

You can modify the code with `#` symbols to exclude any packages you do not want.  

Of note:  

* Install the **pacman** package first before running the below code. You can do this with `install.packages("pacman")`. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use in the current R session. You can also load packages that are already installed with `library()` from **base** R.  
* In the code below, packages that are included when installing/loading another package are indicated by an indent and hash. For example how **ggplot2** is listed under **tidyverse**.  
* The code below only installs packages available on CRAN - not from Github  
* If multiple packages have functions with the same name, *masking* can occur when the function from the more recently-loaded package takes precedent. Read more in the [R Basics] page. Consider using the package **conflicted** to manage such conflicts.  
* See the [R basics] section on packages for more information on **pacman** and masking.  

To see the versions of R, RStudio, and R packages used during the production of this handbook, see the page on [Editorial and technical notes].  


```{r, eval=F}

# List of useful epidemiology R packages  

pacman::p_load(
     
     # learning R
     learnr,   # interactive tutorials in RStudio
        
     # project and file management
     here,     # filepaths relative to root project folder
     rio,      # import/export of many types of data
     openxlsx, # import/export of Excel workbooks 
     
     # package install and management
     pacman,   # package install/load
     renv,     # managing versions of packages when working in collaborative groups
     remotes,  # install from github
     
     # General data management
     tidyverse,    # includes many packages for tidy data wrangling and presentation
          #dplyr,
          #tidyr,
          #ggplot2,
     linelist,     # cleaning linelists
     lubridate,    # working with dates
     naniar,       # assessing missing data
     
     # statistics  
     gtsummary,    # making descriptive and statistical tables
     janitor,      # tables and data cleaning
     
     # epidemic modeling
     epicontacts,  # Analysing transmission networks
     EpiNow2,      # Rt estimation
     EpiEstim,     # Rt estimation
     projections,  # Incidence projections
     incidence2,   # Make epicurves and handle incidence data
     i2extras,     #
     epitrix,      # Useful epi functions
     distcrete,    # Discrete delay distributions
     
     
     # plots - general
     #ggplot2,         # included in tidyverse
     cowplot,          # combining plots
     RColorBrewer,     # color scales
     
     # plots - specific types
     DiagrammeR,       # diagrams using DOT language
     incidence,        # epidemic curves
     gghighlight,      # highlight a subset
     ggrepel,          # smart labels
     
     # gis
     sf,               # to manage spatial data using a Simple Feature format
     tmap,             # to produce simple maps, works for both interactive and static maps
     OpenStreetMap,    # to add OSM basemap in ggplot map
     
     # routine reports  
     rmarkdown,        # produce PDFs, Word Documents, Powerpoints, and HTML files
     reportfactory,    # Auto-organization of Rmarkdown outputs
     
     # tables
     knitr,            # report generation, kable() for html tables
     flextable,        # HTML tables
     DT,               # HTML tables
     gt,               # HTML tables
     
     # phylogenetics  
     ggtree,           # visualization and annotation of trees
     ape,              # analysis of phylogenetics and evolution
     
     # interactive
     plotly,           # interactive graphics
     shiny             # interactive web apps  
)


```

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/packages_suggested.Rmd-->

# (PART) Data Management {-}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/cat_data_management.Rmd-->

# Cleaning data and core functions {}


```{r, out.height = "10%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "cleaning.png"))
```


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->


This page demonstrates common steps necessary to clean a dataset, starting with importing raw data and demonstrating a "pipe chain" of cleaning steps. We use a simulated Ebola case linelist, which is referenced often in this handbook.  

This page also explains the use of many core functions used in data management, including:  

Function       | Utility                               | Package
---------------|---------------------------------------|------------------------------
` %>% `|pipe to pass the dataset from one function to the next|**magrittr** (also **tidyverse**)
`mutate()`|to create, transform, and re-define columns|**dplyr** (also **tidyverse**) 
`select()`|to select or re-name columns| "
`filter()`|to keep certain rows| " 
`across()`|to transform multiple columns at one time| " 
`rename()`|to rename columns| " 
`add_row()`|to add rows manually| " 
`distinct()`|to de-duplicate rows| " 
`recode()`|to re-code values in a column| " 
`case_when()`|to re-code values in a column using more complex logical criteria| " 
`clean_names()`|to standardize the syntax of column names|**janitor**
`replace_na()`, `na_if()`, `coalesce()`|special functions for re-coding|**tidyr** (also **tidyverse**)
`as.character()`, `as.numeric()`, `as.Date()`, etc.|to convert the class of a column|**base** R
`clean_data()`|to re-code/clean using a data dictionary|**linelist**
`age_categories()` and `cut()`|to create categorical groups from a numeric column|**epikit** and **base** R


If you want to see how these functions compare to Stata or SAS, see the page on [Transition to R].  


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Cleaning pipeline

**This page proceeds through typical cleaning steps, adding them sequentially to a cleaning pipe chain.**

In epidemiological analysis and data processing, cleaning steps are often performed linked together, sequentially. In R this often manifests as a cleaning "pipeline", where *the raw dataset is passed or "piped" from one cleaning step to another*.  

Such chain utilize **dplyr** "verb" functions and the **magrittr** pipe operator `%>%`. This pipe begins with the "raw" data ("linelist_raw.xlsx") and ends with a "clean" R data frame (`linelist`).  

In a cleaning pipeline the order of the steps is important. Cleaning steps might include:  

* Importing of data  
* Column names cleaned or changed  
* De-duplication  
* Column creation and transformation (e.g. re-coding or cleaning values)  
* Rows filtered or added  



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Load packages  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, message = F}
pacman::p_load(
  rio,        # importing data  
  here,       # relative file pathways  
  janitor,    # data cleaning and tables
  lubridate,  # working with dates
  epikit,     # age_categories() function
  tidyverse   # data manipulation and visualization
)
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Import data  

### Import {-}  

Here we import the raw .xlsx case linelist using the `import()` function from the package **rio**, and save it as the data frame `linelist_raw`. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page.  

If your dataset is large and takes a long time to import, it can be useful to have the import command be separate from the pipe chain and the "raw" saved as a distinct file. This also allows easy comparison between the original and cleaned versions.  

See the page on [Import and export] for more details and unusual situations, including:  

* Skipping the import of certain rows  
* Dealing with a second row that is a data dictionary  
* Importing from Google sheets   


Below we import the raw .xlsx file. We assume it is located in the working directory and so no sub-folders are specified in the filepath.  

```{r, echo=F, message=F}
# HIDDEN FROM READER
# actually load the data using here()
linelist_raw <- rio::import(here::here("data", "linelist_raw.xlsx"))
```

```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx")
```

You can view the first 50 rows of the the original "raw" dataset below. You can use the **base** R function `head(n)` to view just the first `n` lines in the console.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_raw,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
### Review {-}  

You can use the function `skim()` from the package **skimr** to get an overview of the entire dataframe (see page on [Descriptive tables] for more info). Columns are summarised by class (character, numeric, POSIXct - a type of date class).  


```{r, eval=F}
skimr::skim(linelist_raw)
```

```{r, echo=F}
skimr::skim_without_charts(linelist_raw)
```




 





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Column names {} 

Column names are used very often, so they must have "clean" syntax. We suggest the following:  

* Short names
* No spaces (replace with underscores _ ) 
* No unusual characters (&, #, <, >, ...)  
* Similar style nomenclature (e.g. all date columns named like **date_**onset, **date_**report, **date_**death...)  

The columns names of `linelist_raw` are printed below using `names()` from **base** R. We can see that initially:  

* Some names contain spaces (e.g. `infection date`)  
* Different naming patterns are used for dates (`date onset` vs. `infection date`)  
* There must have been a *merged header* across the two last columns in the .xlsx. We know this because the name of two merged columns ("merged_header") was applied to the first one, and the second column was assigned a placeholder  name "...28", as it was then empty and is the 28th column.  

```{r}
names(linelist_raw)
```

<span style="color: black;">**_NOTE:_** To reference a column name that include spaces, surround the name with back-ticks, for example: linelist$`` `r '\x60infection date\x60'` ``. note that on your keyboard, the back-tick (`) is different from the single quotation mark (').</span>




### Automatic cleaning {-}  

The function `clean_names()` from the package **janitor** standardizes column names and makes them unique by doing the following:  

* Converts all names to consist of only underscores, numbers, and letters  
* Accented characters are transliterated to ASCII (e.g. german o with umlaut becomes "o", spanish "enye" becomes "n")  
* Capitalization preference can be specified using the `case = ` argument ("snake" is default, alternatives include "sentence", "title", "small_camel"...)  
* You can specify name replacements with the `replace = ` argument (e.g. `replace = c(onset = "date_of_onset")`)  
* Here is an online [vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#cleaning)  

Below, the cleaning pipeline begins by using `clean_names()` on the raw linelist.  

```{r clean_names}
# send the dataset through the function clean_names()
linelist <- linelist_raw %>% 
  janitor::clean_names()

# see the new names
names(linelist)
```

<span style="color: black;">**_NOTE:_** The last column name "...28" was changed to "x28".</span>


### Manual name cleaning {-}  

Re-naming columns manually is often necessary, even after the standardization step above. Below, re-naming is performed using the `rename()` function from the **dplyr** package, as part of a pipe chain. `rename()` uses the style "NEW = OLD", the new column name is given before the old column name.  

Below, a re-name command is added to the cleaning pipeline:  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome)
```


**Now you can see that the columns names have been changed:**  

```{r message=FALSE, echo=F}
names(linelist)
```


#### Rename by column position {-} 

You can also rename by column position, instead of column name, for example:  

```{r, eval=F}
rename(newNameForFirstColumn  = 1,
       newNameForSecondColumn = 2)
```



#### Rename via `select()` {-}  

You can also rename columns within the **dplyr** `select()` function, which is used to retain only certain columns (and is covered later in this page). This approach also uses the format `new_name = old_name`. Here is an example:  

```{r, eval=F}
linelist_raw %>% 
  select(# NEW name             # OLD name
         date_infection       = `infection date`,    # rename and KEEP ONLY these columns
         date_hospitalisation = `hosp date`)
```





### Other challenges {-}  


#### Empty Excel column names {-} 

R cannot have dataset columns that do not have column names (headers). So, if you import an Excel dataset with data but no column headers, R will fill-in the headers with names like "...1" or "...2". The number represents the column number (e.g. if the 4th column in the dataset has no header, then R will name it "..4").  

You can clean these names manually by referencing their position number (see example above), or their assigned name (`linelist_raw$...1`).  



#### Merged Excel column names and cells {-}  

Merged cells in an Excel file are a common occurrence when receiving data from operational teams. Merged cells can be nice for human reading of data, but cause many problems for machine reading of data. R cannot accommodate merged cells. 

Remind people doing data entry that **human-readable data is not the same as machine-readable data**. Strive to train users about the principles of [**tidy data**](https://r4ds.had.co.nz/tidy-data.html). If at all possible, try to change procedures so that data arrive in a tidy format without merged cells.  

* Each variable must have its own column.  
* Each observation must have its own row.  
* Each value must have its own cell.  

When using **rio**'s `import()` function, the value in a merged cell will be assigned to the first cell and subsequent cells will be empty.  

One solution to deal with merged cells is to import the data with the function `readWorkbook()` from package **openxlsx**. Set the argument `fillMergedCells = TRUE`. This gives the value in a merged cell to all cells within the merge range.

```{r, eval=F}
linelist_raw <- openxlsx::readWorkbook("linelist_raw.xlsx", fillMergedCells = TRUE)
```

<span style="color: red;">**_DANGER:_** If column names are merged with `readWorkbook()`, you will end up with duplicate column names, which you will need to fix manually - R does not work well with duplicate column names! You can re-name them by referencing their position (e.g. column 5), as explained in the section on manual column name cleaning..</span>






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Select or re-order columns {} 

Use `select()` from **dplyr** to select the columns you want to retain, and specify their order in the data frame. 

<span style="color: orange;">**_CAUTION:_** In the examples below, the `linelist` data frame is modified with `select()` and displayed, but not saved. This is for demonstration purposes. The modified column names are printed by piping the data frame to `names()`.</span>

**Here are ALL the column names in the linelist at this point in the cleaning pipe chain:**

```{r}
names(linelist)
```

### Keep columns {-}  

**Select only the columns you want to remain**  

Put their names in the `select()` command, with no quotation marks. They will appear in the data frame in the order you provide. Note that if you include a column that does not exist, R will return an error (see use of `any_of()` below if you want no error in this situation).  

```{r}
# linelist dataset is piped through select() command, and names() prints just the column names
linelist %>% 
  select(case_id, date_onset, date_hospitalisation, fever) %>% 
  names()  # display the column names
```




### Helper functions {-}  

Helper functions and operators exist to make it easy to specify columns to keep or discard.  

For example, if you want to re-order the columns, `everything()` is useful to signify "all other columns not yet mentioned". The command below pulls columns `date_onset` and `date_hospitalisation` to the beginning, but keeps all the others afterward:  

```{r}
# move date_onset and date_hospitalisation to beginning
linelist %>% 
  select(date_onset, date_hospitalisation, everything()) %>% 
  names()
```

Here are other helpers functions that work *within* `select()`:  

* `everything()`  - all other columns not mentioned  
* `last_col()`    - the last column  
* `where()`       - applies a function to all columns and selects those which are TRUE  
* `starts_with()` - matches to a specified prefix  
  * example: `select(starts_with("date"))`  
* `ends_with()`   - matches to a specified suffix  
  * example: `select(ends_with("_end"))`  
* `contains()`    - columns containing a character string  
  * example: `select(contains("time"))`  
* `matches()`     - to apply a regular expression (regex)  
  * example: `select(contains("[pt]al"))`  
* `num_range()`   - a numerical range like x01, x02, x03  
* `any_of()`      - matches IF column exists but returns no error if it is not found  
  * example: `select(any_of(date_onset, date_death, cardiac_arrest))`  

In addition, use normal operators such as `c()` to list several columns, `:` for consecutive columns, `!` for opposite, `&` for AND, and `|` for OR.  


Use `where()` to specify logical criteria for columns. If providing a function inside `where()`, do not include the empty parentheses. The command below selects columns that are class Numeric.

```{r}
# select columns that are class Numeric
linelist %>% 
  select(where(is.numeric)) %>% 
  names()
```

Use `contains()` to select only columns in which the column name contains a string. `ends_with()` and `starts_with()` provide more nuance.  

```{r}
# select columns containing certain characters
linelist %>% 
  select(contains("date")) %>% 
  names()
```

The function `matches()` works similarly to `contains()` but can be provided a regular expression (see page on [Characters and strings]), such as multiple strings separated by OR bars within the parentheses:  

```{r}
# searched for multiple character matches
linelist %>% 
  select(matches("onset|hosp|fev")) %>%   # note the OR symbol "|"
  names()
```

<span style="color: orange;">**_CAUTION:_** If a column name that you specifically provide does not exist in the data, it can return an error and stop your code. Consider using `any_of()` to cite columns that may or may not exist, especially useful in negative (remove) selections.</span>

Only one of these columns exists, but no error is produced and the code continues. 
```{r}
linelist %>% 
  select(any_of(c("date_onset", "village_origin", "village_detection", "village_residence", "village_travel"))) %>% 
  names()
```



### Remove columns {-} 

**Indicate which columns to remove** by placing a minus symbol "-" in front of the column name (e.g. `select(-outcome)`), or a vector of column names (as below). All other columns will be retained. 

```{r}
linelist %>% 
  select(-c(date_onset, fever:vomit)) %>% # remove onset and all cols from fever to vomit
  names()
```

You can also remove a column using **base** R by defining it as `NULL`. For example:  

```{r, eval=F}
linelist$date_onset <- NULL   # deletes column with base R syntax 
```



### Standalone {-}

`select()` can also be used as an independent command (not in a pipe chain). In this case, the first argument is the original dataframe to be operated upon.  

```{r}
# Create a new linelist with id and age-related columns
linelist_age <- select(linelist, case_id, contains("age"))

# display the column names
names(linelist_age)
```



#### Add to the pipe chain {-}  

In the `linelist_raw`, there are a few columns we do not need: `row_num`, `merged_header`, and `x28`. We remove them with a `select()` command in the cleaning pipe chain:  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################

    # remove column
    select(-c(row_num, merged_header, x28))
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Deduplication


See the handbook page on [De-duplication] for extensive options on how to de-duplicate data. Only a very simple row de-duplication example is presented here.  

The package **dplyr** offers the `distinct()` function. This function examines every row and reduce the data frame to only the unique rows. That is, it removes rows that are 100% duplicates.  

When evaluating duplicate rows, it takes into account a range of columns - by default it considers all columns. As shown in the de-duplication page, you can adjust this column range so that the uniqueness of rows is only evaluated in regards to certain columns.  

In this simple example, we just add the empty command `distinct()` to the pipe chain. This ensures there are no rows that are 100% duplicates of other rows (evaluated across all columns).  

We begin with `r nrow(linelist)` rows in `linelist`. 

```{r}
linelist <- linelist %>% 
  distinct()
```

After de-duplication there are `r nrow(linelist)` rows. Any removed rows would have been 100% duplicates of other rows.  

Below, the `distinct()` command is added to the cleaning pipe chain:

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################
    
    # de-duplicate
    distinct()
```





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Column creation and transformation { }


**We recommend using the dplyr function `mutate()` to add a new column, or to modify an existing one.**  

Below is an example of creating a new column with `mutate()`. The syntax is: `mutate(new_column_name = value or transformation)`  

In Stata, this is similar to the command `generate`, but R's `mutate()` can also be used to modify an existing column.  


### New columns {-}

The most basic `mutate()` command to create a new column might look like this. It creates a new column `new_col` where the value in every row is 10.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(new_col = 10)
```

You can also reference values in other columns, to perform calculations. For example below a new column `bmi` is created to hold the Body Mass Index (BMI) for each case - as calculated using the formula BMI = kg/m^2, using column `ht_cm` and column `wt_kg`.  

```{r}
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

If creating multiple new columns, separate each with a comma and new line. Below, are examples of new columns, including pasting together values from other columns using `str_glue()` from the **stringr** package (see page on [Characters and strings].  

```{r}
linelist <- linelist %>%                       
  mutate(
    new_var_dup    = case_id,             # new column = duplicate/copy another existing column
    new_var_static = 7,                   # new column = all values the same
    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables
    new_var_paste  = stringr::str_glue("{hospital} on ({date_hospitalisation})") # new column = pasting together values from other columns
    ) 
```

***Scroll to the right to see the new columns that have been added (first 50 rows shown):***  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: darkgreen;">**_TIP:_** A variation on `mutate()` is the function `transmute()`. This function adds a new column just like `mutate()`, but also drops/removes all other columns that you do not mention within its parentheses.</span>


```{r, echo=F}
# HIDDEN FROM READER
# removes new demo columns created above
linelist <- linelist %>% 
  select(-contains("new_var"))
```



### Convert column class {-}
  
Often you will need to set the correct class for a column. There are ways to set column class during the import commands, but often this is often cumbersome. See section on [object classes](#objectclasses) to learn more about converting the class of objects, including columns.  

First, run some checks on important columns to see if they are the correct class:  

Currently, the class of the "age" column is character. To perform quantitative analyses, we need these numbers to be recognized as numeric! 

```{r}
class(linelist$age)
```

The class of the "date_onset" column is also character! To perform analyses, these dates must be recognized as dates! 
 
```{r}
class(linelist$date_onset)
```


In this case, use `mutate()` to define the column as itself, but converted to a different class. Here is a basic example, converting or ensuring that the column `age` is class Numeric:  

```{r}
linelist <- linelist %>% 
  mutate(age = as.numeric(age))
```

In a similar way, you can use `as.character()`, `as.double()`, `as.logical()`.  

To convert to class Factor, you can use `factor()` from **base** R or `as_factor()` from **forcats**. Read more about this in the [Factors] page.  

Converting to class date you must take care. Several methods are explained on the page [Working with dates]. Typically, the raw date values must all be in the same format for conversion to work correctly (e.g "MM/DD/YYYY", or "DD MM YYYY"). After converting to class Date, check your data visually or with a cross-table to confirm that each value was converted correctly.  




### Grouped data {-}  

If your dataframe is already *grouped* (see page on [Grouping data]), `mutate()` may behave differently than if the dataframe is not grouped. Any summarizing functions, like `mean()`, `median()`, `max()`, etc. will be based on only the grouped rows, not all the rows.     

```{r, eval=F}
# age normalized to mean of ALL rows
linelist %>% 
  mutate(age_norm = age / mean(age, na.rm=T))

# age normalized to mean of hospital group
linelist %>% 
  group_by(hospital) %>% 
  mutate(age_norm = age / mean(age, na.rm=T))
```

Read more about using mutate on grouped dataframes in this [tidyverse mutate documentation](https://dplyr.tidyverse.org/reference/mutate.html).  



### Transform multiple columns {-}


Often to write concise code you want to apply the same transformation to multiple columns at once. A transformation can be applied to multiple columns at once using the `across()` function from the package **dplyr** (also contained within **tidyverse** package). `across()` can be used with any **dplyr** function, but commonly with `select()`, `mutate()`, `filter()`, or `summarise()`. See how it is applied to `summarise()` in the page on [Descriptive tables].  

You specify the columns to `.cols = ` and the function(s) to `.fns`. Any additional arguments to provide to the function can be included after a comma, still within `across()`.   

#### `across()` column selection {-}  

Specify the columns to the argument `.cols = ` - you can name them individually, or use helper functions. Specify the function to `.fns = `. Note that using the function mode demonstrated below, the function is written *without* its parentheses ( ).  

Here the transformation `as.character()` is applied to specific columns named within `across()`. 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = c(temp, ht_cm, wt_kg), .fns = as.character))
```

There are helpers available to assist you in specifying columns:  

* `everything()`  - all other columns not mentioned  
* `last_col()`    - the last column  
* `where()`       - applies a function to all columns and selects those which are TRUE  
* `starts_with()` - matches to a specified prefix  
  * example: `across(starts_with("date"))`  
* `ends_with()`   - matches to a specified suffix  
  * example: `across(ends_with("_end"))`  
* `contains()`    - columns containing a character string  
  * example: `across(contains("time"))`  
* `matches()`     - to apply a regular expression (regex)  
  * example: `across(contains("[pt]al"))`  
* `num_range()`   - 
* `any_of()`      - matches if column is named. Useful if the name might not exist  
  * example: `across(any_of(date_onset, date_death, cardiac_arrest))`  

Here is an example of how one would change **all columns** to character class:  

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = everything(), .fns = as.character))
```

Columns where the name contains the string "date" (note placement of commas and parentheses):  

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = contains("date"), .fns = as.character))
```

Below, we want to mutate the columns where they are class POSIXct (a datetime class that shows timestamps) - in other words, where the function `is.POSIXct()` evaluates to `TRUE`. Then we want to apply the function `as.Date()` to these columns to convert them to a normal class Date.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = where(lubridate::is.POSIXct), .fns = as.Date))
```

* Note that within `across()` we also use the function `where()`  
* Note that `is.POSIXct()` is from the package **lubridate**. Other similar functions (`is.character()`, `is.numeric()`, and `is.logical()`) are from **base R**  

#### `across()` functions {-}

You can read the documentation with `?across` for details on how to provide functions to `across()`. A few summary points: there are several ways to specify the function(s) to perform on a column and you can even define your own functions:  

* You can provide the function name alone (e.g. `mean` or `as.character`)  
* You can provide the function in **purrr**-style (e.g. `~ mean(.x, na.rm = TRUE)`)  
* You can specify multiple functions by providing a list (e.g. `list(mean = mean, n_miss = ~ sum(is.na(.x))`).  
  * If you provide multiple functions, multiple transformed columns will be returned with unique names (e.g. col_fn). You can adjust how the new columns are named with the `.names =` argument using **glue** syntax (see page on [Characters and strings]) where `{.col}` and `{.fn}` are shorthand for the column and function.  
  
  
Here are a few online resources on using `across()`: [creator Hadley Wickham's thoughts/rationale](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-colwise/)




### `coalesce()` {-}  

This **dplyr** function finds the first non-missing value at each position. 

Say you have two vectors/columns, one for village of detection and another for village of residence. You can use coalesce to pick the first non-missing value for each index:  

```{r}
village_detection <- c("a", "b", NA,  NA)
village_residence <- c("a", "c", "a", "d")

village <- coalesce(village_detection, village_residence)
village    # print
```

This works the same if you provide data frame columns: for each row, the function will assign the new column value with the first non-missing value in the columns you provided (in order provided).

```{r, eval=F}
linelist <- linelist %>% 
  mutate(village = coalesce(village_detection, village_residence))
```

For more complicated row-wise calculations, see the section below on Row-wise calculations.  



### Cumulative math {-}

If you want a column to reflect the cumulative sum/mean/min/max etc as assessed down the rows of a dataframe, use the following functions:  

`cumsum()` returns the cumulative sum, as shown below:  

```{r}
sum(c(2,4,15,10))     # returns only one number
cumsum(c(2,4,15,10))  # returns the cumulative sum at each step
```

This can be used in a dataframe when making a new column. For example, to calculate the cumulative number of cases per day in an outbreak, consider code like this:  

```{r, warning=F, message=F}
cumulative_case_counts <- linelist %>% 
  count(date_onset) %>%                 # count of rows per day   
  mutate(cumulative_cases = cumsum(n))  # new column of the cumulative sum at that row
```

Below are the first 10 rows:  

```{r}
head(cumulative_case_counts, 10)
```

See the page on [Epidemic curves] for how to plot cumulative incidence with the epicurve.  

See also:  
cumsum(), cummean(), cummin(), cummax(), cumany(), cumall()  




### Using **base** R {-}  

To define a new column (or re-define a column) using **base** R, write the name of data frame with the *new* column (or the column to be modified). Use the assignment operator `<-` to define the new value(s). Remember that when using **base** R you must specify the data frame name before the column name every time (e.g. `dataframe$column`). Here is an example of creating the `bmi` column using **base** R:  

```{r, eval=F}
linelist$bmi = linelist$wt_kg / (linelist$ht_cm / 100) ^ 2)
```




#### Add to pipe chain {-}  

**Below, a new column is added to the pipe chain and some classes are converted.**  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    # add new column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>% 
  
    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) 
```





## Re-code values

Here are a few scenarios where you need to re-code (change) values:  

* to edit one specific value (e.g. one date with an incorrect year or format)  
* to reconcile values not spelled the same
* to create a new column of categories  
* to create a new column of numeric categories (e.g. age categories)  



### Specific values {-}  

To change values manually you can use the `recode()` function within the `mutate()` function. 

Imagine there is a nonsensical date in the data (e.g. "2014-14-15"): you could fix the date in the source data, or, you could write the change into the cleaning pipeline via `mutate()` and `recode()`.  

```{r, eval=F}
# fix incorrect values                   # old value       # new value
linelist <- linelist %>% 
  mutate(date_onset = recode(date_onset, "2014-14-15" = "2014-04-15"))
```

The `mutate()` line above can be read as: "mutate the column `date_onset` to equal the column `date_onset` re-coded so that OLD VALUE is changed to NEW VALUE". Note that this pattern (OLD = NEW) for `recode()` is the opposite of most R patterns (new = old). The R development community is working on revising this.  

**Here is another example re-coding multiple values within one column.** 

In `linelist` the values in the column "hospital" must be cleaned. There are several different spellings and many missing values.

```{r}
table(linelist$hospital, useNA = "always")
```

The `recode()` command below re-defines the column "hospital" as the current column "hospital", but with the specified recode changes. Don't forget commas after each!  

```{r}
linelist <- linelist %>% 
  mutate(hospital = recode(hospital,
                      #    reference: OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      ))
```


Now we see the spellings in the `hospital` column have been corrected and consolidated:  

```{r}
table(linelist$hospital, useNA = "always")
```

<span style="color: darkgreen;">**_TIP:_** The number of spaces before and after an equals sign does not matter. Make your code easier to read by aligning the = for all or most rows. Also, consider adding a hashed comment row to clarify for future readers which side is OLD and which side is NEW. </span>  

<span style="color: darkgreen;">**_TIP:_** Sometimes a *blank* character value exists in a dataset (not recognized as R's value for missing - `NA`). You can reference this value with two quotation marks with no space inbetween ("").</span>  




### Missing values {-} 

See the page on [Missing data] for more detailed tips on identifying and handling missing values. For example, the `is.na()` function which logically tests for missingness. 

**dplyr** offers two special functions for handling missing values in the context of data cleaning:  

**`replace_na()`**  

To change missing values (`NA`) to a specific value, such as "Missing", use the function `replace_na()` within `mutate()`. Note that this is used in the same manner as `recode` above - the name of the variable must be repeated within `replace_na()`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = replace_na(hospital, "Missing"))
```


**`na_if()`**  

To convert a *specific value* to `NA`, use `na_if()`. The command below performs the opposite operation of `replace_na()`. In the example below, any values of "Missing" in the column `hospital` are converted to `NA`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = na_if(hospital, "Missing"))
```

Note: `na_if()` **cannot be used for logic criteria** (e.g. "all values > 99") - use `replace()` or `case_when()` for this:  

```{r, eval=F}
# Convert temperatures above 40 to NA 
linelist <- linelist %>% 
  mutate(temp = replace(temp, temp > 40, NA))

# Convert onset dates earlier than 2000 to missing
linelist <- linelist %>% 
  mutate(date_onset = replace(date_onset, date_onset > as.Date("2000-01-01"), NA))
```




### By logic {-}

Below is demonstrated how to re-code values in a column using logic and conditions:  

* Using `replace()`, `ifelse()` and `if_else()` for simple logic
* Using `case_when()` for more complex logic  



### Simple logic {-}  


#### `replace()` {-}  

To re-code with simple logical criteria, you can use `replace()` within `mutate()`. `replace()` is a function from **base** R. Use a logic condition to specify the rows to change . The general syntax is:  

`mutate(col_to_change = replace(col_to_change, criteria for rows, new value))`.  

One common situation is **changing just one value in one row, using an unique row identifier**. Below, the gender is changed to "Female" in the row where the column `case_id` is "2195".  

```{r, eval=F}
# Example: change gender of one specific observation to "Female" 
linelist <- linelist %>% 
  mutate(gender = replace(gender, case_id == "2195", "Female")
```

The equivalent command using **base** R syntax and the indexing brackets `[ ]` is below. It reads as "Change the value of the dataframe `linelist`'s column `gender` (for the rows where `linelist`'s column `case_id` has the value  '2195') to 'Female' ".   

```{r, eval=F}
linelist$gender[linelist$case_id == "2195"] <- "Female"
```




#### `ifelse()` and `if_else()` {-}  

Another tool for simple logical re-coding is `ifelse()` and its partner `if_else()`. However, in most cases it is better to use `case_when()` (for clarity).  

These commands are simplified versions of an `if` and `else` programming statement. The general syntax is:  
`ifelse(condition, value to return if condition evaluates to TRUE, value to return if condition evaluates to FALSE)` 

Below, the column `source_known` is defined (or re-defined). Its value in a given row is set to "known" if the row's value in column `source` is *not* missing. If the value in `source` *is* missing, then the value in `source_known` is set to "unknown".  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(source_known = ifelse(!is.na(source), "known", "unknown"))
```

`if_else()` is a special version from **dplyr** that handles dates. Note that if the 'true' value is a date, the 'false' value must also qualify a date, hence using the special character `NA_real_` instead of just `NA`.

```{r, eval=F}
# Create a date of death column, which is NA if patient has not died.
linelist <- linelist %>% 
  mutate(date_death = if_else(outcome == "Death", date_outcome, NA_real_))
```

**Avoid stringing together many ifelse commands... use `case_when()` instead!** `case_when()` is much easier to read and you'll make fewer errors.  

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "ifelse bad.png"))
```

Outside of the context of a data frame, if you want to have an object used in your code switch its value, consider using `switch()` from **base** R. See the section on using `switch()` in the page on having an [Interactive console].




### Complex logic {-}  

Use **dplyr**'s `case_when()` if you need to use complex logic statements to re-code values. There are important differences from `recode()` in syntax and logic order!  

`case_when()` commands have a Right-Hand Side (RHS) and a Left-Hand Side (LHS) separated by a "tilde" `~`. The logic criteria are in the LHS and the pursuant value is on the RHS. Statements are separated by commas. It is important to note that:  

* Statements are evaluated in the order written - from top-to-bottom. Thus it is best to write the most specific criteria first, and the most general last.  
* End with `TRUE` on the LHS, which signifies any row value that did not meet any of the previous criteria  
* The values on the RHS must all be the same class - either numeric, character, logical, etc.  
  * To assign `NA`, you may need to use special values such as `NA_character_`, `NA_real_` (for numeric or POSIX), and `as.Date(NA)`  
  
Below we utilize the columns `age` and `age_unit` to create a column `age_years`:  

```{r}
linelist <- linelist %>% 
  mutate(age_years = case_when(
            age_unit == "years"  ~ age,       # if age is given in years
            age_unit == "months" ~ age/12,    # if age is given in months
            is.na(age_unit)      ~ age,       # if age unit is missing, assume years
            TRUE                 ~ NA_real_)) # any other circumstance assign missing
```





### Cleaning dictionary {-}

Use the package **linelist** to clean a linelist with a *cleaning dictionary*.  

1) Import a cleaning dictionary with 3 columns:  
    * A "from" column (the incorrect value)  
    * A "to" column (the correct value)  
    * A column specifying the column for the changes to be applied (or ".global" to apply to all columns)  

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "cleaning_dict.png"))
```

```{r, echo=F}
cleaning_dict <- rio::import(here("data", "cleaning_dict.csv"))
```

```{r, eval=F}
cleaning_dict <- import("cleaning_dict.csv")
```

2) Store names of any columns that you want to "protect" from the changes. They must be provided to `clean_data()` as a numeric or logical vector, so you will see use of `names(.)` in the command below (the dot means the dataframe).  

```{r}
protected_cols <- c("case_id", "source")
```

3) Run `clean_data()`, specifying the cleaning dictionary

```{r}
linelist <- linelist %>% 
  linelist::clean_data(
    wordlists = cleaning_dict,
    spelling_vars = "col",       # dict column containing column names, defaults to 3rd column in dict
    protect = names(.) %in% protected_cols
  )
```

Scroll too see how values have changed - particularly `gender` (lowercase to uppercase), and all the symptoms columns have been transformed from yes/no to 1/0.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<span style="color: orange;">**_CAUTION:_** `clean_data()` from **linelist** package will also clean values in your data unless those columns are protected - you may encounter changes to columns with dashes "-" or  .</span>


Note that your column names in the cleaning dictionary must correspond to the names at this point in your cleaning script. `clean_data()` itself also implements a column name cleaning function similar to `clean_names()` from **janitor** that standardizes column names prior to applying the dictionary.  

See this [online reference for the linelist package](https://www.repidemicsconsortium.org/linelist/reference/clean_data.html) for more details.





#### Add to pipe chain {-}  

**Below, some new columns and column transformations are added to the pipe chain.**  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
   # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
   ###################################################

    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_))
```






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Numeric categories {#num_cats}


Here we describe some special approaches for creating numeric categories. Common examples include age categories, groups of lab values, etc. Here we will discuss:  

* `age_categories()`, from the **epikit** package  
* `cut()`, from **base** R  
* `case_when()`  
* quantile breaks  


### Review distribution {-}

For this example we will create an `age_cat` column using the `age_years` column.  

```{r}
#check the class of the linelist variable age
class(linelist$age_years)
```

First, examine the distribution of your data, to make appropriate cut-points. See the page on how to [Plot continuous data].  

```{r, out.height='50%'}
# examine the distribution
hist(linelist$age_years)
```

```{r}
summary(linelist$age_years, na.rm=T)
```

<span style="color: orange;">**_CAUTION:_** Sometimes, numeric variables will import as class "character". This occurs if there are non-numeric characters in some of the values, for example an entry of "2 months" for age, or (depending on your R locale settings) if a comma is used in the decimals place (e.g. "4,5" to mean four and one half years)..</span>


<!-- ======================================================= -->
### `age_categories()` {-}

With the **epikit** package, you can use the `age_categories()` function to easily categorize and label numeric columns (note: this function can be applied to non-age numeric variables too). Of note: *the output is an ordered factor.* 

Here are the required inputs:  

* A numeric vector (column)  
* The `breakers = ` - a numeric vector of break points for the new groups  

First, the most simple example:  

```{r}
# Simple example
################
pacman::p_load(epikit)

linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years,
      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70)))

# show table
table(linelist$age_cat, useNA = "always")
```

The break values you specify are by default included in the "higher" group - groups are "open" on the lower/left side. As shown below, you can add 1 to each break value to achieve groups that are open at the top/right.
 
```{r}
# Include upper ends for the same categories
############################################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 6, 11, 16, 21, 31, 41, 51, 61, 71)))

# show table
table(linelist$age_cat, useNA = "always")
```


You can adjust how the labels are displayed with `separator = `. The default is "-"  

You can adjust the upper cut-off of values allowed to be included in a group. Use `ceiling = `, the default is FALSE. If TRUE, the highest break value is a "ceiling" and a category "XX+" is not included. Any values above highest break value or `upper` (if defined) are categorized as `NA`. Below is an example with `ceiling = TRUE`, so that there is no category of XX+ and values above 70 (the highest break value) are assigned as NA.  

```{r}
# With ceiling set to TRUE
##########################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70),
      ceiling = TRUE)) # 70 is ceiling, all above become NA

# show table
table(linelist$age_cat, useNA = "always")
```

Alternatively, instead of `breakers = `, you can provide all of `lower = `, `upper = `, and `by = `:  

* `lower = ` The lowest number you want considered - default is 0  
* `upper = ` The highest number you want considered  
* `by = `    The number of years between groups  

```{r}
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      lower = 0,
      upper = 100,
      by = 10))

# show table
table(linelist$age_cat, useNA = "always")
```


See the function's Help page for more details (enter `?age_categories` in the R console). 


<!-- ======================================================= -->
### `cut()` {-}

You can also use the **base** R function `cut()`, which creates categories from a numeric column. The differences from `age_categories()` are:  

* You do not need to install/load another package  
* You can specify whether groups are open/closed on the right/left  
* You must provide accurate labels yourself  
* If you want 0 included in the lowest group you must specify this  

The basic syntax within `cut()` is to first provide the numeric variable to be cut (age_years), and then the *breaks* argument, which is a numeric vector (`c()`) of break points. Using `cut()`, the resulting column is an ordered factor. If used within `mutate()` (a **dplyr** verb) it is not necessary to specify the dataframe before the column name (e.g. `linelist$age_years`).

Create new column of age categories (`age_cat`) by cutting the numeric `age_year` column at specified break points.  

* Specify numeric vector of break points  
* Default behavior for `cut()` is that lower break values are *excluded* from each category, and upper break values are *included*. This is the opposite behavior from the `age_categories()` function.  
* Include 0 in the lowest category by adding `include.lowest = TRUE`  
* Add a vector of customized labels using the `labels = ` argument  
* Check your work with cross-tabulation of the numeric and category columns - be aware of missing values  


Below is a detailed description of the behavior of using `cut()` to make the `age_cat` column. Key points:    

* Inclusion/exclusion behavior of break points  
* Custom category labels  
* Handling missing values  
* **Check your work!**  

A simple example of `cut()` applied to `age_years` to make the new variable `age_cat` is below:  

```{r}
# Create new variable, by cutting the numeric age variable
# by default, upper break is excluded and lower break excluded from each category
linelist <- linelist %>% 
  mutate(
    age_cat = cut(
      age_years,
      breaks = c(0, 5, 10, 15, 20,
                 30, 50, 70, 100),
      include.lowest = TRUE         # include 0 in lowest group
      ))

# tabulate the number of observations per group
table(linelist$age_cat, useNA = "always")
```

* **By default**, the categorization occurs so that the right/upper side is "open" and inclusive (and the left/lower side is "closed" or exclusive). The default labels use the notation "(A, B]", which means the group does not include A (the lower break value), but includes B (the upper break value). **Reverse this behavior by providing the `right = TRUE` argument**.  

* Thus, **by default** "0" values are excluded from the lowest group, and categorized as `NA`. "0" values could be infants coded as age 0. To change this **add the argument `include.lowest = TRUE`**. Then, any "0" values are included in the lowest group. The automatically-generated label for the lowest category will change from "(0,B]" to "[0,B]", which signifies that 0 values are included.  

* **Check your work!!!** Verify that each age value was assigned to the correct category by cross-tabulating the numeric and category columns. Examine assignment of boundary values (e.g. 15, if neighboring categories are 10-15 and 15-20).  

```{r class.source = 'fold-hide'}
# Cross tabulation of the numeric and category columns. 
table("Numeric Values" = linelist$age_years,   # names specified in table for clarity.
      "Categories"     = linelist$age_cat,
      useNA = "always")                        # don't forget to examine NA values
```




**Reverse break inclusion behavior in `cut()`**  

Lower break values will be included in each category (and upper break values excluded) if the argument `right = ` is included and and set to `TRUE`. This is applied below - note how the values have shifted among the categories.  

<span style="color: black;">**_NOTE:_** If you include the `include.lowest = TRUE` argument **and** `right = TRUE`, the extreme inclusion will now apply to the *highest* break point value and category, not the lowest.</span>  

```{r class.source = 'fold-show'}
linelist <- linelist %>% 
  mutate(
    age_cat = cut(
      age_years,
      breaks = c(0, 5, 10, 15, 20,
                 30, 50, 70, 100),  # same breaks as above
      right = FALSE,                # include each *lower* break point
      include.lowest = TRUE         # include *highest* value *highest* group
      ))                                                 

table(linelist$age_cat, useNA = "always")
```

**Add labels**  

As these are manually written, be very careful to ensure they are accurate! Check your work using cross-tabulation, as described below. Below is the same code as above, with manual labels added.  

```{r class.source = 'fold-show'}
linelist <- linelist %>% 
  mutate(
    age_cat = cut(
      age_years,
      breaks = c(0, 5, 10, 15, 20,
                 30, 50, 70, 100),  # same breaks as above
      right = FALSE,                # include each *lower* break point
      include.lowest = TRUE,        # include *highest* value *highest* group
      labels = c("0-4", "5-9", "10-14",
                 "15-19", "20-29", "30-49",
                 "50-69", "70-100")
      ))

table(linelist$age_cat, useNA = "always")
```


**Re-labeling `NA` values with `cut()`**

Because `cut()` does not automatically label `NA` values, you may want to assign a label such as "Missing". This requires a few extra steps because `cut()` automatically classified the new column `age_cat` as class Factor (a rigid class limited to the defined values). 

First, convert `age_cut` from Factor to Character class, so you have flexibility to add new character values (e.g. "Missing"). Otherwise you will encounter an error. Then, use the **dplyr** verb `replace_na()` to replace `NA` values with a character value like "Missing". These steps can be combined into one step, as shown below.  

Note that Missing has been added, **but the order of the categories is now wrong (alphabetical considering numbers as characters).**  

```{r}
linelist <- linelist %>% 
  
  # cut() creates age_cat, automatically of class Factor      
  mutate(age_cat = cut(age_years,
                          breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          
                          right = FALSE,
                          include.lowest = TRUE,        
                          labels = c("0-4", "5-9", "10-14", "15-19",
                                     "20-29", "30-49", "50-69", "70-100")),
         
         # convert to class Character, and replace NA with "Missing"
         age_cat = replace_na(as.character(age_cat), "Missing"))


table(linelist$age_cat, useNA = "always")
```

To fix this, re-convert `age_cat` to a factor, and define the order of the levels correctly.

```{r}
linelist <- linelist %>% 
  
  # cut() creates age_cat, automatically of class Factor      
  mutate(age_cat = cut(age_years,
                          breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          
                          right = FALSE,
                          include.lowest = TRUE,        
                          labels = c("0-4", "5-9", "10-14", "15-19",
                                     "20-29", "30-49", "50-69", "70-100")),
         
         # convert to class Character, and replace NA with "Missing"
         age_cat = replace_na(as.character(age_cat), "Missing"),
         
         # re-classify age_cat as Factor, with correct level order and new "Missing" level
         age_cat = factor(age_cat, levels = c("0-4", "5-9", "10-14", "15-19", "20-29",
                                              "30-49", "50-69", "70-100", "Missing")))    
  

table(linelist$age_cat, useNA = "always")
```
If the above seems cumbersome, consider using `age_categories()` instead, as described before.  

**Make breaks and labels**  

For a fast way to make breaks and labels manually, use something like below. See the [R Basics] page for references on `seq()` and `rep()`.  

```{r, eval=F}
# Make break points from 0 to 90 by 5
age_seq = seq(from = 0, to = 90, by = 5)
age_seq

# Make labels for the above categories, assuming default cut() settings
age_labels = paste0(age_seq+1, "-", age_seq + 5)
age_labels

# check that both vectors are the same length
length(age_seq) == length(age_labels)
```


Read more about `cut()` in its Help page by entering `?cut` in the R console.  




### Quantile breaks {-}  

Make breaks from `quantile()`. This is from the **stats** package which comes in **base** R.  

```{r}
age_quantiles <- quantile(linelist$age_years, c(0, .25, .50, .75, .90, .95), na.rm=T)
age_quantiles

# to return only the numbers use unname()
age_quantiles <- unname(age_quantiles)
age_quantiles
```

You can then use these as break points in `age_categories()` or `cut()`.  




<!-- ======================================================= -->
### `case_when()` {-}

The dplyr function `case_when()` can also be used to create numeric categories.  

* Allows explicit setting of break point inclusion/exclusion  
* Allows designation of label for `NA` values in one step  
* More complicated code  
* Allow more flexibility to include other variables in the logic  

**If using `case_when()` please review the proper use as described earlier in this page, as logic and order of assignment are important understand to avoid errors.**

<span style="color: orange;">**_CAUTION:_** In `case_when()` all right-hand side values must be of the same class. Thus, if your categories are character values (e.g. "20-30 years") then any designated outcome for `NA` age values must also be character (either "Missing", or the special `NA_character_` instead of `NA`).</span>

You will need to designate the column as a factor (by wrapping `case_when()` in the function `factor()`) and provide the ordering of the factor levels using the `levels = ` argument *after* the close of the `case_when()` function. When using `cut()`, the factor and ordering of levels is done automatically.  


```{r}
linelist <- linelist %>% 
  mutate(
    age_cat = factor(case_when(
      # provide the case_when logic and outcomes
      age_years >= 0 & age_years < 5     ~ "0-4",          
      age_years >= 5 & age_years < 10    ~ "5-9",
      age_years >= 10 & age_years < 15   ~ "10-14",
      age_years >= 15 & age_years < 20   ~ "15-19",
      age_years >= 20 & age_years < 30   ~ "20-29",
      age_years >= 30 & age_years < 50   ~ "30-49",
      age_years >= 50 & age_years < 70   ~ "50-69",
      age_years >= 45 & age_years <= 100 ~ "70-100",
      is.na(age_years)                   ~ "Missing",      # if age_years is missing
      TRUE                               ~ "Check value"), # trigger for review
      
      # define the levels order for factor()
      levels = c("0-4","5-9", "10-14",
                 "15-19", "20-29", "30-49",
                 "50-69", "70-100", "Missing", "Check value")))
```

And now view the results with a table of the new column:  

```{r}
table(linelist$age_cat, useNA = "always")
```



### Add to pipe chain {-}  

Below, code to create two categorical age columns is added to the cleaning pipe chain:  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################   
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5)))
```








<!-- ======================================================= -->
## Add rows  

Remember that each column must contain values of only one class (either character, numeric, logical, etc.). So adding a row requires nuance to maintain this. 

```{r, eval=F}
linelist <- linelist %>% 
  add_row(row_num = 666,
          case_id = "abc",
          generation = 4,
          `infection date` = as.Date("2020-10-10"),
          .before = 2)
```

Use `.before` and `.after.` to place the row you want to add. `.before = 3` will put the new row before the 3rd row. The default behavior is to add the row to the end. Columns not specified will be left empty.  

The new *row number* may look strange ("...23") but the row numbers in the pre-existing rows *have* changed. So if using the command twice, examine/test the insertion carefully.

If a class you provide is off you will see an error like this:  

```
Error: Can't combine ..1$infection date <date> and ..2$infection date <character>.
```

(when inserting a row with a date value, remember to wrap the date in the function `as.Date()` like `as.Date("2020-10-10")`).







<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Filter rows {  }


A typical early cleaning step is to filter the dataframe for specific rows using the **dplyr** verb `filter()`. Within `filter()`, give the logic that must be `TRUE` for a row in the dataset to be kept. 

Below is shown how to filter rows based on simple and complex logical conditions, and how to filter/subset rows as a stand-alone command and with **base** R

<!-- ======================================================= -->
### Simple `filter()` {-} 

This simple example re-defines the dataframe `linelist` as itself, having filtered the rows to meet a logical condition. **Only the rows where the logical statement within the parentheses is `TRUE` are kept.**  

In this case, the logical statement is `!is.na(case_id)`, which is asking whether the value in the column `case_id` is **not** missing (`NA`). Thus, rows where `case_id` is **not** missing are kept.  

Before the filter is applied, the number of rows in `linelist` is `r nrow(linelist)`.

```{r}
linelist <- linelist %>% 
  filter(!is.na(case_id))  # keep only rows where case_id is not missing
```

After the filter is applied, the number of rows in `linelist` is `r nrow(linelist)`. 



<!-- ======================================================= -->
### Complex `filter()` {-} 

A more complex example using `filter()`:  

#### Examine the data  {-}  

Below is a simple one-line command to create a histogram of onset dates. See that a second smaller outbreak from 2012-2013 is also included in this raw dataset. **For our analyses, we want to remove entries from this earlier outbreak.**  

```{r, out.width = "50%"}
hist(linelist$date_onset, breaks = 50)
```


#### How filters handle missing numeric and date values {-}  

Can we just filter by `date_onset` to rows after June 2013? **Caution! Applying the code `filter(date_onset > as.Date("2013-06-01")))` would remove any rows in the later epidemic with a missing date of onset!**  

<span style="color: red;">**_DANGER:_** Filtering to greater than (>) or less than (<) a date or number can remove any rows with missing values (`NA`)! This is because `NA` is treated as infinitely large and small.</span>

*(See the page on [Working with dates] for more information on working with dates and the package **lubridate**)*

#### Design the filter {-}  

Examine a cross-tabulation to make sure we exclude only the correct rows:  


```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

What other criteria can we filter on to remove the first outbreak (in 2012 & 2013) from the dataset? We see that:  

* The first epidemic  in 2012 & 2013 occurred at Hospital A, Hospital B, and that there were also 10 cases at Port Hospital.  
* Hospitals A & B did *not* have cases in the second epidemic, but Port Hospital did.  

We want to exclude:  

* The `r nrow(linelist %>% filter(hospital %in% c("Hospital A", "Hospital B") | date_onset < as.Date("2013-06-01")))` rows with onset in 2012 and 2013 at either hospital A, B, or Port:  
  * Exclude `r nrow(linelist %>% filter(date_onset < as.Date("2013-06-01")))` rows with onset in 2012 and 2013
  * Exclude `r nrow(linelist %>% filter(hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` rows from Hospitals A & B with missing onset dates  
  * Do **not** exclude `r nrow(linelist %>% filter(!hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` other rows with missing onset dates.  

We start with a linelist of `nrow(linelist)`. Here is our filter statement:  

```{r}
linelist <- linelist %>% 
  # keep rows where onset is after 1 June 2013 OR where onset is missing and it was a hospital OTHER than Hospital A or B
  filter(date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))

nrow(linelist)
```

When we re-make the cross-tabulation, we see that Hospitals A & B are removed completely, and the 10 Port Hospital cases from 2012 & 2013 are removed, and all other values are the same - just as we wanted.  
 
```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

Multiple statements can be included within one filter command (separated by commas), or you can always pipe to a separate filter() command for clarity.  


*Note: some readers may notice that it would be easier to just filter by `date_hospitalisation` because it is 100% complete with no missing values. This is true. But `date_onset` is used for purposes of demonstrating a complex filter.* 




### Standalone {-}  

Filtering can also be done as a stand-alone command (not part of a pipe chain). Like other **dplyr** verbs, in this case the first argument must be the dataset itself.  

```{r, eval=F}
# dataframe <- filter(dataframe, condition(s) for rows to keep)

linelist <- filter(linelist, !is.na(case_id))
```

You can also use **base** R to subset using square brackets which reflect the [rows, columns] that you want to retain.  

```{r, eval=F}
# dataframe <- dataframe[row conditions, column conditions] (blank means keep all)

linelist <- linelist[!is.na(case_id), ]
```

<span style="color: darkgreen;">**_TIP:_** Use bracket-subset syntax with `View()` to quickly review a few records.</span>




### Quickly review records {-} 

This **base** R syntax can be handy when you want to quickly view a subset of rows and columns. Use the **base** R `View()` command (note the capital "V") around the [ ] subset you want to see. The result will appear as a dataframe in your RStudio viewer panel. For example, if I want to review onset and hospitalization dates of 3 specific cases:  

View the linelist in the viewer panel:  

```{r, eval=F}
View(linelist)
```

View specific data for three cases:  

```{r, eval=F}
View(linelist[linelist$case_id %in% c("11f8ea", "76b97a", "47a5f5"), c("date_onset", "date_hospitalisation")])
```

Note: the above command can also be written with **dplyr** verbs `filter()` and `select()` as below:  

```{r, eval=F}
View(linelist %>%
       filter(case_id %in% c("11f8ea", "76b97a", "47a5f5")) %>%
       select(date_onset, date_hospitalisation))
```





#### Add to pipe chain {-}  


```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_)) %>% 
  
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5))) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    filter(
          # keep only rows where case_id is not missing
          !is.na(case_id),  
          
          # also filter to keep only the second outbreak
          date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))
```







<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Row-wise calculations  

If you want to perform a calculation within a row, you can use `rowwise()` from **dplyr**. See the vignette on [row-wise calculations](https://cran.r-project.org/web/packages/dplyr/vignettes/rowwise.html)

For example, this code applies `rowwise()` and then creates a new column that sums the number of symptoms per case:  

```{r, eval=F}
linelist <- linelist %>%
  rowwise() %>%
  mutate(num_symptoms = sum(c(fever, chills, cough, aches, vomit) == "yes"))
```








```{r, echo=F}
# HIDDEN
#
# convert one remaining old outbreak row to missing for ease
linelist <- linelist %>% 
  mutate(
    date_hospitalisation = case_when(
      date_hospitalisation < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                         ~ date_hospitalisation),
    date_outcome = case_when(
      date_outcome < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                 ~ date_outcome)
    )

min(linelist$date_hospitalisation, na.rm=T)
min(linelist$date_outcome, na.rm=T)
```



```{r echo=F}
# REARRANGE COLUMNS FOR EXPORT
linelist <- linelist %>% 
  select(case_id:gender, age, age_unit, age_years, age_cat, age_cat5, everything())
```

```{r echo=F}
# EXPORT CLEANED LINELIST FILE TO "DATA" FOLDER
rio::export(linelist, here::here("data", "linelist_cleaned.xlsx"))
rio::export(linelist, here::here("data", "linelist_cleaned.rds"))
```
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/cleaning.Rmd-->


# Working with dates {}


<!-- ======================================================= -->
## Overview

Working with dates in R can be more difficult than working with other object classes. Below, we offer some tools and example to make this process less painful. Luckily, dates can be wrangled easily with practice, and with a set of helpful packages. 

Upon import of raw data, R often interprets dates as character objects - this means they cannot be used for general date operations such as making time series and calculating time intervals. To make matters more difficult, there are many ways a date can be formatted and you must help R know which part of a date represents what (month, day, hour, etc.). 

Dates in R are their own class of object - the `Date` class. It should be noted that there is also a class that stores objects with date *and* time. Date time objects are formally referred to as  and/or `POSIXt`, `POSIXct`, and/or `POSIXlt` classes (the difference isn't important). These objects are informally referred to as *datetime* classes.

* It is important to make R recognize when a column contains dates.  
* Dates are an object class and can be tricky to work with.  
* Here we present several ways to convert date columns to Date class.  


<!-- ======================================================= -->
## Preparation

### Load packages {-}  

This code chunk shows the loading of packages required for this page. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r dates_packages, warning=F, message=F}
# Checks if package is installed, installs if necessary, and loads package for current session

pacman::p_load(
  lubridate,  # general package for handling and converting dates  
  linelist,   # has function to "guess" messy dates
  aweek,      # another option for converting dates to weeks, and weeks to dates
  zoo,        # additional date/time functions
  tidyverse,  # data management and visualization  
  rio)        # data import/export
```

### Import data {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow along step-by-step, see instruction in the [Download book and data] page.  

```{r, eval=T, echo=F}
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")

```



<!-- ======================================================= -->
## Current date and time  

You can get the system date or system datetime by doing the following with **base** R.  

```{r eval=T}
# get the system date - this is a DATE class
Sys.Date()

# get the system time - this is a DATETIME class
Sys.time()
```


With the **lubridate** package these can also be returned with `today()` and `now()`, respectively. `date()` returns the current date and time with weekday and month names.  
  
  

<!-- ======================================================= -->
## Convert to Date class

After importing a dataset into R, date column values may look like "1989/12/30", "05/06/2014", or "13 Jan 2020". In these cases, R is likely still treating these values as Character values. R must be *told* that these values are dates... and what the format of the date is (which part is Day, which is Month, which is Year, etc).  

Once told, R converts these values to class Date. In the background, R will store the dates as numbers (the number of days from the "origin" date 1 Jan 1970). You will not interface with the date number often, but this allows for R to treat dates as continuous variables and to allow special operations such as calculating the distance between dates.  

By default, values of class Date in R are displayed as YYYY-MM-DD. Later in this section we will discuss how to change the display of date values.  

Below we present two approaches to converting a column from character values to class Date.  


<span style="color: darkgreen;">**_TIP:_** You can check the current class of a column with **base** R function `class()`, like `class(linelist$date_onset)`.</span>  

  

### **base** R {-}  

`as.Date()` is the standard, **base** R function to convert an object or column to class Date (note capitalization of "D").  

Use of `as.Date()` requires that:  

* You *specify the **existing** format of the raw character date* or the origin date if suppling dates as numbers (see section on Excel dates)  
* If used on a character column, all date values must have the same format (if this is not the case, try `guess_dates()` from the **linelist** package)  

**First**, check the class of your column with `class()` from **base** R. If you are unsure or confused about the class of your data (e.g. you see "POSIXct", etc.) it can be easiest to first convert the column to class Character with `as.character()`, and then convert it to class Date.  

**Second**, within the `as.Date()` function, use the `format =` argument to tell R the *current* format of the character date components - which characters refer to the month, the day, and the year, and how they are separated. If your values are already in one of R's standard date formats ("YYYY-MM-DD" or "YYYY/MM/DD") the `format =` argument is not necessary.  

To `format = `, provide a character string (in quotes) that represents the *current* date format using the special "strptime" abbreviations below. For example, if your character dates are currently in the format "DD/MM/YYYY", like "24/04/1968", then you would use `format = "%d/%m/%Y"` to convert the values into dates. **Putting the format in quotation marks is necessary. And don't forget any slashes or dashes!**  

```{r eval=F}
# Convert to class date
linelist <- linelist %>% 
  mutate(date_onset = as.Date(date_of_onset, format = "%d/%m/%Y"))
```

Most of the strptime abbreviations are listed below. You can see the complete list by running `?strptime`.  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds
%z = offset from GMT  
%Z = Time zone (character)  

<span style="color: darkgreen;">**_TIP:_** The `format =` argument is *not* telling R the format you want the dates to be, but rather how to identify the date parts as they are *before* you run the command.</span>  

<span style="color: darkgreen;">**_TIP:_** Be sure that in the `format =` argument you use the *date-part separator* (e.g. /, -, or space) that is present in your dates.</span>  

Once the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.



### **lubridate** {-}  

Converting character objects to dates can be made easier by using the **lubridate** package. This is a **tidyverse** package designed to make working with dates and times more simple and consistent than in **base** R. For these reasons, **lubridate** is often considered the gold-standard package for dates and time, and is recommended whenever working with them.

The **lubridate** package provides several different helper functions designed to convert character objects to dates in an intuitive, and more lenient way than specifying the format in `as.Date()`. These functions are specific to the rough date format, but allow for a variety of separators, and synonyms for dates (e.g. 01 vs Jan vs January) - they are named after abbreviations of date formats. 


```{r, eval = T}
# install/load lubridate 
pacman::p_load(lubridate)
```

The `ymd()` function flexibly converts date values supplied as **year, then month, then day**.  

```{r}
# read date in year-month-day format
ymd("2020-10-11")
ymd("20201011")
```

The `mdy()` function flexibly converts date values supplied as **month, then day, then year**.  

```{r}
# read date in month-day-year format
mdy("10/11/2020")
mdy("Oct 11 20")
```

The `dmy()` function flexibly converts date values supplied as **day, then month, then year**.  

```{r}
# read date in day-month-year format
dmy("11 10 2020")
dmy("11 October 2020")
```

<!-- The `as.character()` and `as.Date()` commands can optionally be combined as:   -->

<!-- ```{r eval=F} -->
<!-- linelist_cleaned$date_of_onset <- as.Date(as.character(linelist_cleaned$date_of_onset), format = "%d/%m/%Y") -->
<!-- ``` -->

If using [piping](#piping) and the tidyverse, the converting of a character column to dates with **lubridate** might look like this:  

```{r, eval=F}
linelist <- linelist %>%
  mutate(date_onset = lubridate::dmy(date_onset))
```

Once complete, you can run `class()` to verify the class of the column  

```{r, eval=F}
# Check the class of the column
class(linelist$date_onset)  
```

Once the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.

### Combine columns {-}  

You can use the **lubridate** functions `make_date()` and `make_datetime()` to combine multiple numeric columns into one date column. For example if you have numeric columns `day`, `month`, and `year` in the data frame `linelist`:  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(date = make_date(year = year, month = month, day = day))
```




<!-- ======================================================= -->
## Excel (numeric) dates

In the background, most software store dates as numbers. R stores dates from an origin of 1st January, 1970. Thus, if you run `as.numeric(as.Date("1970-01-01))` you will get `0`. 

Microsoft Excel stores dates with an origin of either December 30, 1899 (Windows) or January 1, 1904 (Mac), depending on your operating system. See this [Microsoft guidance](https://docs.microsoft.com/en-us/office/troubleshoot/excel/1900-and-1904-date-system) for more information.  

Excel dates often import into R as these numeric values instead of as characters. If the dataset you imported from Excel shows dates as numbers or characters like "41369"... use `as.Date()` (or **lubridate**'s `as_date()` function) to convert, but **instead of supplying a "format" as above, supply the Excel origin date** to the argument `origin = `.  

This will not work if the Excel date is stored in R as a character type, so be sure to ensure the date is a class Numeric or Double!

<span style="color: black;">**_NOTE:_** You should provide the origin date in R's default date format ("YYYY-MM-DD").</span>

```{r, eval = FALSE}
# An example of providing the Excel 'origin date' when converting Excel number dates
data_cleaned <- data %>% 
  mutate(date_onset = as.Date(as.numeric(date_onset), origin = "1899-12-30")) # convert to numeric, then convert to date
```



<!-- ======================================================= -->
## Messy dates  

The function `guess_dates()` from the **linelist** package attempts to read a "messy" date column containing dates in many different formats and convert the dates to a standard format. You can [read more online about `guess_dates()`](https://www.repidemicsconsortium.org/linelist/reference/guess_dates.html). If `guess_dates()` is not yet available on CRAN for R 4.0.2, try install via `pacman::p_load_gh("reconhub/linelist")`.

For example `guess_dates` would see a vector of the following character dates "03 Jan 2018", "07/03/1982", and "08/20/85" and convert them to class Date as: `2018-01-03`, `1982-03-07`, and `1985-08-20`.  

```{r, eval = T}
linelist::guess_dates(c("03 Jan 2018",
                        "07/03/1982",
                        "08/20/85"))
```

Some optional arguments for `guess_dates()` that you might include are:  

* `error_tolerance` - The proportion of entries which cannot be identified as dates to be tolerated (defaults to 0.1 or 10%)
* `last_date` - the last valid date (defaults to current date)  
* `first_date` - the first valid date. Defaults to fifty years before the last_date.


```{r eval = FALSE}
# An example using guess_dates on the column dater_onset
linelist <- linelist %>%                 # the dataset is called linelist
  mutate(
    date_onset = linelist::guess_dates(  # the guess_dates() from package "linelist"
      date_onset,
      error_tolerance = 0.1,
      first_date = "2016-01-01"
    )
```




<!-- ======================================================= -->
## Convert to `datetime` classes

As previously mentioned, R also supports a `datetime` class - a column that contains date **and** time information. As with the `Date` class, these often need to be converted from `character` objects to `datetime` objects. 

### Convert dates with times {-}  

A standard `datetime` object is formatted with the date first, which is followed by a time component - for example  _01 Jan 2020, 16:30_. As with dates, there are many ways this can be formatted, and there are numerous levels of precision (hours, minutes, seconds) that can be supplied.  

Luckily, **lubridate** helper functions also exist to help convert these strings to `datetime` objects. These functions are extensions of the date helper functions, with `_h` (only hours supplied), `_hm` (hours and minutes supplied), or `_hms` (hours, minutes, and seconds supplied) appended to the end (e.g. `dmy_hms()`). These can be used as shown:

Convert datetime with only hours to datetime object  

```{r}
ymd_h("2020-01-01 16hrs")
ymd_h("2020-01-01 4PM")
```

Convert datetime with hours and minutes to datetime object  

```{r}
dmy_hm("Jan 1st 2020 16:20")
```

Convert datetime with hours, minutes, and seconds to datetime object  

```{r}
mdy_hms("01 January 20, 16:20:40")
```

You can supply time zone but it is ignored. See section later in this page on time zones.  

```{r}
mdy_hms("01 January 20, 16:20:40 PST")

```

When working with a dataframe, time and date columns can be combined to create a datetime column using `str_glue()` from **stringr** package and an appropriate **lubridate** function. See the page on [Characters and strings] for details on **stringr**.  
In this example, the `linelist` data frame has a column in format "hours:minutes". To convert this to a datetime we follow a few steps:  

1) Create a "clean" time of admission column with missing values filled-in with the column median. We do this because **lubridate** won't operate on missing values. combine it with the column `date_hospitalisation`, and then use the function `ymd_hm()` to convert.  

```{r, eval = FALSE}
# packages
pacman::p_load(tidyverse, lubridate, stringr)

# time_admission is a column in hours:minutes
linelist <- linelist %>%
  
  # when time of admission is not given, assign the median admission time
  mutate(
    time_admission_clean = ifelse(
      is.na(time_admission),
      median(time_admission),
      time_admission
  ) %>%
  
    # use str_glue() to combine two columns to create a character column
    # and then use ymd_hm() to convert to datetime
  mutate(
    date_time_of_admission = str_glue("{date_hospitalisation} {time_admission_clean}") %>% 
      ymd_hm()
  )

```

### Convert times alone {-}  

If your data contain only a character time (hours and minutes), you can convert and manipulate them as times using `strptime()` from **base** R. For example, to get the difference between two of these times:  

```{r}
# raw character times
time1 <- "13:45" 
time2 <- "15:20"

# Times converted to a datetime class
time1_clean <- strptime(time1, format = "%H:%M")
time2_clean <- strptime(time2, format = "%H:%M")

# Difference is of class "difftime" by default, here converted to numeric hours 
as.numeric(time2_clean - time1_clean)   # difference in hours

```

Note however that without a date value provided, it assumes the date is today. To combine a string date and a string time together see how to use **stringr** in the section just above. Read more about `strptime()` [here](https://rdrr.io/r/base/strptime.html).  


### Extract time {-}  

You can extract elements of a time with `hour()`, `minute()`, or `second()` from **lubridate**.  

Here is an example of extracting the hour, and then using to classify times. We begin with the column `time_admission`, which is class Character in format "HH:MM". First, the `strptime()` is used as described above to convert the characters to datetime class. Then, the hour is extracted with `hour()`, returning a number from 0-24. Finally, a column `time_period` is created using logic with `case_when()` to classify rows into Morning/Afternoon/Evening/Night based on their hour of admission.  

```{r}
linelist <- linelist %>%
  mutate(hour_admit = hour(strptime(time_admission, format = "%H:%M"))) %>%
  mutate(time_period = case_when(
    hour_admit > 06 & hour_admit < 12 ~ "Morning",
    hour_admit >= 12 & hour_admit < 17 ~ "Afternoon",
    hour_admit >= 17 & hour_admit < 21 ~ "Evening",
    hour_admit >=21 | hour_admit <= 6 ~ "Night"))
```

To learn more about `case_when()` see the page on [Cleaning data and core functions].  

<!-- ======================================================= -->
## Working with dates   

`lubridate` can also be used for a variety of other functions, such as **extracting aspects of a date/datetime**, **performing date arithmetic**, or **calculating date intervals**

Here we define a date to use for the examples:  

```{r, eval = T}
# create object of class Date
example_date <- ymd("2020-03-01")
```

You can extract common aspects such as month, day, weekday:  

```{r}
month(example_date)  # month number
day(example_date)    # day (number) of the month
wday(example_date)   # day number of the week (1-7)
```

You can also extract time components from a `datetime` object or column. This can be useful if you want to view the distribution of admission times.  

```{r, eval=F}
example_datetime <- ymd_hm("2020-03-01 14:45")

hour(example_datetime)     # extract hour
minute(example_datetime)   # extract minute
second(example_datetime)   # extract second
```

There are several options to retrieve weeks. See the section on Epidemiological weeks below.  

Note that if you are seeking to *display* a date a certain way (e.g. "Jan 2020" or "Thursday 20 March" or "Week 20, 1977") you can do this more flexibly as described in the section on Date display.  


### Date math {-}  

You can add certain numbers of days or weeks using their respective function from **lubridate**.  

```{r}
# add 3 days to this date
example_date + days(3)
  
# add 7 weeks and subtract two days from this date
example_date + weeks(7) - days(2)
```

### Date intervals {-}  

The difference between dates can be calculated by:  

1. Ensure both dates are of class date  
2. Use subtraction to return the "difftime" difference between the two dates  
3. If necessary, convert the result to numeric class to perform subsequent mathematical calculations  

Below the interval between two dates is calculated and displayedYou can find intervals by using the subtraction "minus" symbol on values that are class Date. Note, however that the class of the returned value is "difftime" as displayed below, not numeric. 

```{r}
# find the interval between this date and Feb 20 2020 
output <- example_date - ymd("2020-02-20")
output    # print
class(output)
```

To do subsequent operations on a "difftime", convert it to numeric with `as.numeric()`. 

This can all be brought together to work with data - for example:

```{r, eval = F}
pacman::p_load(lubridate, tidyverse)   # load packages

linelist <- linelist %>%
  
  # convert date of onset from character to date objects by specifying dmy format
  mutate(date_onset = dmy(date_onset),
         date_hospitalisation = dmy(date_hospitalisation)) %>%
  
  # filter out all cases without onset in march
  filter(month(date_onset) == 3) %>%
    
  # find the difference in days between onset and hospitalisation
  mutate(onset_to_hosp_days = date_hospitalisation - date_of_onset)
```



In a dataframe format (i.e. when working with a linelist), if either of the above dates is missing, the operation will fail for that row. This will result in an `NA` instead of a numeric value. When using this column for calculations, be sure to set the `na.rm` option to `TRUE`. For example:

```{r, eval = FALSE}

# add a new column
# calculating the number of days between symptom onset and patient outcome
linelist_delay <- linelist_cleaned %>%
  mutate(
    days_onset_to_outcome = as.double(date_of_outcome - date_of_onset)
  )

# calculate the median number of days to outcome for all cases where data are available
med_days_outcome <- median(linelist_delay$dats_onset_to_outcome, na.rm = T)

# often this operation might be done only on a subset of data cases, e.g. those who died
# this is easy to look at and will be explained later in the handbook

```


<!-- ======================================================= -->
## Date display  

Once dates are the correct class, you often want them to display differently, for example to display as "Monday 05 January" instead of "2018-01-05". You may also want to adjust the display in order to then group rows by the date elements displayed - for example to group by month-year.  

Adjust date display with the **base** R function `format()`. This function accepts a character string (in quotes) specifying the *desired* output format in the "%" strptime abbreviation (the same syntax as used in `as.Date()`). Below are most of the common abbreviations.  

Note: using `format()` will convert the values to class Character, so this is generally used towards the end of an analysis or for display purposes only! You can see the complete list by running `?strptime`.  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)  
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds  
%z = offset from GMT  
%Z = Time zone (character)

An example of formatting today's date:  

```{r}
# today's date, with formatting
format(Sys.Date(), format = "%d %B %Y")

# easy way to get full date and time (no formatting)
date()

# formatted date, time, and time zone (using paste0() function)
paste0(
  format(Sys.Date(), format = "%A, %B %d %Y, %z  %Z, "), 
  format(Sys.time(), format = "%H:%M:%S")
)

# Using format to display weeks
format(Sys.Date(), "%Y Week %W")
```


### Month-Year {-}  

To convert a Date column to Month-year format, we suggest you use the function `as.yearmon()` from the **zoo** package. This converts the date to class "yearmon" and retains the proper ordering. In contract, using format(column, "%Y %B) will convert to class Character and will order the values alphabetically. 

Below, a new column `yearmon` is created from the column `date_onset`, using `as.yearmon()` function. The default ordering of the resulting values are given in the table.  

```{r}
# create new column 
test_zoo <- linelist %>% 
     mutate(yearmonth = zoo::as.yearmon(date_onset))

# print table
table(test_zoo$yearmon)
```

In contrast, you can see how only using `format()` does achieve the desired display, but not the correct ordering.  

```{r}
# create new column
test_format <- linelist %>% 
     mutate(yearmonth = format(date_onset, "%b %Y"))

# print table
table(test_format$yearmon)
```

Note: if you are working within a `ggplot()` and want to adjust how dates are *displayed* only, it may be sufficient to provide a strptime format to the `date_labels = ` argument in `scale_x_date()` - you can use "%b %Y" or "%Y %b". 

**zoo** also offers the function `as.yearqtr()`, and you can use `scale_x_yearmon()` when using `ggplot()`.  



<!-- ======================================================= -->
## Epidemiological weeks  

### **lubridate** {-}  

See the page on [Grouping data] for examples of grouping by date. Below we briefly describe grouping data by weeks.  

We generally recommend using the `floor_date()` function from **lubridate**, with the argument `unit = "week"`. This rounds the date down to the "start" of the week, as defined by the argument `week_start = `. The default week start is 1 (for Mondays) but you can specify any day of the week as the start (e.g. 7 for Sundays). `floor_date()` can also be used to round down to other time units by setting `unit = ` to "second", "minute", "hour", "day", "month", or "year").  

The returned value is the start date of the week, in Date class. Date class is useful when plotting the data, as it will be easily recognized and ordered correctly by `ggplot()`.

See the section on Date display for tips on how to adjust the *display* of dates in a plot. For example, if plotting an epicurve you format the date display by providing the desired strptime "%" nomenclature. For example, use "%Y-%W" or "%Y-%U" to return the year and week number (given Monday or Sunday week start, respectively).  

#### Weekly counts {-}  

See the page on [Grouping data] for a thorough explanation of grouping data with `count()`, `group_by()`, and `summarise()`. A brief example is below.  

1) Create a new 'week' column with `mutate()`, using `floor_date()` with `unit = "week"  
2) Get counts of rows (cases) per week with `count()`; filter out any cases with missing date  
3) Follow-up with `complete()` from **tidyr** to ensure that *all* weeks appear in the data - even those with no rows/cases.  

```{r}
# Make aggregated dataset of weekly case counts
weekly_counts <- linelist %>% 
  filter(!is.na(date_onset)) %>%    # remove cases missing onset date
  mutate(week = floor_date(         # make new column, week of onset
    date_onset,
    unit = "week")) %>%            
  count(week) %>%                   # group data by week and count rows per group
  tidyr::complete(week = seq.Date(  # ensure all weeks are present, even those with no cases reported
    from = min(week),               
    to = max(week),
    by = "week"))
```

Here are the first rows of the resulting data frame:  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_counts, 20), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Epiweek alternatives {-}  

Note that **lubridate** also has functions `week()`, `epiweek()`, and `isoweek()`, each of which has slightly different start dates and other nuances. Generally speaking though, `floor_date()` should be all that you need. Read the details for these functions by entering `?week` into the console or reading the documentation [here](https://www.rdocumentation.org/packages/lubridate/versions/1.7.4/topics/week). 


You might consider using the package **aweek** to set epidemiological weeks. You can read more about it [on the RECON website](https://www.repidemicsconsortium.org/aweek/). It has the functions `date2week()` and `week2date()` in which you can set the week start day with `week_start = "Monday"`. This package is easiest if you want "week"-style outputs (e.g. "2020-W12"). Another advantage of **aweek** is that when `date2week()` is applied to a date column, the returned column (week format) is automatically of class Factor and includes levels for all weeks in the time span (this avoids the extra step of `complete()` described above). However, **aweek** does not have the functionality to round dates to other time units such as months, years, etc.  
Another alternative for time series which also works well to show a a "week" format ("2020 W12") is `yearweek()` from the package **tsibble**, as demonstrated in the page on [Time series and outbreak detection].  


<!-- ======================================================= -->
## Converting dates/time zones

When data is present in different time time zones, it can often be important to standardise this data in a unified time zone. This can present a further challenge, as the time zone component of data must be coded manually in most cases.

In R, each *datetime* object has a timezone component. By default, all datetime objects will carry the local time zone for the computer being used - this is generally specific to a *location* rather than a named timezone, as time zones will often change in locations due to daylight savings time. It is not possible to accurately compensate for time zones without a time component of a date, as the event a date column represents cannot be attributed to a specific time, and therefore time shifts measured in hours cannot be reasonably accounted for.

To deal with time zones, there are a number of helper functions in lubridate that can be used to change the time zone of a datetime object from the local time zone to a different time zone. Time zones are set by attributing a valid tz database time zone to the datetime object. A list of these can be found here - if the location you are using data from is not on this list, nearby large cities in the time zone are available and serve the same purpose. 

https://en.wikipedia.org/wiki/List_of_tz_database_time_zones


```{r}
# assign the current time to a column
time_now <- Sys.time()
time_now

# use with_tz() to assign a new timezone to the column, while CHANGING the clock time
time_london_real <- with_tz(time_now, "Europe/London")

# use force_tz() to assign a new timezone to the column, while KEEPING the clock time
time_london_local <- force_tz(time_now, "Europe/London")


# note that as long as the computer that was used to run this code is NOT set to London time, there will be a difference in the times (the number of hours difference from the computers time zone to london)

time_london_real - time_london_local

```

This may seem largely abstract, and is often not needed if the user isn't working across time zones. One simple example of its implementation is:

```{r, eval = FALSE}
# TODO add when time column is here
# set the time column to time zone for ebola outbreak 

# "Africa/Lubumbashi" is the time zone for eastern DRC/Kivu Nord


```




<!-- ======================================================= -->
## Lagging and leading calculations  

`lead()` and `lag()` are functions from the **dplyr** package which help find previous (lagged) or subsequent (leading) values in a vector - typically a numeric or date vector. This is useful when doing calculations of change/difference between time units.  


```{r, echo=F}
counts <- import(here("data", "district_count_data.xlsx")) %>% 
  filter(District == "Nibari") %>% 
  mutate(Date = as.Date(Date),
         week_start = lubridate::floor_date(Date, "week")) %>%
  group_by(week_start) %>% 
  summarize(cases_wk = sum(Cases, na.rm=T)) %>% 
  complete(week_start = seq.Date(min(week_start), max(week_start), by = "week"))
```

Let's say you want to calculate the difference in cases between a current week and the previous one. The data are initially provided in weekly counts as shown below. To learn how to aggregate counts from daily to weekly see the page on aggregating (LINK).  

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

**When using `lag()` or `lead()` the order of rows in the dataframe is very important! - pay attention to whether your dates/numbers are ascending or descending**  

First, create a new column containing the value of the previous (lagged) week.  

* Control the number of units back/forward with `n = ` (must be a non-negative integer)  
* Use `default = ` to define the value placed in non-existing rows (e.g. the first row for which there is no lagged value). By default this is `NA`.  
* Use `order_by = TRUE` if your reference column is not ordered  

```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1))
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
Next, create a new column which is the difference between the two cases columns:  

```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1),
         case_diff = cases_wk - cases_prev_wk)
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

You can read more about `lead()` and `lag()` in the documentation [here](https://dplyr.tidyverse.org/reference/lead-lag.html) or by entering `?lag` in your console.  

<!-- ======================================================= -->
## Dates miscellaneous  

* `Sys.Date( )` from **base** R returns the current date of your computer  
* `Sys.Time()` from **base** R returns the current time of your computer
* `date()` from **lubridate** returns the current date and time.  

<!-- ======================================================= -->
## Resources  

**lubridate** [tidyverse page](https://lubridate.tidyverse.org/)  
**lubridate** RStudio [cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)  
R for Data Science page on [dates and times](https://r4ds.had.co.nz/dates-and-times.html)  
[Online tutorial](https://www.statmethods.net/input/dates.html)
[Date formats](https://www.r-bloggers.com/2013/08/date-formats-in-r/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/dates.Rmd-->

# Factors {}

 
In R, *factors* allow for ordered categorical data. A column can be converted from class numeric, categorical, or even logical to class *factor*. In this case, the values are stored as *ordered integer **levels***, and can display with assigned *labels*.  

In a column of class *factor*:  

* the possible values are restricted - values not already defined as levels are rejected  
* values are ordered, which impacts how they display in tables and plots  

This page demonstrates use of functions from the package **forcats** (a short name for "For categorical variables") and some **base** R functions. We also touch upon the use of **lubridate** and **aweek** for special cases related to epiweeks.  

Factors are useful in statistical modeling, which allows integer values such as 1/0 to be evaluated categorically and not continuously.  



<!-- ======================================================= -->
## Preparation  

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # import/export
  here,          # filepaths
  lubridate,     # working with dates
  forcats,       # factors
  aweek,         # create epiweeks with automatic factor levels
  tidyverse      # data mgmt and viz
  )
```



### Import data {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```


```{r, eval=F}
# fake import the linelist
linelist <- import("linelist_cleaned.xlsx")
```


### Example: new categorical column  

For demonstration in this page we will use a common scenario - the creation of a categorical variable.  

We use the existing column `days_onset_hosp` (days from symptom onset to hospital admission) and classify each row into one of several categorical groupings.  

We can use the **dplyr** function `case_when()` to apply logical criteria on each row, resulting in values for the new column `delay`  

```{r}
linelist <- linelist %>% 
  mutate(delay = case_when(
    days_onset_hosp < 2                        ~ "<2 days",
    days_onset_hosp >= 2 & days_onset_hosp < 5 ~ "2-5 days",
    days_onset_hosp >= 5                       ~ ">5 days",
    is.na(days_onset_hosp)                     ~ NA_character_,
    TRUE                                       ~ "Check me"))  
```


## Non-factor categorical  

The column `delay` (created in the Preparation section above) is a categorical column of class Character - *not* yet a Factor. Thus, in a frequency table, we see that the values appear in a default alphabetical order - an order that does not make much intuitive sense:  

```{r}
table(linelist$delay, useNA = "always")
```

Likewise, if we make a bar plot, the values also appear in this order onthe x-axis. This order does not make sense.  

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay))
```



## Convert to factor  

To initially convert a character or numeric column to class Factor, we suggest using the **base** R function `factor()`. Below the data frame `linelist` is modified such that the column `delay` is converted to a factor. The default order of the values will be alpha-numeric.  

```{r}
linelist <- linelist %>%
  mutate(delay = factor(delay))
```

Unless specified, the levels will still be in alphabetic (or numeric) order. Use the **base** R function `levels()` to see how the levels of `time_period` are ordered. Note that `NA` is not a factor level.  

```{r}
levels(linelist$delay)
```

You can specify the levels and their order *in your initial conversion command* to the `levels = ` argument. 

```{r}
linelist <- linelist %>%
  mutate(delay = factor(delay, levels = c("<2 days", "2-5 days", ">5 days")))
```

```{r}
levels(linelist$delay)
```




## Adjust level order  

The package **forcats** offers several useful functions to easily *adjust* the order of a factor's levels: 

* Use `fct_relevel()` to *manually* adjust the order  
* Use `fct_infreq()` to reorder by frequency (highest to lowest)  
* Use `fct_inorder()` to reorder by order of appearance in the data  
* Use `fct_reorder()` to reorder by another column (e.g. order time_period levels by their row's median delay to admission)  
* Use `fct_rev()` to reverse the existing order  
* Use `fct_reorder2()` to reorder by the final values when plotted with two other columns  


These functions can be applied outside of a plot to re-define the column, or within a plot to affect just one specific plot.  
 



### Examples {-}  


**`fct_relevel()`**  

This function is used to manually order the factor levels. Within the parentheses, first provide the factor column, then provide the levels in the desired order (as a character vector within `c()`). Here is an example of redefining the column `delay` (which is already class Factor) and specifying the desired order of levels.  

```{r}
# re-define level order
linelist <- linelist %>% 
  mutate(delay = fct_relevel(delay, c("<2 days", "2-5 days", ">5 days")))
```

If you only want to move one level, you can specify it to `fct_relevel()` alone and give a number to the `after = ` argument to indicate where in the order it should be. For example: 

```{r}
# re-define level order
linelist <- linelist %>% 
  mutate(delay = fct_relevel(delay, "<2 days", after = 1))

levels(linelist$delay)
```


The above will define the level order in the data frame. Alternatively, you can adjust the levels from *within* a ggplot, and the re-ordering of the levels will only apply within the plot. Below, as the the `delay` column is mapped to the x-axis of the plot it is wrapped within `fct_relevel()`.  


```{r, warning=F, message=F, out.width = c('50%', '50%'), fig.show='hold'}
# Incorrect order - no adjustment within ggplot
ggplot(data = linelist)+
    geom_bar(mapping = aes(x = delay))

# Factor level order adjusted within ggplot
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = fct_relevel(delay, c("<2 days", "2-5 days", ">5 days"))))
```

Note how the default x-axis label is now quite complicated - you can overwrite this with the `labs()` in ggplot.  




**`fct_infreq()`**  

To order by frequency that the value appears in the data, use `fct_infreq()`. Any missing values (`NA`) will automatically be included at the end. You can reverse this order by further modifying with `fct_rev()`.  

This function can be used within a `ggplot()`, as shown below.  

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# ordered by frequency
ggplot(data = linelist, aes(x = fct_infreq(delay)))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)")

# reversed frequency
ggplot(data = linelist, aes(x = fct_rev(fct_infreq(delay))))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)")
```

Here is an example within a data frame:  

```{r, eval = F}
linelist %>% 
  mutate(delay = fct_infreq(delay),
         delay = fct_rev(delay))
```


**`fct_reorder()`**  

Use this function to order the levels by another column. For example, to order boxplots showing delay *by the median CT value of each delay group*.  

In the examples below, the x-axis if delay group, and the y = axis is CT value. The boxplots are also colored by delay group.  

In the first example, the baseline order of the levels applies (as set earlier in this page) - they increase incrementally updward by delay.  
In the second example, the x-axis column has been wrapped in `fct_reorder()`, with the column `ct_blood` as the second argument. The default is order `delay` by the *median* `ct_value`. An alternative function can be supplied, e.g. "mean", or "max".  

Note there are no explicit grouping steps required prior to the `ggplot()` - the grouping and calculations are all done internally.  

```{r, warning=F, message=F}
# boxplots ordered by original factor levels
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = delay,
        y = ct_blood, 
        fill = delay))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by increasing delay (original factor levels)")+
  theme_classic()+
  theme(legend.position = "none")


# boxplots ordered by median CT value
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = fct_reorder(delay, ct_blood, "median"),
        y = ct_blood,
        fill = delay))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by median CT value in group")+
  theme_classic()+
  theme(legend.position = "none")
```



**`fct_reorder2()`**  

Use this function to order the *legend colors* by the vertical order of groups at the "end" of the plot. For example, if you have lines showing case counts by hospital over time, you can apply `fct_reorder2()` to the `color = ` argument within `aes()`, such that the vertical order of hospitals appearing in the legend aligns with the order of lines at the terminal end of the plot. Read more in the [function documentation](https://forcats.tidyverse.org/reference/fct_reorder.html).  

```{r, warning=F, message=F}
linelist %>%         # begin with the linelist            
  count(             # summarise so n = counts of rows by epiweek and by hospital
    epiweek = lubridate::floor_date(date_onset, "week"),  # create and group by epiweeks
    hospital         # also group by hospital
    ) %>% 
  
  ggplot()+           # start plot
  geom_line(          # make lines
    aes(x = epiweek,  # x-axis epiweek
        y = n,        # height in number of rows
        color = fct_reorder2(hospital, epiweek, n)))+ # grouped by hospital and colors ordered by n value at end of plot
  labs(color = "Hospital")  # change legend title
```







**`fct_lump()`**

To "lump" together many low-frequency levels into an "Other" group, you can use this function. Do one of the following:  

* Set `n = ` argument as the number of groups you want to keep. All other values will combine into "Other".  
* set `prop = ` argument as the proportion above which you want to keep. All other values will combine into "Other".  

You can also change the label for "Other" by using `other_level = `. Below, all but the two most-frequent hospitals are combined into "Other hospitals".  

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(aes(x = fct_lump(hospital,    # column for x-axis
                            n = 2,       # keep two most-frequent levels
                            other_level = "Other hospitals"))) # label for "Other" group
```

You can also use `fct_other()` to manually assign factor levels to an "Other" group. Below, all `hospital` values aside from "Port Hospital" and "Central Hospital" are combined into "Other".  

You can use the arguments `keep = `, or `drop = `, and can change the label of "Other" with `other_label = `.  

```{r}
linelist %>% 
  mutate(hospital = fct_other(hospital, keep = c("Port Hospital", "Central Hospital"))) %>% 
  select(hospital) %>% 
  table()

```


## Missing values  

If you have `NA` values in your column, you can easily convert them to a named value such as "Missing" with `fct_explicit_na()`, as performed below temporarily on the column `delay`:  

```{r}
linelist %>% 
  mutate(delay = fct_explicit_na(delay, na_level = "Missing")) %>% 
  select(delay) %>% 
  table(useNA = "always")
```





## Edit labels  

Adjust the factor labels with `fct_recode()`. remember that these do not change the underlying values, only their labels.  
Below, the labels of the factor column `delay` (grouped days from onset to admission) are edited:  

The old labels:  

```{r}
table(linelist$delay, useNA = "always")
```

Now the labels are changed, using syntax `fct_recode(column, "new" = "old","new" = "old", "new" = "old")`. Remember that `NA` is not a formal level unless changed (e.g. with `fct_explicit_na()` as shown above).  

```{r}
linelist <- linelist %>% 
  mutate(delay = fct_recode(delay,
                            "Less than 2 days" = "<2 days",
                            "2 to 5 days"      = "2-5 days",
                            "More than 5 days" = ">5 days"))

table(linelist$delay)
```


## Add/drop levels  

If you have a factor and want to add levels (regardless of whether there are any rows with those values), use `fct_expand()`.  

See how if we classify "hospital" as a factor, and then try to change the values, an error is returned:  

```{r}
linelist <- linelist %>% 
  mutate(hospital = factor(hospital))

levels(linelist$hospital)
```

Now we can add the level "University Hospital":  

```{r}
linelist <- linelist %>% 
  mutate(hospital = fct_expand(hospital, "University Hospital"))

levels(linelist$hospital)
```


### Epiweeks {-}  

Please see the extensive discussion of how to create epidemiological weeks in the [Grouping data] page.  
Please also see the [Working with dates] page for tips on how to create and format epi weeks.  


#### `floor_date()` {-}  

If you create epiweeks with **lubridate**'s `floor_date()`, the values returned be of class Date with format YYYY-MM-DD. If you use them in a plot the dates will naturally order correctly, and you do not need to worry about class Factor. For example, in the `ggplot()` histogram of onset dates below.  

You can adjust the *display* of the dates on an axis with `scale_x_date()`. See the page on [Epidemic curves] for more information. You can specify a "strptime" display format to the `date_labels = ` argument. These formats use "%" placeholders and are covered in the [Working with dates] page. Use "%Y" to represent a 4-digit year, and either "%W" or "%U" to represent the week number (Monday or Sunday weeks respectively).  

```{r, warning=F, message=F}
linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week")) %>%  # create week column
  ggplot()+                                                  # begin ggplot
  geom_histogram(mapping = aes(x = epiweek_date))+           # histogram of date of onset
  scale_x_date(date_labels = "%Y-W%W")                       # adjust disply of dates to be YYYY-WWw
```

However, if your purpose in factoring is *not* to plot, you may convert this epiweek column (YYYY-MM-DD) to a different display format (YYYY-WWw) *within the data frame itself*, and convert it to class Factor. Use `format()` from **base** R to convert the display, then convert to class Factor with `factor()`.  

```{r}
linelist <- linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week"),       # create epiweeks (YYYY-MM-DD)
         epiweek_formatted = format(epiweek_date, "%Y-W%W"),  # Convert to display (YYYY-WWw)
         epiweek_formatted = factor(epiweek_formatted))       # Convert to factor

# Display levels
levels(linelist$epiweek_formatted)
```

<span style="color: red;">**_DANGER:_** If you use "Www-YYYY" ("%W-%Y") display format instead, the default alpha-numeric level ordering will be incorrect (e.g. 01-2015 will be before 35-2014).</span>  

#### **aweek** {-}  

An alternative method if you want to convert dates to epiweeks *within a data frame* is to use the **aweek** package function `date2week()`. You can set the `week_start = ` and if you set `factor = TRUE` then the output column is an ordered factor. As a bonus, the factor includes levels for *all* weeks in the span - even if there are no cases that week. This ensure that if you create a table or bar plot that all weeks will appear.  

```{r, eval=F}
df <- linelist %>% 
  mutate(epiweek = date2week(date_onset, week_start = "Monday", factor = TRUE))

levels(df$epiweek)
```

See the [Working with dates] page for more information about **aweek**. It also offers the reverse function `week2date()`.  



<!-- ======================================================= -->
## Resources {} 

R for Data Science page on [factors](https://r4ds.had.co.nz/factors.html).
[aweek vignette](https://cran.r-project.org/web/packages/aweek/vignettes/introduction.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/factors.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Pivoting data {}

When manipulating data, *pivoting* can be understood to refer to one of two processes:  

1. the creation of *pivot tables*, which are tables "... of statistics that summarize the data of a more extensive table (such as from a database, spreadsheet, or business intelligence program). This summary might include sums, averages, or other statistics, which the pivot table groups together in a meaningful way... They arrange and rearrange (or "pivot") statistics in order to draw attention to useful information. This leads to finding figures and facts quickly making them integral to data analysis." see [wiki](https://en.wikipedia.org/wiki/Pivot_table#).  
2. The conversion of a table from **long** to **wide** format, or vice versa. 

**In this page, we will focus on the latter definition.** The former is a crucial step in data analysis, and is covered elsewhere in the [Grouping data] and [Descriptive tables] pages. 

## Preparation  

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse)    # data management + ggplot2 graphics
```


### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
## Wide-to-long {}

Transforming a dataset from wide to long ([image source](https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png))

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivot_longer.png"))
```


<!-- ======================================================= -->
### Data {-}

Data are often entered and stored in a format that might be useful for presentation, but not for analysis. Let us take the `count_data` dataset as an example, which is stored in a "wide" format, which means that each column is a variable and each row an observation. This is useful for presenting the information in a table or for entering data (e.g. in Excel) from case report forms. However, these typically needs to be transformed to "long" format in order to analyse and visualise.


```{r, echo=F}
count_data <- rio::import(here::here("data", "facility_count_data.rds"))
```

```{r, eval=F}
count_data <- import("facility_count_data.rds")
```


```{r, echo=F}
DT::datatable(count_data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T) )
```

Each observation in this dataset refers to the malaria counts at one of 65 facilities on a given date, ranging from `r count_data$data_date %>% min()` to `r count_data$data_date %>% max()`. These facilties are located in one `Province` (North) and four `District`s (Spring, Bolo, Dingo, and Barnard). The dataset provides the overall counts of malaria, as well as age-specific counts in each of three age groups - <4 years, 5-14 years, and 15 years and older.

Visualising the overall malaria counts over time poses no difficulty with the data in it's current format:

```{r, warning=F, message=F}
ggplot(count_data) +
  geom_col(aes(x = data_date, y = malaria_tot))
```

However, what if we wanted to display the relative contributions of each age group to this total count? In this case, we need to ensure that the variable of interest (age group), appears in the dataset in a single column that can be passed to `{ggplot2}`'s "aesthetics" (`aes()`) function.

---
**Consider also using the common problem whereby data are stored with dates as the columns, as in the example dataset `tidyr::table4a`**

```{r}
tidyr::table4a
```


<!-- ======================================================= -->
### `pivot_longer()` {-}

First, let's begin by loading our packages and converting `count_data` to a tibble for easy printing:

```{r, warning=F, message=F}
pacman::p_load(tidyverse)

# Convert count_data to `tibble` for better printing
count_data <- 
  count_data %>% 
  as_tibble() 

count_data
```

Next, we want to use `{tidyr}`'s `pivot_longer()` function to convert the wide dataset to a long format, converting the four columns with data on malaria counts to two new columns: one which captures the variable *name* and one which captures the *values* from the cells. Since these four variables all begin with the prefix `malaria_`, we can make use of the handy function `starts_with()`. 

```{r}
df_long <- 
  count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_")
  )

df_long
```

However, we could also have specified the columns by position: 

```{r}
count_data %>% 
  pivot_longer(
    cols = 6:9
  )
```

or by named range:

```{r}
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_tot
  )
```

Notice that the newly created dataframe (`df_long`) has more rows (12,152 vs 3,038); it has become *longer*. In fact, it is precisely four times as long, because each row in the original dataset now represents four rows in df_long, one for each of the malaria count observations (<4y, 5-14y, 15y+, and total).

In addition to becoming longer, the new dataset has fewer columns (8 vs 10), as the data previously stored in four columns (those beginning with the prefix `malaria_`) is now stored in two. These two columns are given the default names of `name` and `value`, but we can override these defaults to provide more meaningful names, which can help remember what is stored within, using the `names_to` and `values_to` arguments. Let's use the names `age_group` and `count`:

```{r}
df_long <- 
  count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_"),
    names_to = "age_group",
    values_to = "counts"
  )

df_long
```

We can now pass this new dataset to `{ggplot2}` to display the malaria counts by age group:

```{r, warning=F, message=F}
ggplot(df_long) +
  geom_col(
    aes(x = data_date, y = counts, fill = age_group)
  )
```

Have a look at the plot - what is wrong here? We have encountered a common problem - we have also included the total counts from the `malaria_tot` column, so the magnitude of each bar in the plot is twice as high as it should be. 

We can handle this in a number of ways. We could simply filter it from the dataset we pass to `{ggplot2}`:

```{r, warning=F, message=F}
df_long %>% 
  filter(age_group != "malaria_tot") %>% 
  ggplot() +
  geom_col(
    aes(x = data_date, y = counts, fill = age_group)
  )
```

Alternatively, we could have excluded this variable when we ran `pivot_longer`, thereby maintaining it in the dataset as a separate variable:

```{r, warning=F, message=F}
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_rdt_15,
    names_to = "age_group",
    values_to = "counts"
  ) %>% 
  ggplot() +
  geom_col(
    aes(x = data_date, y = counts, fill = age_group)
  )
```



<!-- ======================================================= -->
## Long-to-wide {}

Transforming a dataset from long to wide ([image source](https://d33wubrfki0l68.cloudfront.net/8350f0dda414629b9d6c354f87acf5c5f722be43/bcb84/images/tidy-8.png))

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivot_wider.png"))
```



In some instances, we may wish to convert a dataset to a wider format. For this, we can use the `pivot_wider()` function.

A typical use case is when we want to transform the results of an analysis into a format which is more digestible for the reader. Typically, we are transforming a dataset in which the observations are spread over multiple rows to one in which each observation occupies a single row.

This introduces the useful topic of "tidy data", in which each variable has it's own column, each observation has it's own row, and each value has it's own cell. More about this topic can be found here https://r4ds.had.co.nz/tidy-data.html. 

### Data {-}

Let us use the `linelist` dataset. Suppose that we want to know the counts of individuals in the different age groups, by sex:

```{r}
linelist <- 
  linelist %>% 
  as_tibble()
  
df_wide <- 
  linelist %>% 
  count(age_cat, gender)
```

This gives us a long dataset that is great for visualisation, but not ideal for presentation in a table:

```{r}
ggplot(df_wide) +
  geom_col(aes(x = age_cat, y = n, fill = gender))
```

### Pivot wider {-}  

Therefore, we can use `pivot_wider()` to put this into a better format for inclusion as tables in our reports. The argument `names_from` specifies the column *from* which to generate the new column *names*, while the argument `values_from` specifies the column *from* which to take the *values* to populate the cells:

```{r}
table_wide <- 
  df_wide %>% 
  pivot_wider(
    names_from = gender,
    values_from = n
  )

table_wide
```

This table is much nicer for inclusion in our reports:

```{r}
table_wide %>% 
  janitor::adorn_totals(c("row", "col")) %>% # adds a total row and column
  knitr::kable() %>% 
  kableExtra::row_spec(row = 9, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```


<!-- ======================================================= -->
## Fill {}

Filling in missing data

<!-- ======================================================= -->
### Data {-}

In some situations after a `pivot`, and more commonly after a `bind`, we are left with gaps in some cells that we would like to fill. For example, take two datasets, each with observations for the measurement number, the name of the facility, and the case count at that time. However, the second dataset also has a variable `Year`. When we perform a `bind_rows()` to join the two datasets together, the `Year` variable is filled with `NA` for those rows where there was no prior information (i.e. the first dataset):


```{r}
df1 <- 
  tibble::tribble(
       ~Measurement, ~Facility, ~Cases,
                  1,  "Hosp 1",     66,
                  2,  "Hosp 1",     26,
                  3,  "Hosp 1",      8,
                  1,  "Hosp 2",     71,
                  2,  "Hosp 2",     62,
                  3,  "Hosp 2",     70,
                  1,  "Hosp 3",     47,
                  2,  "Hosp 3",     70,
                  3,  "Hosp 3",     38,
       )

df1 

df2 <- 
  tibble::tribble(
    ~Year, ~Measurement, ~Facility, ~Cases,
     2000,            1,  "Hosp 4",     82,
     2001,            2,  "Hosp 4",     87,
     2002,            3,  "Hosp 4",     46
  )

df2

df_combined <- 
  bind_rows(df1, df2) %>% 
  arrange(Measurement, Facility)

df_combined

```

<!-- ======================================================= -->
### `fill()` {-}

In this case, `Year` is a useful variable to include, particularly if we want to explore trends over time. Therefore, we use `fill()` to *fill* in those empty cells, by specifying the column to fill and the direction (in this case **up**):

```{r}
df_combined %>% 
  fill(Year, .direction = "up")
```

We can rearrange the data so that we would need to fill in a downward direction:

```{r}
df_combined <- 
  df_combined %>% 
  arrange(Measurement, desc(Facility))

df_combined

df_combined <- 
  df_combined %>% 
  fill(Year, .direction = "down")

df_combined
```

This dataset is now useful for plotting:

```{r}
ggplot(df_combined) +
  aes(Year, Cases, fill = Facility) +
  geom_col()
```

But less useful for presenting in a table, so let's practice converting this long, untidy dataframe into a wider, tidy dataframe:

```{r}
df_combined %>% 
  pivot_wider(
    id_cols = c(Facility, Year, Cases),
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  arrange(Facility) %>% 
  janitor::adorn_totals(c("row", "col")) %>% 
  knitr::kable() %>% 
  kableExtra::row_spec(row = 5, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

N.B. In this case, we had to specify to only include the three variables `Facility`, `Year`, and `Cases` as the additional variable `Measurement` would interfere with the creation of the table:

```{r}
df_combined %>% 
  pivot_wider(
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  knitr::kable()
```

## Resources  

Here is a helpful [tutorial](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/pivoting.Rmd-->


# Grouping data { }  
     
This page reviews how to group and aggregate data for descriptive analysis. It makes use of tidyverse packages for common and easy-to-use functions. 




<!-- ======================================================= -->
## Overview {  }

Grouping data is a core component of data management and analysis. Grouped data can be plotted, or statistically summarised by group. Functions from the **dplyr** package (part of the **tidyverse**) make grouping and subsequent operations quite easy.  

This page will address the following topics:  

* Grouping data with the `group_by()` function  
* Un-group data  
* `summarise()` grouped data with statistics  
* The difference between `count()` and `tally()`  
* `arrange()` applied to grouped data  
* `filter()` applied to grouped data  
* `mutate()` applied to grouped data  
* `select()` applied to grouped data  
* The **base** R `aggregate()` command as an alternative  




<!-- ======================================================= -->
## Preparation {  }
     
### Load packages {-}  
     
This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

Ensure the **tidyverse** package is installed and loaded (this includes **dplyr**).  

```{r}
pacman::p_load(
  rio,       # to import data
  here,      # to locate files
  tidyverse, # to clean, handle, and plot the data (includes dplyr)
  janitor)   # adding total rows and columns
```




### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
linelist <- rio::import(here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
linelist <- rio::import(here("data", "linelist_cleaned.xlsx"))
```


The first 50 rows of `linelist`:  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Grouping {  }
     
The function `group_by()` from **dplyr** groups the rows by the unique values in the specified columns. Each unique value constitutes a group (or unique combination of values, if multiple grouping columns are specified). Subsequent changes to the dataset or calculations can then be performed within the context of each unique group.  

For example, the command below takes the linelist and groups the rows by unique values in column `outcome`, saving the output as a new dataframe `ll_by_outcome`. The grouping column name is placed inside the parentheses of the function `group_by()`.  

```{r}
ll_by_outcome <- linelist %>% 
  group_by(outcome)
```

**Note that there is no perceptible change to the dataset** after `group_by()`, *until* another **dplyr** verb such as `mutate()` or `summarise()` is applied on the "grouped" dataframe.  

You can however "see" the groupings by printing the dataframe. When you print a grouped dataframe, you will see it has been transformed into a `tibble` class object (LINK) which, when printed, displays which grouping columns have been applied and how many groups there are - written just above the header row.  

```{r}
# print to see which groups are active
ll_by_outcome
```


### Unique groups {-}  

**The groups created reflect each unique combination of values in the grouping columns.** To see the groups and the number of rows in each group, pass the grouped data to `tally()`. To see just the unique groups without counts you can pass to `group_keys()`.  

See below that there are **three** unique values in the grouping column `outcome`: "Death", "Recover", and `NA`. See that there were `r nrow(linelist %>% filter(outcome == "Death"))` deaths, `r nrow(linelist %>% filter(outcome == "Recover"))` recoveries, and `r nrow(linelist %>% filter(is.na(outcome)))` with no outcome recorded.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally()
```

You can group by more than one column. Below, the dataframe is grouped by `outcome` and `gender`, and then tallied. Note how each unique combination of `outcome` and `gender` is registered as its own group - including missing values for either column.   

```{r}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally()
```

### New columns {-} 

You can also create a new grouping column *within* the `group_by()` statement. This is equivalent to calling `mutate()` before the `group_by()`. For a quick tabulation this style can be handy, but for more clarity in your code consider creating this column in it's own `mutate()` step and then piping to `group_by()`.

```{r}
# group dat based on a binary column created *within* the group_by() command
linelist %>% 
  group_by(
    age_class = ifelse(age >= 18, "adult", "child")) %>% 
  tally(sort = T)
```

### Add/drop grouping columns {-}  

By default if you run `group_by()` on data that are already grouped, the old groups will be removed and the new one(s) will apply. If you want to add new groups to the existing ones, include the argument `.add=TRUE`.  

````{r, eval=F}
# Grouped by outcome
by_outcome <- linelist %>% 
  group_by(outcome)

# Add grouping by gender in addition
by_outcome_gender <- by_outcome %>% 
  group_by(gender, .add = TRUE)
```

If you group on a column of class Factor there may be levels of the Factor that are not present in the data. If you group on this column, by default those non-present levels are dropped and not included as groups. If you want to change this, set `.drop = FALSE` in your `group_by()` command.  


## Un-group  

Data that have been grouped will remain grouped until specifically ungrouped via `ungroup()`. If you forget to ungroup, it can lead to incorrect calculations! Below is an example of removing all grouping columns:  

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup()
```

You can also remove grouping by only specific columns, by placing the column name inside.  

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup(gender) # remove the grouping by gender, leave grouping by outcome
```


<span style="color: black;">**_NOTE:_** The verb `count()` automatically ungroups the data after counting.</span>



## Summarise  

See the page on [Descriptive tables] for a detailed description of how to produce summary tables with `summarise()`. Here we briefly address how its behavior changes when applied to grouped data.  

By applying the **dplyr** verb `summarise()` to grouped data, you can produce summary tables containing descriptive statistics *for each group*. The grouping columns will always be returned in the new data frame.  

Within the summarise statement, provide the name(s) of the **new** summary column(s), an equals sign, and then a statistical function to apply to the data, as shown below. For example, `min()`, `max()`, `median()`, `sd()`. Within the statistical function, list the column to be operated on and any relevant argument (e.g. `na.rm = TRUE`). You can use `sum()` to count the number of rows that meet a logical criteria (use double equals `==`).   

Below is an example of `summarise()` applied *without grouped data*. The statistics returned are produced from the entire dataset.     

```{r}
# summary statistics on ungrouped linelist
linelist %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males  = sum(gender == "m", na.rm=T))
```

In contrast, below is the same `summarise()` statement applied to grouped data. The statistics are calculated for each `outcome` group.  

```{r}
# summary statistics on grouped linelist
linelist %>% 
  group_by(outcome) %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males    = sum(gender == "m", na.rm=T))
```

<span style="color: darkgreen;">**_TIP:_** The summarise function works with both UK and US spelling - `summarise()` and `summarize()` call the same function.</span>




## Count and tally  

`count()` and `tally()` provide similar functionality but are different.  

`tally()` is shorthand for `summarise()`, and *does not* automatically group data. Thus, to achieve grouped tallys it must follow a `group_by()` command. You can add `sort = TRUE` to see the largest groups first.    

```{r}
linelist %>% 
  tally
```
```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally(sort = TRUE)
```

In contrast, `count()` does the following:  

* applies `group_by()` on the specified column(s)  
* applies `summarise()` and returns column `n` with the number of observations per group  
* applies `ungroup()`  

```{r}
linelist %>% 
  count(outcome)
```

Just like with `group_by()` you can create a new column within the `count()` command:  

```{r}
linelist %>% 
  count(age_class = ifelse(age >= 18, "adult", "child"), sort = T)
```

Read more about the distinction between `tally()` and `count()` [here](https://dplyr.tidyverse.org/reference/tally.html)  

Both of these verbs can be called multiple times, with the functionality "rolling up". For example, to summarise the number of genders present for each outcome, run the following. Note, the name of the final column is changed from default "n" for clarity.  

```{r}
linelist %>% 
  # produce counts by outcome-gender groups
  count(outcome, gender) %>% 
  # produce counts of gender within each outcome group
  count(outcome, name = "number of genders per outcome" ) 
```


### Add totals {-} 

If you want to add total rows or column after using `tally()` or `count()`, see the page on [Descriptive tables] for demonstrations of the **janitor** package. This package offers functions like `tabyl()` to make cross-tabulations, and `adorn_totals()` and `adorn_percentages()` to add totals and convert to show percentages. Below is a brief example:  

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts of two columns
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions with column denominator
  adorn_pct_formatting() %>%                  # convert proportions to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```


## Grouping by date  

When grouping data by date, you must have or create a column for the date unit of interest, for example "day", "epiweek", "month", etc. You can make this column using `floor_date()` from **lubridate**, as explained in the page on [Working with dates] (Epidemiological weeks section). Once you have this column, you can use `count()` to group the rows and achieve aggregate counts. 

One additional step that is particular to dates is to use `complete()` from **tidyr** so that the aggregated date series is *complete* including *all possible date units* within the range. Without this step, a week with no cases reported might not appear in your data! Within `complete()` you re-define the date column as a sequence of dates from the minimum to the maximum - thus the dates are expanded.  


### Linelist cases into days  {-}  

Here is an example of grouping cases into days *without* using `complete()`. Note the first rows skip over dates with no cases.  

```{r}
daily_counts <- linelist %>% 
  filter(!is.na(date_onset)) %>%        # remove that were missing date_onset
  count(date_onset)                     # count number of rows per unique date
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below we add the `complete()` command to ensure every day in the range is represented.

```{r, eval=F}
daily_counts <- linelist %>% 
  filter(!is.na(date_onset)) %>%                  # remove case missing date_onset
  count(date_onset) %>%                           # count number of rows per unique date
  complete(date_onset = seq.Date(min(date_onset), # ensure all days appear even if no cases
                                 max(date_onset),
                                 by="day")) %>% 
  mutate(n = replace_na(n, 0))                    # replace NA counts with 0
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Linelist cases into weeks {-}  


The same principle can be applied for weeks. First create a new column that is the week of the case using `floor_date()` with `unit = "week"`. Then, use `count()` as above to achieve weekly case counts. Finish with `complete()` to ensure that all weeks are represented, even if they contain no cases.

```{r}
# Make dataset of weekly case counts
weekly_counts <- linelist %>% 
  filter(!is.na(date_onset)) %>%                      # remove cases missing date_onset
  mutate(week = lubridate::floor_date(date_onset, unit = "week")) %>%  # new column of week of onset
  count(week) %>%                                     # group data by week and count rows per group
  complete(week = seq.Date(from = min(week),          # include all weeks, even if no cases
                           to = max(week),
                           by="week")) %>% 
  mutate(n = replace_na(n, 0))                        # fill-in NA values with 0
```

Here are the first 50 rows of the resulting dataframe:  

```{r message=FALSE, echo=F}
DT::datatable(weekly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
#### Daily counts into weeks {-}

To aggregate daily counts into weekly counts, use `floor_date()` as above. However, use `group_by()` and `summarize()` instead of `count()` because you need to `sum()` daily case counts instead of just counting the number of rows per week.


#### Linelist cases into months {-}

To aggregate cases into months, again use `floor_date()` from the **lubridate** package, but with the argument `unit = "months"`. This rounds each date down to the 1st of its month. The output will be class Date. Note that in the `complete()` step we also use `by = "months"`.  


```{r}
# Make dataset of monthly case counts
monthly_counts <- linelist %>% 
  filter(!is.na(date_onset)) %>% 
  mutate(month = lubridate::floor_date(date_onset, unit = "months")) %>%  # new column, 1st of month of onset
  count(month) %>%                          # count cases by month
  complete(month = seq.Date(min(month),     # include all months with no cases reported
                            max(month),
                            by="month")) %>% 
  mutate(month = replace_na(n, 0))          # replace NA with 0 for months with no cases
```

```{r message=FALSE, echo=F}
DT::datatable(monthly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Daily counts into months {-}

To aggregate daily counts into months counts, use `floor_date()` with `unit = "month"` as above. However, use `group_by()` and `summarize()` instead of `count()` because you need to `sum()` daily case counts instead of just counting the number of rows per month.  




## Arranging grouped data

Using the **dplyr** verb `arrange()` to order the rows in a dataframe behaves the same when the data are grouped, *unless* you set the argument `.by_group =TRUE`. In this case the rows are ordered first by the grouping columns and then by any other columns you specify to `arrange()`.   



## Filter on grouped data

### `filter()` {-}

When applied in conjunction with functions that evaluate the dataframe (like `max()`, `min()`, `mean()`), these functions will now be applied to the groups. For example, if you want to filter and keep rows where patients are above the median age, this will now apply per group - filtering to keep rows above the group's median age. 




### Slice rows per group {-} 

The **dplyr** function `slice()`, which [subsets rows based on their position](https://dplyr.tidyverse.org/reference/slice.html) in the data, can also be applied per group. Remember to account for sorting the data within each group to get the desired "slice".  

For example, to retrieve only the latest 5 admissions from each hospital:  

1) Group the linelist by column `hospital`  
2) Arrange the records from latest to earliest `date_hospitalisation` *within each hospital group*  
3) Slice to retrieve the first 5 rows from each hospital  

```{r, eval=T}
linelist %>%
  group_by(hospital) %>%
  arrange(hospital, date_hospitalisation) %>%
  slice_head(n = 5) %>% 
  arrange(hospital) %>% 
  select(case_id, hospital, date_hospitalisation)
```
`slice_head()` - selects n rows from the top  
`slice_tail()` - selects n rows from the end  
`slice_sample()` - randomly selects n rows  
`slice_min()` - selects n rows with highest values in `order_by = ` column, use `with_ties = TRUE` to keep ties  
`slice_max()` - selects n rows with lowest values in `order_by = ` column, use `with_ties = TRUE` to keep ties  




### Filter on group size {-} 

The function `add_count()` adds a column `n` to the original data giving the number of rows in that row's group. 

Shown below for simplicity is a selection of two columns from the `linelist` data. `add_count()` is applied to `hospital`, so the values in column `n` reflect the number of rows in that row's hospital group. Note how values in column `n` are repeated. In the example below, the column name `n` could be changed using `name = ` within `add_count()`.       

```{r}
linelist %>% 
  select(case_id, hospital) %>% 
  add_count(hospital) %>%          # add "number of rows admitted to same hospital as this row" 
  head(10)                         # show just the first 10 rows, for demo purposes
```

It then becomes easy to filter for case rows who were hospitalized at a "small" hospital, say, a hospital that admitted fewer than 500 patients:  

```{r, eval=F}
linelist %>% 
  select(case_id, hospital) %>% 
  add_count(hospital) %>% 
  filter(n < 500)
```





## Mutate on grouped data  

To retain all columns and rows (not summarize) and *add a new variable containing group statistics*, use `mutate()` instead of `summarise()`. 

This is useful if you want group statistics in the original dataset with all other column present - e.g. for calculations comparing one row to the group.  

For example, this code below calculates the difference between a row's delay-to-admission and the median delay for their hospital. The steps are:  

1) Group the data by hospital  
2) Use the column `days_onset_hosp` (delay to hospitalisation) to create a new column containing the mean delay at the hospital of *that row*  
3) Calculate the difference between the two columns  


```{r}
linelist %>% 
  # group data by hospital (no change to linelist yet)
  group_by(hospital) %>% 
  
  # new columns
  mutate(
    # mean days to admission per hospital (rounded to 1 decimal)
    group_delay_admit = round(mean(days_onset_hosp, na.rm=T), 1),
    
    # difference between row's delay and mean delay at their hospital (rounded to 1 decimal)
    diff_to_group     = round(days_onset_hosp - group_delay_admit, 1)) %>%
  
  # select certain rows only - for demonstration/viewing purposes
  select(case_id, hospital, days_onset_hosp, group_delay_admit, diff_to_group)
```



## Select on grouped data  

The verb `select()` works on grouped data, but the grouping columns are always included (even if not mentioned in `select()`). If you do not want these grouping columns, use `ungroup()` first.  










<!-- ======================================================= -->
## Resources {  }

Here are some useful resources for more information:
* https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf
* https://datacarpentry.org/R-genomics/04-dplyr.html
* https://dplyr.tidyverse.org/reference/group_by.html
* https://dplyr.tidyverse.org/articles/grouping.html  
* https://itsalocke.com/files/DataManipulationinR.pdf

[Summarize with conditions](https://stackoverflow.com/questions/23528862/summarize-with-conditions-in-dplyr)  

You can perform any summary function on grouped data; see the Cheat Sheet here for more info:
https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/grouping.Rmd-->


# Joining data { }  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
```

This page describes common "joins" and also probabilistic matching between dataframes.  


<!-- ======================================================= -->
## Preparation { }

### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,            # import/export
  here,           # relative filepaths
  tidyverse,      # data management/viz
  RecordLinkage,  # probabilistic matches
  fastLink        # probabilistic matches
)
```
Because traditional joins (non-probabilistic) can be very specific, requiring exact string matches, you may need to do cleaning on the datasets *prior to* the join (e.g. change spellings, change case to all lower or upper).  


### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
### Datasets { }

In the joining examples, we'll use the following datasets:  

1) A "miniature" version of the `linelist`, containing only the columns `case_id`, `date_onset`, and `hospital`, and only the first 10 rows  
2) A separate dataframe named `hosp_info`, which contains more details about each hospital  
3) Two separate small dataframes for the probabilistic matching section  


#### "Miniature" linelist {-}  

Below is the the miniature linelist, which contains only 10 rows and only columns `case_id`, `date_onset`, and `hospital`.  

```{r}
linelist_mini <- linelist %>%                 # start with original linelist
  select(case_id, date_onset, hospital) %>%   # select columns
  head(10)                                    # only take the first 10 rows
```

```{r message=FALSE, echo=F}
DT::datatable(linelist_mini, rownames = FALSE, options = list(pageLength = nrow(10)))
```




#### Hospital information dataframe {-}  

Below is the separate dataframe with additional information about each hospital.  

```{r}
# Make the hospital information dataframe
hosp_info = data.frame(
  hosp_name     = c("central hospital", "military", "military", "port", "St. Mark's", "ignace", "sisters"),
  catchment_pop = c(1950280, 40500, 10000, 50280, 12000, 5000, 4200),
  level         = c("Tertiary", "Secondary", "Primary", "Secondary", "Secondary", "Primary", "Primary")
)
```

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(hosp_info, rownames = FALSE, options = list(pageLength = nrow(hosp_info)))
```





<!-- ======================================================= -->
### Pre-cleaning {-}

Because traditional (non-probabilistic) joins are case-sensitive and require exact string matches, we will clean-up the `hosp_info` dataset prior to the joins.  

**Identify differences**  

We need the values of `hosp_name` column in `hosp_info` dataframe to match the values of `hospital` column in the `linelist` dataframe.  

Here are the values in `linelist_mini`:  

```{r}
unique(linelist_mini$hospital)
```

and here are the values in `hosp_info`:  

```{r}
unique(hosp_info$hosp_name)
```



**Align matching values**  

We begin by cleaning the values in `hosp_name`. We use logic to code the values in the new column using `case_when()` (read more about `case_when()` in the [Cleaning data and core functions] page). We correct the hospital names that exist in both dataframes, and leave the others as they are (`TRUE ~ hosp_name`).   

<span style="color: orange;">**_CAUTION:_** Typically, one should create a new column (e.g. `hosp_name_clean`), but for ease of demonstration we show modification of the old column</span>

```{r}
hosp_info <- hosp_info %>% 
  mutate(
    hosp_name = case_when(
      hosp_name == "military"          ~ "Military Hospital",
      hosp_name == "port"              ~ "Port Hospital",
      hosp_name == "St. Mark's"        ~ "St. Mark's Maternity Hospital (SMMH)",
      hosp_name == "central hospital"  ~ "Central Hospital",
      TRUE                             ~ hosp_name
      )
    )
```

We now see that the hospital names that appear in both dataframe are aligned. There are some hospitals in `hosp_info` that are not present in `linelist` - we will deal with these later, in the join.  

```{r}
unique(hosp_info$hosp_name)
```

If you need to convert all values to UPPER or lower case, use these functions from **stringr**, as shown in the page on [Characters and strings].  

`str_to_upper()`  
`str_to_upper()`  
`str_to_title()`  




<!-- ======================================================= -->
## **dplyr** joins { }

The **dplyr** package offers several different joins. **dplyr** is included in the **tidyverse** package. These join functions are described below, with simple use cases. Many thanks to [https://github.com/gadenbuie](https://github.com/gadenbuie/tidyexplain/tree/master/images) for the moving images!  




<!-- ======================================================= -->
### General syntax {-}


**General function structure**  

Any of these join commands can be run independently, like below.  

An object is being created, or re-defined: dataframe 2 (`df2`) is being joined to dataframe 1 (`df1`), on the basis of matches between the column "ID" in `df1` and the column "identifier" in `df2`. Because this example uses `left_join()`, any rows in `df2` that do not match to a row in `df1` will be dropped.     

```{r, eval=F}
object <- left_join(df1, df2, by = c("ID" = "identifier"))
```

The join commands can also be run within a pipe chain. The first dataframe `df1` is the dataframe that is being passed through the pipes. `df2` is joined to it with the `left_join()` command. An example is shown below.  

```{r eval=F}
object <- df1 %>%
  left_join(df2, by = c("ID" = "identifier"))  # join df2 to df1
```

**Join columns (`by = `)**  

You must specify the columns in each dataset in which the values must match, using the arguemnt `by = `. You have a few options:  

* Specify only one column name (`by = "ID"`) - this only works if this exact column name is present in both dataframes!  
* Specify the different names (`by = c("ID" = "Identifier")` - use this if the column names are different in the 2 dataframes  
* Specify multiple columns to match on (`by = c("ID" = "Identifier", "date_onset" = "Date_of_Onset")`) - this will require exact matches on multiple columns for rows to join.  


<span style="color: orange;">**_CAUTION:_** Joins are case-specific! Therefore it is useful to convert all values to lowercase or uppercase prior to joining. See the page on characters/strings.</span>





<!-- ======================================================= -->
### Add columns: left & right joins {-}  

**A left or right join is commonly used to add information to a dataframe** - new information is added only to rows that already exist in the baseline (starting) dataframe.  

These are common joins in epidemiological work - they are used to add information from one dataset into another. 

*The order of the dataframes is important*.  

* In a *left join*, the *first* (left) dataframe listed is the baseline  
* In a *right join*, the *second* (right) dataframe listed is the baseline  

**All rows of the baseline dataframe are kept.** Information in the secondary dataframe is joined to the baseline dataframe *only if there is a match via the identifier column(s)*. In addition:  

* Rows in the secondary dataframe that do not match are dropped.  
* If there are many baseline rows that match to one row in the secondary dataframe (many-to-one), the baseline information is added to each matching baseline row.  
* If a baseline row matches to multiple rows in the secondary dataframe (one-to-many), all combinations are given, meaning *new rows may be added to your returned dataframe!*  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
knitr::include_graphics(here::here("images", "right-join.gif"))
```

**Example**  

Below is the output of a `left_join()` of `hosp_info` (secondary dataframe)  *into* `linelist_mini` (baseline dataframe). Note the following:  

* Two new columns, `catchment_pop` and `level` have been added on the left  
* All original rows of the baseline dataframe `linelist_mini` are kept  
* Any original rows of `linelist_mini` for "Military Hospital" are duplicated because it matched to *two* rows in the secondary dataframe, so both combinations are returned  
* The join identifier column of the secondary dataset (`hosp_name`) has disappeared because it is redundant with the identifier column in the primary dataset (`hospital`)  
* When a baseline row did not match to any secondary row (e.g. when `hospital` is "Other" or "Missing"), `NA` fills in the columns from the secondary dataframe  
* Rows in the secondary dataframe with no match to the baseline dataframe ("sisters" and "ignace") were dropped  


```{r, eval=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```




**"Should I use a right join, or a left join?"**  
Most important is to ask "which dataframe should retain all of its rows?" - use this one as the baseline.

The two commands below achieve the same output - 10 rows of `hosp_info` joined *into* a `linelist_mini` baseline. However, the column order will differ based on whether `hosp_info` arrives from the right (in the left join) or arrives from the left (in the right join). The order of the rows may also shift consequently.   

Also consider whether your use-case is within a pipe chain (`%>%`). If the dataset in the pipes is the baseline, you will likely use a left join to add data to it.  

```{r, eval=F}
# The two commands below achieve the same data, but with differently ordered rows and columns
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name"))
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital"))
```

```{r message=FALSE, echo=F}
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

```{r message=FALSE, echo=F}
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```




<!-- ======================================================= -->
### Full join {-} 

**A full join is the most *inclusive* of the joins** - it returns all rows from both dataframes.  

If there are any rows present in one and not the other (where no match was found), the dataframe will become wider as `NA` values are added to fill-in. Watch the number of columns and rows carefully and troubleshoot case-sensitivity and exact string matches. 

Adjustment of the "baseline" (first) dataframe will not impact which records are returned, but it will impact the column order, row order, and which identifier column is retained.  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "full-join.gif"))
```


**Example**  

Below is the output of a `full_join()` of `hosp_info`  *into* `linelist_mini`. Note the following:  

* All baseline rows (`linelist_mini`) are kept  
* Any baseline rows for "Military Hospital" are duplicated because they match to two secondary rows and both combinations are returned  
* Only the identifier column from the baseline is kept (`hospital`)  
* `NA` fills in where baseline rows did not match to secondary rows (`hospital` was "Other" or "Missing"), or the opposite (where `hosp_name` was "ignace" or "sisters")  


```{r, eval=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 13))
```





<!-- ======================================================= -->
### Inner join {-} 

**An inner join is the most *restrictive* of the joins** - it returns only rows with matches across both dataframes.  
This means that your original dataset may reduce in number of rows. Adjustment of the "baseline" (first) dataframe will not impact which records are returned, but it will impact the column order, row order, and which identifier column is retained.   


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "inner-join.gif"))
```


**Example**  

Below is the output of an `inner_join()` of `linelist_mini` (baseline) with `hosp_info` (secondary). Note the following:  

* Not all baseline rows are kept (rows where `hospital` is "Missing" or "Other" are removed because had no match in the secondary dataframe  
* Likewise, secondary rows where `hosp_name` is "sisters" or "ignace" are removed as they have no match in the baseline dataframe  
* Only the identifier column from the baseline is kept (`hospital`)  


```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```


```{r, eval=F}
hosp_info %>% 
  inner_join(linelist_mini, by = c("hosp_name" = "hospital"))
```


```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```






<!-- ======================================================= -->
### Semi join {-} 

A semi join is a "filtering join" which uses another dataset *not to add rows or columns, but to perform filtering*.  
A **semi-join keeps all observations in dataframe 1 that have a match in dataframe 2** (but does not add new columns or duplicate any rows with multiple matches). Read more about filtering joins [here](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "semi-join.gif"))
```

The below code would return **0** rows, because the two dataframes are completely different - there are no rows that are in both.  

```{r, eval=F}
hosp_info %>% 
  semi_join(linelist_mini, by = c("hosp_name" = "hospital"))
```



<!-- ======================================================= -->
### Anti join {-} 

**The anti join is a "filtering join" that returns rows in dataframe 1 that *do not* have a match in dataframe 2.**  

Read more about filtering joins [here](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

Common scenarios for an anti-join include identifying records not present in another dataframe, troubleshooting spelling in a join (catching records that *should have* matched), and examining records that were excluded after another join.  

**As with right_join() and left_join(), the *baseline* dataframe (listed first) is important**. The returned rows are from it only. Notice in the gif below that row in the non-baseline dataframe (purple 4) is not returned even though it does not match.  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "anti-join.gif"))
```

**Simple `anti_join()` example**  

For an example, let's find the `hosp_info` hospitals that do not have any cases present in `linelist_mini`. We list `hosp_info` first, as the baseline dataframe. The two hospitals which are not present in `linelist_mini` are returned.  

```{r, eval=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital"))
```

```{r message=FALSE, echo=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```


**`anti_join()` example 2**  

For another example, let us say we ran an `inner_join()` between `linelist_mini` and `hosp_info`. This returns only 8 of the original 11 `linelist_mini` records.  

```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```

To review the 3 `linelist_mini` records that were excluded in the inner join, we can run an anti-join with `linelist_mini` as the baseline dataframe.  

```{r, eval = F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 5))
```

To see the `hosp_info` records that were excluded in the inner join, we could also run an anti-join with `hosp_info` as the baseline dataframe.  






<!-- ======================================================= -->
## Probabalistic matching { }

If you do not have a unique identifier common across datasets to join on, consider using a probabilistic matching algorithm. This would find matches between records based on similarity (e.g. Jaro–Winkler string distance, or numeric distance).  Below is a simple example using the package **fastLink** .  

**Load packages**  

```{r}
pacman::p_load(
  tidyverse,      # data manipulation and visualization
  fastLink        # record matching
  )
```


Here are two small example datasets that we will use to demonstrate the probabilistic matching (`cases` and `test_results`):  

Here is the code used to make the datasets:  


```{r}
# make datasets

cases <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural"
)

results <- tribble(
  ~gender,  ~first,     ~middle,     ~last,          ~yr, ~mon, ~day, ~district, ~result,
  "M",      "Amir",     NA,          "Khan",         1989, 11,   22,  "River", "positive",
  "M",      "Tony",   "B",         "Smith",          1970, 09,   19,  "River", "positive",
  "F",      "Maria",    "Contreras", "Rodriguez",    1972, 04,   15,  "Cty",   "negative",
  "F",      "Betty",    "Castel",   "Chase",        1954,  03,   30,  "City",  "positive",
  "F",      "Andrea",   NA,          "Kumaraswamy",  2001, 01,   05,  "Rural", "positive",      
  "F",      "Caroline", NA,          "Wang",         1988, 12,   11,  "Rural", "negative",
  "F",      "Trang",    NA,          "Nguyen",       1981, 06,   10,  "Rural", "positive",
  "M",      "Olivier" , "Laurent",   "De Bordeaux",  NA,   NA,   NA,  "River", "positive",
  "M",      "Mike",     "Murphy",    "O'Callaghan",  1969, 04,   12,  "Rural", "negative",
  "F",      "Cassidy",  "Jones",     "Davis",        1980, 07,   02,  "City",  "positive",
  "M",      "Mohammad", NA,          "Ali",          1942, 01,   17,  "City",  "negative",
  NA,       "Jose",     "Sanchez",   "Lopez",        1995, 01,   06,  "City",  "negative",
  "M",      "Abubakar", NA,          "Abullahi",     1960, 01,   01,  "River", "positive",
  "F",      "Maria",    "Salinas",   "Contreras",    1955, 03,   03,  "River", "positive"
  )

```


**The `cases` dataset has 9 records** of patients who are awaiting test results.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(cases, rownames = FALSE, options = list(pageLength = nrow(cases), scrollX=T), class = 'white-space: nowrap')
```



**The `test_results` dataset** has 14 records and contains the column `result`, which we want to add to the records in `cases` based on probabilistic matching of records.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(results, rownames = FALSE, options = list(pageLength = nrow(results), scrollX=T), class = 'white-space: nowrap')
```

### Probabilistic matching {-}  

The `fastLink()` function from the **fastLink** package can be used to apply a matching algorithm. Here is the basic informaton. You can read more detail by entering `?fastLink` in your console.  

* Define the two dataframes for comparison to arguments `dfA = ` and `dfB = `  
* In `varnames = ` give all column names to be used for matching. They must all exist in both `dfA` and `dfB`.  
* In `stringdist.match = ` give columns from those in `varnames` to be evaluated on string "distance".  
* In `numeric.match = ` give columns from those in `varnames` to be evaluated on numeric distance.  
* Missing values are ignored  
* By default, each row in either dataframe is matched to at most one row in the other dataframe. If you want to see all the evaluated matches, set `dedupe.matches = FALSE`. The deduplication is done using Winkler's linear assignment solution.  

*Tip: split one date column into three separate numeric columns using `day()`, `month()`, and `year()` from **lubridate** package*  

The default threshold for matches is 0.94 (`threshold.match = `) but you can adjust it higher or lower. If you define the threshold, consider that higher thresholds could yield more false-negatives (rows that do not match which actually should match) and likewise a lower threshold could yield more false-positive matches.  

Below, the data are matched on string distance across the name and district columns, and on numeric distance for year, month, and day of birth. A match threshold of 95% probability is set.  


```{r, message=F, warning=F}
fl_output <- fastLink::fastLink(
  dfA = cases,
  dfB = results,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district"),
  stringdist.match = c("first", "middle", "last", "district"),
  numeric.match = c("yr", "mon", "day"),
  threshold.match = 0.95)
```

**Review matches**  

We defined the object returned from `fastLink()` as `fl_output`. It is of class `list`, and it actually contains several dataframes within it, detailing the results of the matching. One of these dataframes is `matches`, which contains the most likely matches across `cases` and `results`. You can access this "matches" dataframe with `fl_output$matches`. Below, it is saved as `my_matches` for ease of accessing later.    

When `my_matches` is printed, you see two column vectors: the pairs of row numbers/indices (also called "rownames") in `cases` ("inds.a") and in `results` ("inds.b") representing the best matches. If a row number from a datafrane is missing, then no match was found in the other at the specified match threshold.    

```{r}
# print matches
my_matches <- fl_output$matches
my_matches
```

Things to note:  

* Matches occurred despite slight differences in name spelling and dates of birth:  
  * "Tony" matched to "Anthony"  
  * "Maria" matched to "Marialisa"  
  * "Betty" matched to "Elizabeth"  
  * "Olivier Laurent De Bordeaux" matched to "Oliver Laurent De Bordow" (missing date of birth ignored)  
* One row from `cases` (for "Blessing Adebayo", row 9) had no good match in `results`, so it is not present in `my_matches`.  




**Join based on the probabilistic matches**  

To use these matches to join `results` to `cases`, one strategy is:  

1) Use `left_join()` to join `my_matches` to `cases` (matching rownames in `cases` to "inds.a" in `my_matches`)  
2) Then use another `left_join()` to join `results` to `cases` (matching the newly-acquired "inds.b" in `cases` to rownames in `results`)  

Before the joins, we should clean the three datasets:  

* Both `dfA` and `dfB` should have their row numbers ("rowname") converted to a proper column  
* Both the columns in `my_matches` are converted to class character, so they can be joined to the character rownames  

```{r}
# Clean data prior to joining
#############################

# convert cases rownames to a column 
cases_clean <- cases %>% rownames_to_column()

# convert test_results rownames to a column
results_clean <- results %>% rownames_to_column()  

# convert all columns in matches dataset to character, so they can be joined to the rownames
matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))



# Join matches to dfA, then add dfB
###################################
# column "inds.b" is added to dfA
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))

# column(s) from dfB are added 
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```

As performed using the code above, the resulting dataframe `complete` will contain *all* columns from both `cases` and `results`. Many will be appended with suffixes ".x" and ".y", because the column names would otherwise be duplicated.  

```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```

Alternatively, to achieve only the "original" 9 records in `cases` with the new column(s) from `results`, use `select()` on `results` before the joins, so that it contains only rownames and the columns that you want to add to `cases` (e.g. the column `result`).  

```{r}
cases_clean <- cases %>% rownames_to_column()

results_clean <- results %>%
  rownames_to_column() %>% 
  select(rowname, result)    # select only certain columns 

matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))

# joins
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```


```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```


If you want to subset either dataset to only the rows that matched, you can use the codes below:  

```{r}
cases_matched <- cases[my_matches$inds.a,]  # Rows in cases that matched to a row in results
results_matched <- results[my_matches$inds.b,]  # Rows in results that matched to a row in cases
```

Or, to see only the rows that did **not** match:  

```{r}
cases_not_matched <- cases[!rownames(cases) %in% my_matches$inds.a,]  # Rows in cases that did NOT match to a row in results
results_not_matched <- results[!rownames(results) %in% my_matches$inds.b,]  # Rows in results that did NOT match to a row in cases
```


### Probabilistic deduplication {-}  

Probabilistic matching can be used to deduplicate a dataset as well. See the page on deduplication for other methods of deduplication.  

Here we began with the `cases` dataset, but are now calling it `cases_dup`, as it has 2 additional rows that could be duplicates of previous rows:
See "Tony" with "Anthony", and "Marialisa Rodrigues" with "Maria Rodriguez".  

```{r, echo=F}
## Add duplicates
#cases_dup <- rbind(cases, cases[sample(1:nrow(cases), 3, replace = FALSE),])

cases_dup <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural",
  
  "M",     "Tony",   "B.",        "Smith",         1970, 09, 19,      "River", 
  "F",     "Maria",  "Contreras", "Rodriguez",     1972, 04, 15,      "River",
)

```

```{r message=FALSE, echo=F}
DT::datatable(cases_dup, rownames = FALSE, options = list(pageLength = nrow(cases_dup)))
```


Run the same `fastLink()` command as before, but compare the `cases_dup` dataframe to itself. When the two dataframes provided are identical, the function assumes you want to de-duplicate.  

```{r, message = F, warning = F}
## Run fastLink on the same dataset
dedupe_output <- fastLink(
  dfA = cases_dup,
  dfB = cases_dup,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district"),
  stringdist.match = c("first", "middle", "last", "district"),
  numeric.match = c("yr", "mon", "day")
)
```

`fl.out` must be of class `fastLink.dedupe`, or in other words, the result of either `fastLink()`.  

Now, you can review the potential duplicates with `getMatches()`. Provide the dataframe as both `dfA = ` and `dfB = `, and provide the output of the `fastLink()` function as `fl.out = `.  

```{r}
## Run getMatches()
cases_dedupe <- getMatches(
  dfA = cases_dup,
  dfB = cases_dup,
  fl.out = dedupe_output)
```

See the right-most column, which indicates the duplicate IDs - the final two rows are identified as being likely duplicates of rows 2 and 3.  

```{r message=FALSE, echo=F}
DT::datatable(cases_dedupe, rownames = FALSE, options = list(pageLength = nrow(cases_dedupe)))
```

To return the row numbers of rows which are likely duplicates, you can count the number of rows per unique value in the `dedupe.ids` column, and then filter to keep only those with more than one row. In this case this leaves rows 2 and 3.  

```{r}
cases_dedupe %>% 
  count(dedupe.ids) %>% 
  filter(n > 1)
```

To inspect the whole rows of the likely duplicates, put the row number in this command:  

```{r}
# displays row 2 and all likely duplicates of it
cases_dedupe[cases_dedupe$dedupe.ids == 2,]   
```





<!-- ======================================================= -->
## Resources { }

The [dplyr page on joins](https://dplyr.tidyverse.org/reference/join.html)  

See this vignette on [fastLink](https://github.com/kosukeimai/fastLink) at the package's Github page  

Publication describing methodolgy of [fastLink](https://imai.fas.harvard.edu/research/files/linkage.pdf)  

Publication describing [RecordLinkage package](https://journal.r-project.org/archive/2010/RJ-2010-017/RJ-2010-017.pdf)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/matching_joining.Rmd-->


# Characters and strings { }  


This page demonstrates use of the **stringr** package to evaluate and manage character (strings).  

1. Evaluate and extract by position - `str_length()`, `str_sub()`, `word()`  
2. Combine, order, arrange - `str_c()`, `str_glue()`, `str_order()`  
3. Modify and replace - `str_sub()`, `str_replace_all()`  
4. Adjust length - `str_pad()`, `str_trunc()`, `str_wrap()`  
5. Change case - `str_to_upper()`, `str_to_title()`, `str_to_lower()`, `str_to_sentence()`  
6. Search for patterns - `str_detect()`, `str_subset()`, `str_match()`  
7. Regular expressions (regex)


For ease of display most examples are shown acting on a short defined character vector, however they can easily be applied/adapted to a column within a dataset.  

Much of this page is adapted from this [online vignette](
https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)




<!-- ======================================================= -->
## Preparation { }

Install or load the **stringr** package.  

```{r}
# install/load the stringr package
pacman::p_load(
  stringr,    # many functions for handling strings
  tidyverse,  # for optional data manipulation
  tools)      # alternative for converting to title case

```






<!-- ======================================================= -->
## Handle by position { }


### Extract by character position {-}  

Use `str_sub()` to return only a part of a string. The function takes three main arguments:  

1) the character vector(s)  
2) start position  
3) end position  

A few notes on position numbers:  

* If a position number is positive, the position is counted starting from the left end of the string.  
* If a position number is negative, it is counted starting from the right end of the string.  
* Position numbers are inclusive.  
* Positions extending beyond the string will be truncated (removed).  

Below are some examples applied to the string "pneumonia":  

```{r}
# start and end third from left (3rd letter from left)
str_sub("pneumonia", 3, 3)

# 0 is not present
str_sub("pneumonia", 0, 0)

# 6th from left, to the 1st from right
str_sub("pneumonia", 6, -1)

# 5th from right, to the 2nd from right
str_sub("pneumonia", -5, -2)

# 4th from left to a position outside the string
str_sub("pneumonia", 4, 15)
```



### Extract by word position {-} 

To extract the nth 'word', use `word()`, also from **stringr**. Provide the string(s), then the first word position to extract, and the last word position to extract.  

By default, the separator between 'words' is assumed to be a space, unless otherwise indicated with `sep = ` (e.g. `sep = "_"` when words are separated by underscores.  


```{r}
# strings to evaluate
chief_complaints <- c("I just got out of the hospital 2 days ago, but still can barely breathe.",
                      "My stomach hurts",
                      "Severe ear pain")

# extract 1st to 3rd words of each string
word(chief_complaints, start = 1, end = 3, sep = " ")
```


### Replace by character position {-} 

`str_sub()` paired with the assignment operator (`<-`) can be used to modify a part of a string: 

```{r}
word <- "pneumonia"

# convert the third and fourth characters to X 
str_sub(word, 3, 4) <- "XX"

word
```

An example applied to multiple strings (e.g. a column). Note the expansion in length of "HIV".  

```{r}
words <- c("pneumonia", "tubercolosis", "HIV")

# convert the third and fourth characters to X 
str_sub(words, 3, 4) <- "XX"

words
```





### Evaluate length  {-}


```{r}
str_length("abc")
```

Alternatively, use `nchar()` from **base** R








<!-- ======================================================= -->
## Unite, split, and arrange { }


This section covers:  

* Using `str_c()`, `str_glue()`, and `unite()` to combine strings  
* Using `str_order()` to arrange strings  
* Using `str_split()` and `separate()` to split strings  

```{r, echo=F, message = F, warning=F}
df <- data.frame(case_ID = c(1:6),
                 symptoms  = 
                   c("jaundice, fever, chills",     # patient 1
                     "chills, aches, pains",        # patient 2 
                     "fever",                       # patient 3
                     "vomiting, diarrhoea",         # patient 4
                     "bleeding from gums, fever",   # patient 5
                     "rapid pulse, headache"),      # patient 6
                 outcome = c("Success", "Failure", 
                             "Failure", "Success",
                             "Success", "Success"))

df_split <- df %>% 
     separate(symptoms, into = c("sym_1", "sym_2", "sym_3"), extra = "merge")
```

<!-- ======================================================= -->
### Combine strings {-}

To combine or concatenate multiple strings into one string, we suggest using `str_c` from **stringr**.   

```{r}
str_c("String1", "String2", "String3")
```

The argument `sep = ` inserts characters between each input (e.g. a comma or newline `"\n"`)  

```{r}
str_c("String1", "String2", "String3", sep = ", ")
```

The argument `collapse = ` is relevant if producing multiple combined elements in the output. The example below shows the combination of two vectors into one (first names and last names). Another similar example might be jurisdictions and their case counts.    

`sep` displays between the respective string inputs, while `collapse` displays between the elements produced.  

In this example:  

* The `sep` value goes between each first and last name  
* The `collapse` value goes between each person  


```{r}
first_names <- c("abdul", "fahruk", "janice") 
last_names  <- c("hussein", "akinleye", "musa")

# sep displays between the respective input strings, while collapse displays between the elements produced
str_c(first_names, last_names, sep = " ", collapse = ";  ")
```

When printing such a combined string with newlines, you may need to wrap the whole phrase in `cat()` for the newlines to print properly:  

```{r}
# For newlines to print correctly, the phrase may need to be wrapped in cat()
cat(str_c(first_names, last_names, sep = " ", collapse = ";\n"))
```



<!-- ======================================================= -->
### Dynamic strings {-}

Use `str_glue()` to insert dynamic R code into a string. This is a very useful function for creating dynamic plot captions, as demonstrated below.  

* All content goes between quotation marks `str_glue("")`  
* Any dynamic code or calls of defined values are within curly brackets `{}` within the parentheses. There can be many curly brackets.  
* To display quotes within the outer quotation marks, use single quotes (e.g. when providing date format)  
* You can use `\n` within the quotes to force a new line  
* You use `format()` to adjust date display, and use `Sys.Date()` to display the current date  

A simple example, of a dynamic plot caption:  

```{r}
str_glue("The linelist is current to {format(Sys.Date(), '%d %b %Y')} and includes {nrow(linelist)} cases.")
```

An alternative format is to use placeholders within the brackets and define the code in separate arguments at the end of the `str_glue()` function, as below. This can improve code readability if the codes are long.

```{r}
str_glue("Data source is the confirmed case linelist as of {current_date}.\nThe last case was reported hospitalized on {last_hospital}.\n{n_missing_onset} cases are missing date of onset and not shown",
         current_date = format(Sys.Date(), '%d %b %Y'),
         last_hospital = format(as.Date(max(linelist$date_hospitalisation, na.rm=T)), '%d %b %Y'),
         n_missing_onset = nrow(linelist %>% filter(is.na(date_onset)))
         )

```


**Pulling from a dataframe**  

Sometimes, it is useful to pull data from dataframe and have it pasted together in sequence. Below is an example using this dataset to make a summary output of jurisdictions and the new and total cases:  

```{r}
# make case table
case_table <- data.frame(
  zone       = c("Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5"),
  new_cases = c(3, 0, 7, 0, 15),
  total_cases = c(40, 4, 25, 10, 103))
```

```{r, echo=F}
DT::datatable(case_table, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

**Option 1:**  

Use `str_c()` with the dataframe and column names. Provide `sep` and `collapse` arguments.  

```{r}
str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  ")
```

Add text "New Cases: " to the beginning of the summary by wrapping with a separate `str_c()` (if "New Cases:" was within the original `str_c()` it would appear multiple times).  

```{r}
str_c("New Cases: ", str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  "))
```

**Option 2:**  

You can achieve a similar result with `str_glue()`, with newlines added automatically:  

```{r}
str_glue("{case_table$zone}: {case_table$new_cases} new cases ({case_table$total_cases} total cases)")
```

To use str_glue() but have more control (e.g. to use double newlines), wrap it within `str_c()` and adjust the `collapse` value. You may need to print using `cat()` to correctly print the newlines.  

```{r}
case_summary <- str_c(str_glue("{case_table$zone}: {case_table$new_cases} new cases ({case_table$total_cases} total cases)"), collapse = "\n\n")

cat(case_summary) # print
```




### Unite columns  {-}

Within a dataframe, bringing together character values from multiple columns can be achieved with `unite()` from **tidyr**. This is the opposite of `separate()`.  

Provide the name of the new united column. Then provide the names of the columns you wish to unite.  

* By default the separator used in the united column is underscore `_`, but this can be changed with the `sep` argument.  
* `remove = ` removes the input columns from the data frame (TRUE by default)  
* `na.rm = ` removes missing values while uniting (FALSE by default)  

Below, we unite the three symptom columns in this dataframe.  

```{r, echo=F}
DT::datatable(df_split, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

```{r}
df_split %>% 
  unite(
    col = "all_symptoms",         # name of the new united column
    c("sym_1", "sym_2", "sym_3"), # columns to unite
    sep = ", ",                   # separator to use in united column
    remove = TRUE,                # if TRUE, removes input cols from the data frame
    na.rm = TRUE                  # if TRUE, missing values are removed before uniting
  )
```







<!-- ======================================================= -->
### Split  

To split a string based on a pattern, use `str_split()`. It evaluates the strings and returns a list of character vectors consisting of the newly-split values.

The simple example below evaluates one string and splits it into three. By default it returns a list with one element (a character vector) for each string provided. If `simplify = TRUE` it returns a character matrix.  

One string is provided, and returned is a list with one element, which is a character vector with three values  

```{r}
str_split(string = "jaundice, fever, chills",
          pattern = ",")
```

You can assign this as a named object, and access the nth symptom. To access a specific symptom you can use syntax like this: `the_split_return_object[[1]][2]`, which would access the second symptom from the first evaluated string ("fever"). See the [R Basics] page for more detail on accessing elements.    

```{r}
pt1_symptoms <- str_split("jaundice, fever, chills", ",")

pt1_symptoms[[1]][2]  # extracts 2nd value from 1st (and only) element of the list
```

If multiple strings are evaluated, there will be more than one element in the returned list.  

```{r}
symptoms <- c("jaundice, fever, chills",     # patient 1
              "chills, aches, pains",        # patient 2 
              "fever",                       # patient 3
              "vomiting, diarrhoea",         # patient 4
              "bleeding from gums, fever",   # patient 5
              "rapid pulse, headache")       # patient 6

str_split(symptoms, ",")                     # split each patient's symptoms
```


To return a "character matrix" instead, which may be useful if creating dataframe columns, set the argument `simplify = TRUE` as shown below:  

```{r}
str_split(symptoms, ",", simplify = TRUE)
```

You can also adjust the number of splits to create with the `n = ` argument. For example, this restricts the number of splits (from the left side) to 2 splits. The further commas remain within the second split. 

```{r}
str_split(symptoms, ",", simplify = TRUE, n = 2)
```

*Note - the same outputs can be achieved with `str_split_fixed()`, in which you do not give the `simplify` argument, but must instead designate the number of columns (`n`).* 

```{r, eval=F}
str_split_fixed(symptoms, ",", n = 2)
```




### Split columns {-}  

Use `separate()` from **dplyr** within a dataframe, to split one character column into other columns.  

If we have a simple dataframe `df` consisting of a case ID column, one character column with symptoms, and one outcome column:  


```{r, echo=F}
DT::datatable(df, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

First, provide the column to be separated. Then provide `into = ` as a vector `c( )` containing the *new* columns names, as shown below.  

* `sep = ` the separator, can be a character, or a number (interpreted as the character position to split at). 
* `remove = ` FALSE by default, removes the input column  
* `convert = ` FALSE by default, will cause string "NA"s to become `NA`  
* `extra = ` this controls what happens if there are more values created by the separation than new columns named.  
     * `extra = "warn"` means you will see a warning but it will drop excess values (the default)  
     * `extra = "drop"` means the excess values will be dropped with no warning  
     * **`extra = "merge"` will only split to the number of new columns listed in `into` - *this setting will preserve all your data***  


An example with `extra = "merge"` - no data is lost and third symptoms are combined into the second new named column:  

```{r}
# third symptoms combined into second new column
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",", extra = "merge")
```

When the default `extra = "drop"` is used below, a warning is given but the third symptoms are lost:  

```{r}
# third symptoms are lost
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",")
```


<span style="color: orange;">**_CAUTION:_** If you do not provide enough `into` values for the new columns, your data may be truncated.</span>  






<!-- ======================================================= -->
### Arrange {-} 

Several strings can be sorted by alphabetical order. `str_order()` returns the order, while `str_sort()` returns the strings in that order.  

```{r}
# strings
health_zones <- c("Alba", "Takota", "Delta")

# return the alphabetical order
str_order(health_zones)

# return the strings in alphabetical order
str_sort(health_zones)
```

To use a different alphabet, add the argument `locale = `. See the full list of locales by entering `stringi::stri_locale_list()` in the R console.  



To arrange strings in order of their value in another column, use `arrange()` like this: TO DO






<!-- ======================================================= -->
### base R functions

It is common to see **base** R functions `paste()` and `paste0()`, which concatenate vectors after converting all parts to character. They act similarly to `str_c()` but the syntax differs - in the code each part is separated by a comma. The parts are either text (in quotes) or pre-defined code objects. For example:

```{r}
n_beds <- 10
n_masks <- 20

paste("Regional hospital needs", n_beds, "beds and", n_masks, "masks.")
```

`sep` and `collapse` arguments can be adjusted. By default, `sep` is a space, unless using `paste0()` where there is no space between parts.  









<!-- ======================================================= -->
## Adjust length { }


### Pad  {-}

Use `str_pad()` to add characters to a string, to a minimum length. By default spaces are added, but you can also pad with other characters using the `pad = ` argument.  


```{r}
# ICD codes of differing length
ICD_codes <- c("R10.13",
               "R10.819",
               "R17")

# ICD codes padded to 7 characters on the right side
str_pad(ICD_codes, 7, "right")

# Pad with periods instead of spaces
str_pad(ICD_codes, 7, "right", pad = ".")
```

For example, to pad numbers with leading zeros (such as for hours or minutes), you can pad the number to minimum length of 2 with `pad = "0"`.

```{r}
# Add leading zeros to two digits (e.g. for times minutes/hours)
str_pad("4", 2, pad = "0") 

# example using a numeric column named "hours"
# hours <- str_pad(hours, 2, pad = "0")
```


### Truncate {-} 

`str_trunc()` sets a maximum length for each string. If a string exceeds this length, it is truncated (shortened) and an ellipsis (...) is included to indicate that the string was previously longer. Note that the ellipsis *is* counted in the length. The ellipsis characters can be changed with the argument `ellipsis = `.  The optional `side = ` argument specifies which where the ellipsis will appear within the truncated string ("left", "right", or "center").  

```{r}
original <- "Symptom onset on 4/3/2020 with vomiting"
str_trunc(original, 10, "center")
```


### Standardize length {-}

Use `str_trunc()` to set a maximum length, and then use `str_pad()` to expand the very short strings to that truncated length. In the example below, 6 is set as the maximum length (one value is truncated), and then a very short value is padded to achieve length of 6.    

```{r}
# ICD codes of differing length
ICD_codes   <- c("R10.13",
                 "R10.819",
                 "R17")

# truncate to maximum length of 6
ICD_codes_2 <- str_trunc(ICD_codes, 6)
ICD_codes_2

# expand to minimum length of 6
ICD_codes_3 <- str_pad(ICD_codes_2, 6, "right")
ICD_codes_3
```


### Remove leading/trailing whitespace {-}  

Use `str_trim()` to remove spaces, newlines (`\n`) or tabs (`\t`) on sides of a string input. Add `"right"` `"left"`, or `"both"` to the command to specify which side to trim (e.g. `str_trim(x, "right")`. 

```{r}
# ID numbers with excess spaces on right
IDs <- c("provA_1852  ", # two excess spaces
         "provA_2345",   # zero excess spaces
         "provA_9460 ")  # one excess space

# IDs trimmed to remove excess spaces on right side only
str_trim(IDs)
```


### Remove repeated whitespace within {-}  

Use `str_squish()` to remove repeated spaces that appear *inside* a string. For example, to convert double spaces into single spaces. It also removes spaces, newlines, or tabs on the outside of the string like `str_trim()`.  


```{r}
# original contains excess spaces within string
str_squish("  Pt requires   IV saline\n") 
```

Enter `?str_trim`, `?str_pad` in your R console to see further details.  


### Wrap into paragraphs {-}  

Use `str_wrap()` to wrap a long unstructured text into a structured paragraph with fixed line length. Provide the ideal character length for each line, and it applies an algorithm to insert newlines (`\n`) within the paragraph, as seen in the example below.   

```{r}
pt_course <- "Symptom onset 1/4/2020 vomiting chills fever. Pt saw traditional healer in home village on 2/4/2020. On 5/4/2020 pt symptoms worsened and was admitted to Lumta clinic. Sample was taken and pt was transported to regional hospital on 6/4/2020. Pt died at regional hospital on 7/4/2020."

str_wrap(pt_course, 40)
```

The **base** function `cat()` can be wrapped around the above command in order to print the output, displaying the new lines added.  

```{r}
cat(str_wrap(pt_course, 40))
```





<!-- ======================================================= -->
## Change case { }

Often one must alter the case/capitalization of a string value, for example names of jursidictions. Use `str_to_upper()`, `str_to_upper()`, and `str_to_title()`, as shown below:  

```{r}
str_to_upper("California")

str_to_lower("California")
```

Using *base** R, the above can also be achieved with `toupper()`, `tolower()`.  


**Title case**  

Transforming the string so each word is capitalized can be achieved with `str_to_title()`:  

```{r}
str_to_title("go to the US state of california ")
```

Use `toTitleCase()` from the **tools** package to achieve more nuanced capitalization (words like "to", "the", and "of" are not capitalized).  

```{r}
tools::toTitleCase("This is the US state of california")
```

You can also use `str_to_sentence()`, which capitalizes only the first letter of the string.

```{r}
str_to_sentence("the patient must be transported")
```


 


<!-- ======================================================= -->
## Patterns { }

Many **stringr** functions work to detect, locate, extract, match, replace, and split based on a specified *pattern*.  



<!-- ======================================================= -->
### Detect a pattern {-}

Use `str_detect()` as below to detect presence/absence of a pattern within a string. First list the string or vector to search in, and then the pattern to look for. Note that by default the search *is case sensitive*!

```{r}
str_detect("primary school teacher", "teach")
```

The argument `negate = ` can be included and set to `TRUE` if you want to know if the pattern is NOT present.  
 
```{r}
str_detect("primary school teacher", "teach", negate = TRUE)
```

To ignore case/capitalization, wrap the pattern within `regex()` and *within* `regex()` add the argument `ignore_case = T`.  

```{r}
str_detect("Teacher", regex("teach", ignore_case = T))
```

When `str_detect()` is applied to a character vector/column, it will return a TRUE/FALSE for each of the values in the vector. 

```{r}
# a vector/column of occupations 
occupations <- c("field laborer",
                 "university professor",
                 "primary school teacher & tutor",
                 "tutor",
                 "nurse at regional hospital",
                 "lineworker at Amberdeen Fish Factory",
                 "physican",
                 "cardiologist",
                 "office worker",
                 "food service")

# Detect presence of pattern "teach" in each string - output is vector of TRUE/FALSE
str_detect(occupations, "teach")
```

If you need to count these, apply `sum()` to the output. This counts the number TRUE.  

```{r}
sum(str_detect(occupations, "teach"))
```

To search inclusive of multiple terms, include them separated by OR bars (|) within the pattern, as shown below:  
```{r}
sum(str_detect(occupations, "teach|professor|tutor"))
```

If you need to make a long list of search terms, you can combine them using `str_c()` and `sep = |`, define this is a character object, and reference it later more succinctly. The example below includes possible occupation search terms for frontline medical providers.     

```{r}
# search terms
occupation_med_frontline <- str_c("medical", "medicine", "hcw", "healthcare", "home care", "home health",
                                "surgeon", "doctor", "doc", "physician", "surgery", "peds", "pediatrician",
                               "intensivist", "cardiologist", "coroner", "nurse", "nursing", "rn", "lpn",
                               "cna", "pa", "physician assistant", "mental health",
                               "emergency department technician", "resp therapist", "respiratory",
                                "phlebotomist", "pharmacy", "pharmacist", "hospital", "snf", "rehabilitation",
                               "rehab", "activity", "elderly", "subacute", "sub acute",
                                "clinic", "post acute", "therapist", "extended care",
                                "dental", "dential", "dentist", sep = "|")

occupation_med_frontline
```

This command returns the number of occupations which contain any one of the search terms for front-line medical providers (`occupation_med_frontline`):  

```{r}
sum(str_detect(occupations, occupation_med_frontline))
```



**Base R string search functions**  

The **base** function `grepl()` works similarly to `str_detect()`, in that it searches for matches to a pattern and returns a logical vector. The basic syntax is `grepl(pattern, strings_to_search, ignore.case = FALSE, ...)`. One advantage is that the `ignore.case` argument is easier to write (there is no need to involve `regex()` function).  

Likewise, the **base** functions `sub()` and `gsub()` act similarly to `str_replace()`. Their basic syntax is: `gsub(pattern, replacement, strings_to_search, ignore.case = FALSE)`. `sub()` will replace the first instance of the pattern, whereas `gsub()` will replace all instances of the pattern.  


#### Convert commas to periods {-}  

Here is an example of using `gsub()` to convert commas to periods in a vector of numbers. This could be useful if your data come from much of the world other from the United States or Great Britain.  

The inner `gsub()` which acts first on `lengths` is converting any periods to no space "". The period character "." has to be "escaped" with two slashes to actually signify a period, because "." in regex means "any character". Then, the result (with only commas) is passed to the outer `gsub()` in which commas are replaced by periods.   

```{r, eval=F}
lengths <- c("2.454,56", "1,2", "6.096,5")

as.numeric(gsub(pattern = ",",                # find commas     
                replacement = ".",            # replace with periods
                x = gsub("\\.", "", lengths)  # vector with other periods removed (periods escaped)
                )
           )                                  # convert outcome to numeric
```





### Replace all {-}  

Use `str_replace_all()` as a "find and replace" tool. First, provide the strings to be evaluated, then the pattern to be replaced, and then the replacement value. The example below replaces all instances of "dead" with "deceased". Note, this IS case sensitive.  

```{r}
outcome <- c("Karl: dead",
            "Samantha: dead",
            "Marco: not dead")

str_replace_all(outcome, "dead", "deceased")
```

To replace a pattern with `NA`, use `str_replace_na()`.  The function `str_replace()` replaces only the first instance of the pattern within each evaluated string.  





<!-- ======================================================= -->
### Detect within logic {-}


**Within `case_when()`**  

`str_detect()` is often used within `case_when()` (from **dplyr**). Let's say the occupations are a column in the linelist called `occupations`. The `mutate()` below creates a new column called `is_educator` by using conditional logic via `case_when()`. See the page on data cleaning to learn more about `case_when()`.  


```{r, eval=F}
df <- df %>% 
  mutate(is_educator = case_when(
    # term search within occupation, not case sensitive
    str_detect(occupations,
               regex("teach|prof|tutor|university",
                     ignore_case = TRUE))              ~ "Educator",
    # all others
    TRUE                                               ~ "Not an educator"))
```

As a reminder, it may be important to add exclusion criteria to the conditional logic (`negate = F`):  

```{r, eval=F}
df <- df %>% 
  # value in new column is_educator is based on conditional logic
  mutate(is_educator = case_when(
    
    # occupation column must meet 2 criteria to be assigned "Educator":
    # it must have a search term AND NOT any exclusion term
    
    # Must have a search term AND
    str_detect(occupations,
               regex("teach|prof|tutor|university", ignore_case = T)) &              
    # Must NOT have an exclusion term
    str_detect(occupations,
               regex("admin", ignore_case = T),
               negate = T)                          ~ "Educator"
    
    # All rows not meeting above criteria
    TRUE                                            ~ "Not an educator"))
```





<!-- ======================================================= -->
### Locate pattern position {-}  

To locate the *first* position of a pattern, use `str_locate()`. It outputs a start and end position.   

```{r}
str_locate("I wish", "sh")
```

Like other `str` functions, there is an "_all" version (`str_locate_all()`) which will return the positions of *all* instances of the pattern within each string. This outputs as a `list`.  

```{r}
phrases <- c("I wish", "I hope", "he hopes", "He hopes")

str_locate(phrases, "h" )     # position of *first* instance of the pattern
str_locate_all(phrases, "h" ) # position of *every* instance of the pattern
```





<!-- ======================================================= -->
### Extract a match {-}  

`str_extract_all()` returns the matching patterns themselves, which is most useful when you have offered several patterns via "OR" conditions. For example, looking in the string vector of occupations (see previous tab) for *either* "teach", "prof", or "tutor".

`str_extract_all()` returns a `list` which contains *all matches* for each evaluated string. See below how occupation 3 has two pattern matches within it.  

```{r}
str_extract_all(occupations, "teach|prof|tutor")
```


`str_extract()` extracts *only the first match* in each evaluated string, producing a character vector with one element for each evaluated string. It returns `NA` where there was no match. The `NA`s can be removed by wrapping the returned vector with `na.exclude()`. Note how the second of occupation 3's matches is not shown.  

```{r}
str_extract(occupations, "teach|prof|tutor")
```

<!-- ======================================================= -->
### Subset and count {-}  

**Subset, Count**  

Aligned functions include `str_subset()` and `str_count()`.  

`str_subset()` returns the actual values which contained the pattern: 

```{r}
str_subset(occupations, "teach|prof|tutor")
```

`str_count() returns a vector of numbers: the **number of times** a search term appears in each evaluated value.  

```{r}
str_count(occupations, regex("teach|prof|tutor", ignore_case = TRUE))
```












<!-- ======================================================= -->
### Regex groups {-}


**Groups within strings**  

`str_match()`   TBD







<!-- ======================================================= -->
## Regex and special characters { } 

Regular expressions, or "regex", is a concise language for describing patterns in strings.

*Much of this section is adapted from [this tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432) and [this cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)*  



<!-- ======================================================= -->
### Special characters {-}

**Backslash `\` as escape**  

The backslash `\` is used to "escape" the meaning of the next character. This way, a backslash can be used to have a quote mark display *within* other quote marks (`\"`) - the middle quote mark will not "break" the surrounding quote marks.  

Note - thus, if you want to *display* a backslash, you must escape it's meaning with *another backslash. So you must write two backslashes `\\` to display one.  

**Special characters**  

Special character | Represents  
----------------- | --------------------------------------------------------------    
`"\\"` | backslash  
`"\n"` | a new line (newline)   
`"\""` | double-quote *within* double quotes  
`'\''` | single-quote *within* single quotes  
`"\`"` | grave accent  
`"\r"` | carriage return  
`"\t"` | tab  
`"\v"` | vertical tab 
`"\b"` | backspace  


Run `?"'"` in the R Console to display a complete list of these special characters (it will appear in the RStudio Help pane). 



<!-- ======================================================= -->
### Regular expressions (regex) {-}

 If you are not familiar with it, a regular expression can look like an alien language:  

```{r, eval=F}

```

A regular expression is applied to extract specific patterns from unstructured text - for example medical notes, chief complaint, matient history, or other free text columns in a dataset.  

There are four basic tools one can use to create a basic regular expression:  

1) Character sets  
2) Meta characters  
3) Quantifiers  
4) Groups  


**Character sets**  

Character sets, are a way of expressing listing options for a character match, within brackets. So any a match will be triggered if any of the characters within the brackets are found in the string. For example, to look for vowels one could use this character set: "[aeiou]". Some other common character sets are:  

Character set | Matches for  
----------------- | --------------------------------------------------------------    
`"[A-Z]"` | any single capital letter  
`"[a-z]"` | any single lowercase letter  
`"[0-9]"` | any digit  
`[:alnum:]` | any alphanumeric character  
`[:digit:]` | any numeric digit  
`[:alpha:]` | any letter (upper or lowercase)  
`[:upper:]` | any uppercase letter  
`[:lower:]` | any lowercase letter  


Character sets can be combined within one bracket (no spaces!), such as `"[A-Za-z]"` (any upper or lowercase letter), or another example `"[t-z0-5]"` (lowercase t through z OR number 0 through 5).  



**Meta characters**  

Meta characters are shorthand for character sets. Some of the important ones are listed below:  

Meta character | Represents  
----------------- | --------------------------------------------------------------    
`"\\s"` | a single space  
`"\\w"` | any single alphanumeric character (A-Z, a-z, or 0-9)  
`"\\d"` | any single numeric digit (0-9)  


**Quantifiers**  

Typically you do not want to search for a match on only one character. Quantifiers allow you to designate the length of letters/numbers to allow for the match.  

Quantifiers are numbers written within curly brackets `{ }` *after* the character they are quantifying, for example,  

* `"A{2}"` will return instances of **two** capital A letters.  
* `"A{2,4}"` will return instances of **between two and four** capital A letters *(do not put spaces!)*.  
* `"A{2,}"` will return instances of **two or more** capital A letters.  
* `"A+"` will return instances of **one or more** capital A letters (group extended until a different character is encountered).  
* Precede with an `*` asterisk to return **zero or more** matches (useful if you are not sure the pattern is present)  


Using the `+` plus symbol as a quantifier, the match will occur until a different character is encountered. For example, this expression will return all *words* (alpha characters: `"[A-Za-z]+"`  


```{r}
# test string for quantifiers
test <- "A-AA-AAA-AAAA"
```

When a quantifier of {2} is used, only pairs of consecutive A's are returned. Two pairs are identified within `AAAA`.  

```{r}
str_extract_all(test, "A{2}")
```

When a quantifier of {2,4} is used, groups of consecutive A's that are two to four in length are returned.  

```{r}
str_extract_all(test, "A{2,4}")
```

With the quantifier `+`, groups of **one or more** are returned:  

```{r}
str_extract_all(test, "A+")
```

**Relative position**  

These express requirements for what precedes or follows a pattern. For example, to extract sentences, "two numbers that are followed by a period" (`""`).  (?<=\\.)\\s(?=[A-Z]) 

```{r}
str_extract_all(test, "")
```

Position statement | Matches to  
----------------- | --------------------------------------------------------------    
`"(?<=b)a"` | "a" that **is preceded** by a "b"  
`"(?<!b)a"` | "a" that **is NOT preceded** by a "b"  
`"a(?=b)"` | "a" that **is followed** by a "b"  
`"a(?!b)"` | "a" that **is NOT followed** by a "b"  





**Groups**  

Capturing groups in your regular expression is a way to have a more organized output upon extraction.  




**Regex examples**  

Below is a free text for the examples. We will try to extract useful information from it using a regular expression search term.  

```{r}
pt_note <- "Patient arrived at Broward Hospital emergency ward at 18:00 on 6/12/2005. Patient presented with radiating abdominal pain from LR quadrant. Patient skin was pale, cool, and clammy. Patient temperature was 99.8 degrees farinheit. Patient pulse rate was 100 bpm and thready. Respiratory rate was 29 per minute."
```

This expression matches to all words (any character until hitting non-character such as a space):  

```{r}
str_extract_all(pt_note, "[A-Za-z]+")
```

The expression `"[0-9]{1,2}"` matches to consecutive numbers that are 1 or 2 digits in length. It could also be written `"\\d{1,2}"`, or `"[:digit:]{1,2}"`.  

```{r}
str_extract_all(pt_note, "[0-9]{1,2}")
```

```{r}
str_split(pt_note, ".")
```

This expression will extract all sentences (assuming first letter is capitalized, and the sentence ends with a period). The pattern reads in English as: "A capital letter followed by some lowercase letters, a space, some letters, a space,    

```{r}
str_extract_all(pt_note, "[A-Z][a-z]+\\s\\w+\\s\\d{1,2}\\s\\w+\\s*\\w*")
```


You can view a useful list of regex expressions and tips on page 2 of [this cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)  

Also see this [tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432).  




<!-- ======================================================= -->
## Resources { }

A reference sheet for **stringr** functions can be found [here](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)


A vignette on **stringr** can be found [here](
https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/character_regex.Rmd-->


# De-duplication {}  

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "deduplication2.png"))
```

<!-- ======================================================= -->
## Overview { }

This page covers the following subjects:  

1. Identifying and removing duplicate rows  
2. "Slicing" and keeping only certain rows (min, max, random...), also from each group  
3. "Rolling-up", or combining values from multiple rows into one  


<!-- ======================================================= -->
## Preparation { }


### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,   # deduplication, grouping, and slicing functions
  janitor,     # function for reviewing duplicates
  stringr)      # for string searches, can be used in "rolling-up" values
```

### Import data {-}

For demonstration, we will use the example dataset that is created with the R code below. The data are records of COVID-19 phone encounters, including with contacts and with cases.  

* The first two records are 100% complete duplicates including duplicate `recordID` (computer glitch)  
* The second two rows are duplicates, in all columns *except for `recordID`*  
* Several people had multiple phone encounters, at various dates/times and as contacts or cases  
* At each encounter, the person was asked if they had **ever** had symptoms, and some of this information is missing.  

Here is the code to create the dataset:  

```{r}
obs <- data.frame(
  recordID  = c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18),
  personID  = c(1,1,2,2,3,2,4,5,6,7,2,1,3,3,4,5,5,7,8),
  name      = c("adam", "adam", "amrish", "amrish", "mariah", "amrish", "nikhil", "brian", "smita", "raquel", "amrish",
                "adam", "mariah", "mariah", "nikhil", "brian", "brian", "raquel", "natalie"),
  date      = c("1/1/2020", "1/1/2020", "2/1/2020", "2/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020","5/1/2020", "2/1/2020",
                "5/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "7/1/2020", "7/1/2020", "7/1/2020"),
  time      = c("09:00", "09:00", "14:20", "14:20", "12:00", "16:10", "13:01", "15:20", "14:20", "12:30", "10:24",
                "09:40", "07:25", "08:32", "15:36", "15:31", "07:59", "11:13", "17:12"),
  encounter = c(1,1,1,1,1,3,1,1,1,1,2,
                2,2,3,2,2,3,2,1),
  purpose   = c("contact", "contact", "contact", "contact", "case", "case", "contact", "contact", "contact", "contact", "contact",
                "case", "contact", "contact", "contact", "contact", "case", "contact", "case"),
  symptoms_ever = c(NA, NA, "No", "No", "No", "Yes", "Yes", "No", "Yes", NA, "Yes",
                    "No", "No", "No", "Yes", "Yes", "No","No", "No"))
```

Here is the dataset:  

```{r message=FALSE, echo=F}
DT::datatable(obs, rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```


And here is a quick summary of the people in the dataset and the purposes of their encounters:  

```{r}
obs %>% 
  tabyl(name, purpose)
```
<!-- ======================================================= -->
## Deduplication { }


This section describes how to review and remove duplicate rows in a dataframe. It also show how to handle duplicate elements in a vector.  


<!-- ======================================================= -->
### Examine duplicate rows {-}  


To quickly review rows that have duplicates, you can use `get_dupes()` from the **janitor** package. *By default*, all columns are considered when duplicates are evaluated - rows returned are 100% duplicates considering the values in *all* columns.  

In the `obs` dataframe, the first two rows are *100% duplicates* - they have the same value in every column (including the `recordID` column, which is *supposed* to be unique - it must be some computer glitch). The returned dataframe automatically includes a new column `dupe_count`, showing the number of rows with that combination of duplicate values. 

```{r, eval=F}
# 100% duplicates across all columns
obs %>% 
  janitor::get_dupes()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes() %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

However, if we choose to ignore `recordID`, the 3rd and 4th rows rows are also duplicates of each other. That is, they have the same values in all columns *except* for `recordID`. You can specify specific columns to be ignored in the function using a `-` minus symbol.  

```{r, eval=F}
# Duplicates when column recordID is not considered
obs %>% 
  janitor::get_dupes(-recordID)         # if multiple columns, wrap them in c()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(-recordID) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

You can also positively specify the columns to consider. Below, only rows that have the same values in the `name` and `purpose` columns are returned. Notice how "amrish" now has `dupe_count` equal to 3 to reflect his three "contact" encounters.  

*Scroll left for more rows**  

```{r, eval=F}
# duplicates based on name and purpose columns ONLY
obs %>% 
  janitor::get_dupes(name, purpose)
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(name, purpose) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 7, scrollX=T), class = 'white-space: nowrap' )
```

See `?get_dupes` for more details, or see this [online reference](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  






<!-- ======================================================= -->
### Keep only unique rows  {-}


To keep only unique rows of a dataframe, use `distinct()` from **dplyr**. Rows that are duplicates are removed such that only the first of such rows is kept. By default, "first" means the highest `rownumber` (order of rows top-to-bottom). Only unique rows are kept.  

In the example below, we run `distinct()` such that the column `recordID` is excluded from consideration - thus **two duplicate rows are removed**. The first row (for "adam") was 100% duplicated and has been removed. Also row 3 (for "amrish") was a duplicate in every column *except* `recordID` (which is not being considered) and so is also removed. The `obs` dataset n is now `r nrow(obs)-2`, not `r nrow(obs)` rows).  

*Scroll to the left to see the entire dataframe*  


```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(across(-recordID), # reduces dataframe to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) 

# if outside pipes, include the data as first argument 
# distinct(obs)
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(across(-recordID), # reduces dataframe to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">**_CAUTION:_** If using `distinct()` on grouped data, the function will apply to each group.</span>


**Deduplicate based on specific columns**  

You can also specify columns to be the basis for de-duplication. In this way, the de-duplication only applies to rows that are duplicates within the specified columns. Unless specified with `.keep_all = TRUE`, all columns not mentioned will be dropped.  

In the example below, the de-duplication only applies to rows that have identical values for `name` and `purpose` columns. Thus, "brian" has only 2 rows instead of 3 - his *first* "contact" encounter and his only "case" encounter. To adjust so that brian's *latest* encounter of each purpose is kept, see the tab on Slicing within groups.  

*Scroll to the left to see the entire dataframe*  

```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name)                                  # arrange for easier viewing
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Duplicate elements in a vector {-}  


The function `duplicated()` from **base** R will evaluate a vector (column) and return a logical vector of the same length (TRUE/FALSE). The first time a value appears, it will return FALSE (not a duplicate), and subsequent times that value appears it will return TRUE. Note how `NA` is treated the same as any other value.    

```{r}
x <- c(1, 1, 2, NA, NA, 4, 5, 4, 4, 1, 2)
duplicated(x)
```

To return only the duplicated elements, you can use brackets to subset the original vector: 

```{r}
x[duplicated(x)]
```

To return only the unique elements, use `unique()` from **base** R. To remove `NA`s from the output, nest `na.omit()` within `unique()`.  

```{r}
unique(x)           # alternatively, use x[!duplicated(x)]
unique(na.omit(x))  # remove NAs 
```


<!-- ======================================================= -->
### with **base** R {-}

**To return duplicate rows**  

In **base** R, you can also see which rows are 100% duplicates in a dataframe `df` with the command `duplicated(df)` (returns a logical vector of the rows).  

Thus, you can also use the base subset `[ ]` on the dataframe to see the *duplicated* rows with `df[duplicated(df),]` (don't forget the comma, meaning that you want to see all columns!). 

**To return unique rows**  

See the notes above. To see the *unique* rows you add the logical negator `!` in front of the `duplicated()` function:  
`df[!duplicated(df),]`  


**To return rows that are duplicates of only certain columns**  

Subset the `df` that is *within the `duplicated()` parentheses*, so this function will operate on only certain columns of the `df`.  

To specify the columns, provide column numbers or names after a comma (remember, all this is *within* the `duplicated()` function).  

Be sure to keep the comma `,` *outside* after the `duplicated()` function as well! 

For example, to evaluate only columns 2 through 5 for duplicates:  `df[!duplicated(df[, 2:5]),]`  
To evaluate only columns `name` and `purpose` for duplicates: `df[!duplicated(df[, c("name", "purpose)]),]`  





<!-- ======================================================= -->
## Slicing { }


To "slice" a dataframe is useful in de-duplication if you have multiple rows per functional group (e.g. per "person") and you only want to analyze one or some of them. Think of slicing as a filter on the rows, by row number/position. 

**The basic `slice()` function** accepts a number `n`. If positive, only the *nth* row is returned. If negative, all rows *except the nth* are returned.   

**Variations** include:    

* `slice_min()` and `slice_max()`  - to keep only the row with the minimium or maximum value of the specified column. Also worked with ordered factors.    
* `slice_head()` and `slice_tail` - to keep only the *first* or *last* row  
* `slice_sample()`  - to keep only a random sample of the rows  

Use arguments `n = ` or `prop = ` to specify the number or proportion of rows to keep. If not using the function in a pipe chain, provide the data argument first (e.g. `slice(df, n = 2)`). See `?slice` for more information.  

Other arguments:  

`.order_by = ` used in `slice_min()` and `slice_max()` this is a column to order by before slicing.  
`with_ties = ` TRUE by default, meaning ties are kept.  
`.preserve = ` FALSE by default. If TRUE then the grouping structure is re-calculated after slicing.  
`weight_by = ` Optional, numeric column to weight by (bigger number more likely to get sampled).  Also `replace = ` for whether sampling is done with/without replacement.  

<span style="color: darkgreen;">**_TIP:_** When using `slice_max()` and `slice_min()`, be sure to specify/write the `n = `  (e.g. `n = 2`, not just `2`). Otherwise you may get an error `Error: `...` is not empty.` </span>

<span style="color: black;">**_NOTE:_** You may encounter the function [`top_n()`](https://dplyr.tidyverse.org/reference/top_n.html), which has been superseded by the `slice` functions.</span>

Here, the basic `slice()` function is used to keep only the 4th row:  

```{r, eval=F}
obs %>% 
  slice(4)  # keeps the 4th row only
```

```{r message=FALSE, echo=F}
obs %>% 
  slice(4) %>%   # keeps the 4th row only
  DT::datatable(rownames = FALSE, options = list(pageLength = 1, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Slice with groups  {-}

The `slice_*()` functions can be very useful if applied to a grouped dataframe, as the slice operation is performed on each group separately. Use the **function** `group_by()` in conjunction with `slice()` to group the data and then take a slice from each group.  

This is helpful for de-duplication if you have multiple rows per person but only want to keep one of them. You first use `group_by()` with key columns that are the same, and then use a slice function on a column that will differ among the grouped rows.  

In the example below, to keep only the *latest* encounter *per person*, we group the rows by `name` and then use `slice_max()` with `n = 1` on the `date` column. Be aware! To apply a function like `slice_max()` on dates, the date column must be class Date.   

By default, "ties" (e.g. same date in this scenario) are kept, and we would still get multiple rows for some people (e.g. adam). To avoid this we set `with_ties = FALSE`. We get back only one row per person.  

<span style="color: orange;">**_CAUTION:_** If using `arrange()`, specify `.by_group = TRUE` to have the data arranged within each group.</span>

<span style="color: red;">**_DANGER:_** If `with_ties = FALSE`, the first row of a tie is kept. This may be deceptive. See how for Mariah, she has two encounters on her latest date (6 Jan) and the first (earliest) one was kept. Likely, we want to keep her later encounter on that day. See how to "break" these ties in the next example. </span>  




```{r, eval=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) # if there's a tie (of date), take the first row
```

```{r message=FALSE, echo=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) %>%  # if there's a tie (of date), take the first row
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

**Breaking "ties"**  

Multiple slice statements can be run to "break ties". In this case, if a person has multiple encounters on their latest *date*, the encounter with the latest *time* is kept (`lubridate::hm()` is used to convert the character times to a sortable time class).  
Note how now, the one row kept for "Mariah" on 6 Jan is encounter 3 from 08:32, not encounter 2 at 07:25.  

```{r, eval=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE)
```
```{r message=FALSE, echo=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE) %>% 
  
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

*In the example above, it would also have been possible to slice by `encounter` number, but we showed the slice on `date` and `time` for example purposes.*  

<span style="color: darkgreen;">**_TIP:_** To use `slice_max()` or `slice_min()` on a "character" column, mutate it to an *ordered* factor class!</span>



<!-- ======================================================= -->
### Keep all but mark them  {-}

If you want to keep all records but mark only some for analysis, consider a two-step approach utilizing a unique recordID/encounter number:  

1) Reduce/slice the orginal dataframe to only the rows for analysis. Save/retain this reduced dataframe.  
2) In the original dataframe, mark rows as appropriate with `case_when()`, based on whether their record unique identifier (recordID in this example) is present in the reduced dataframe.  


```{r, eval=F}
# 1. Define dataframe of rows to keep for analysis
obs_keep <- obs %>%
  group_by(name) %>%
  slice_max(encounter, n = 1, with_ties = FALSE) # keep only latest encounter per person


# 2. Mark original dataframe
obs_marked <- obs %>%

  # make new dup_record column
  mutate(dup_record = case_when(
    
    # if record is in obs_keep dataframe
    recordID %in% obs_keep$recordID ~ "For analysis", 
    
    # all else marked as "Ignore" for analysis purposes
    TRUE                            ~ "Ignore"))

# print
obs_marked
```

<!-- ======================================================= -->
### Calculate row completeness {-} 

Create a column that contains a metric for the row's completeness (non-missingness). This could be helpful when deciding which rows to prioritize over others when de-duplicating/slicing.  

In this example, "key" columns over which you want to measure completeness are saved in a vector of column names.  

Then the new column `key_completeness` is created with `mutate()`. The new value in each row is defined as a calculated fraction: the number of non-missing values in that row among the key columns, divided by the number of key columns.  

This involves the function `rowSums()` from **base** R. Also used is `.`, which within piping refers to the dataframe at that point in the pipe (in this case, it is being subset with brackets `[]`).  

*Scroll to the right to see more rows**  

```{r, eval=F}
# create a "key variable completeness" column
# this is a *proportion* of the columns designated as "key_cols" that have non-missing values

key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) 
```

```{r message=FALSE, echo=F}
key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Roll-up values { }


This section describes:  

1) How to "roll-up" values from multiple rows into just one row, with some variations  
2) Once you have "rolled-up" values, how to overwrite/prioritize the values in each cell  

This tab uses the example dataset from the Preparation tab.  



<!-- ======================================================= -->
### Roll-up values into one row {-}  

The code example below uses `group_by()` and `summarise()` to group rows by person, and then paste together all unique values within the grouped rows. Thus, you get one summary row per person. A few notes:  

* A suffix is appended to all new columns ("_roll" in this example)  
* If you want to show only unique values per cell, then wrap the `na.omit()` with `unique()`  
* `na.omit()` removes `NA` values, but if this is not desired it can be removed `paste0(.x)`...  

*Scroll to the left to see more rows*  

```{r, eval=F}
# "Roll-up" values into one row per group (per "personID") 
cases_rolled <- obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                           # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) # function is defined which combines non-NA values
```

The result is one row per group (`ID`), with entries arranged by date and pasted together.  

```{r message=FALSE, echo=F}
# "Roll-up" values into one row per group (per "personID") 
obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                                # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) %>%  # function is defined which combines non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

**This variation shows unique values only:**  

```{r}
# Variation - show unique values only 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) # function is defined which combines unique non-NA values
```

```{r message=FALSE, echo=F}
# Variation - show unique values only 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) %>%  # function is defined which combines unique non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


**This variation appends a suffix to each column.**  
In this case "_roll" to signify that it has been rolled:  

```{r, eval=F}
# Variation - suffix added to column names 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) # _roll is appended to column names
```

```{r message=FALSE, echo=F}
# display the linelist data as a table
# Variation - suffix added to column names 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) %>%  # _roll is appended to column names
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Overwrite values/hierarchy {-} 


If you then want to evaluate all of the rolled values, and keep only a specific value (e.g. "best" or "maximum" value), you can use `mutate()` across the desired columns, to implement `case_when()`, which uses `str_detect()` from the **stringr** package to sequentially look for string patterns and overwrite the cell content.  

```{r}
# CLEAN CASES
#############
cases_clean <- cases_rolled %>% 
    
    # clean Yes-No-Unknown vars: replace text with "highest" value present in the string
    mutate(across(c(contains("symptoms_ever")),                     # operates on specified columns (Y/N/U)
             list(mod = ~case_when(                                 # adds suffix "_mod" to new cols; implements case_when()
               
               str_detect(.x, "Yes")       ~ "Yes",                 # if "Yes" is detected, then cell value converts to yes
               str_detect(.x, "No")        ~ "No",                  # then, if "No" is detected, then cell value converts to no
               str_detect(.x, "Unknown")   ~ "Unknown",             # then, if "Unknown" is detected, then cell value converts to Unknown
               TRUE                        ~ as.character(.x)))),   # then, if anything else if it kept as is
      .keep = "unused")                                             # old columns removed, leaving only _mod columns
```


Now you can see in the column `symptoms_ever` that if the person EVER said "Yes" to symptoms, then only "Yes" is displayed.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(cases_clean, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap')
```


## Probabilistic de-duplication  

Sometimes, you may want to identify "likely" duplicates based on similarity (e.g. string "distance") across several columns such as name, age, sex, date of birth, etc. You can apply a probabilistic matching algorithm to identify likely duplicates.  

See the page on [Joining data] for an explanation on this method. The section on Probabilistic Matching contains an example of applying these algorithms to compare a dataframe to *itself*, thus performing probabilistic de-duplication.  



<!-- ======================================================= -->
## Resources { }

Much of the information in this page is adapted from these resources and vignettes online:  

[datanovia](https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/)

[dplyr tidyverse reference](https://dplyr.tidyverse.org/reference/slice.html)  

[cran janitor vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/deduplication.Rmd-->


# Iteration and loops { }  

PAGE IS CURRENTLY UNDER CONSTRUCTION

This page will introduce two approaches to iterative operations - using *for loops* and using the package **purrr**. Iterative operations help you perform repetitive tasks, reduce the chances of error, reduce code length, and maximize efficiency.  

1) **purrr** facilitates "mapping" a function across many inputs (columns, datasets, etc.)  

2) *for loops* also iterate code across a series of inputs, but are less common in R than in other programming languages because R can wrap up such processes into functions  


<!-- ======================================================= -->
## Preparation {  }


### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
     rio,         # import/export
     here,        # file locator
     purrr,       # iteration
     tidyverse    # data management and visualization
)
```


### Import data {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.
 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



<!-- ======================================================= -->
## **purrr** { }

One approach to iterative operations is the **purrr** package. If you are faced with performing the same task several times, it is probably worth creating a generalised solution that you can use across many inputs. For example, producing plots for multiple jurisdictions, or importing and combining many files.  

If you are using a *for loop*, you can probably do it more cleanly with **purrr**! 

### Load packages {-}  

**purrr** is part of the **tidyverse**, so there is no need to install/load a separate package.  

```{r}
pacman::p_load(
  rio,            # import/export
  here,           # relative filepaths
  tidyverse,      # data mgmt and viz
  writexl,        # write Excel file with multiple sheets
  readxl          # import Excel with multiple sheets
  )
```


### `map()` {-}  

One core **purrr** function is `map()`, which "maps" (applies) a function to each input element you provide.  
The basic syntax is `map(.x = SEQUENCE, .f = FUNCTION, OTHER ARGUMENTS)`. In a bit more detail:  

* `.x = ` are the inputs upon which the `.f` function will be iteratively applied - e.g. a vector of jurisdiction names, columns in a data frame, or a list of data frames  
* `.f = ` is the function to apply to each element of the `.x` input - it could be a function you define. It is written after a tilde `~`. 

A few more notes on syntax:  

* If `.f` needs no arguments specified, it can be written with no parentheses (e.g. `map(.x, ~mean)`)  
* To provide arguments that will be the same value for each iteration, provide them outside the `.f` function `map(.x ~mean, na.rm=T)`   
* If the value of an argument will change each iteration, or is the value of `.x` itself, provide it within the `.f` function parentheses  
  * You can use `.x` (or simply `.`) *within* the `.f` function as a placeholder for the `.x` value of that iteration  


**Let's demonstrate with a common epidemiologist task:** *you want to import an Excel workbook with case data, but the data are split across different named sheets in the workbook. How do you efficiently import and combine the sheets into one data frame?*  

Let's say we are sent the below Excel workbook. Each sheet contains cases from a given hospital.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "hospital_linelists_excel_sheets.png"))
```

Here is one approach that uses `map()`:  

1) `map()` the function `import()` on to each Excel sheet  
2) Combine the data frames into one using `bind_rows()`  
3) Along the way, we preserve the sheet name of origin for each case, storing this in a new column in the data frame  

First, we need to extract the sheet names and save them. We provide the Excel workbook's file path to the function `excel_sheets()` from the package **readxl**, which extracts the sheet names. We store them in a character vector called `sheet_names`.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here("data", "hospital_linelists.xlsx"))

```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")
```

Here are the names:  

```{r}
sheet_names
```

Now that we have this vector of names, `map()` can provide them one-by-one to the function `import()`. In this example, the `sheet_names` are `.x` and `import()` is the function `.f`.  

Recall from the [Import and export] page that when used on Excel workbooks, `import()` can accept the argument `which = ` specifying the sheet to import. Within the `.f` function (`import()`), we provide `which = .x`, whose value will change with each iteration through the vector `sheet_names` - first "Central Hospital", then "Military Hospital", etc.  

Of note - because we have used `map()`, the data in each Excel sheet will be saved into R as a separate data frame within a List. We want each of these list elements (data frames) to have a *name*, so before we pass `sheet_names` to `map()` we pass it through `set_names()`, which ensures that each list elements gets the appropriate name.  

We save the output List as `combined`.  

```{r, echo=F}
combined <- sheet_names %>% 
  set_names() %>% 
  map(.f = ~import(here("data", "hospital_linelists.xlsx"), which = .x))
```

```{r, eval=F}
combined <- sheet_names %>% 
  set_names() %>% 
  map(.f = ~import("hospital_linelists.xlsx", which = .x))
```

When we inspect the `combined` List output, we see that the data from each Excel sheet is saved as *named* data frames within the List. This is good, but we are not quite finished.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_list.png"))
```

Lastly, we use the function `bind_rows()` (from **dplyr**) which accepts the list of data frames and combines them into one data frame. To create a column from the list element *names*, we use the argument `.id = ` and provide it with the desired name of the new column.  

Below is the whole sequence of commands:  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here("data", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
  set_names() %>% 
  map(.f = ~import(here("data", "hospital_linelists.xlsx"), which = .x)) %>% 
  bind_rows(.id = "origin_sheet")
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")

combined <- sheet_names %>% 
  set_names() %>% 
  map(.f = ~import("hospital_linelists.xlsx", which = .x)) %>% 
  bind_rows(.id = "origin_sheet")
```

And now we have one data frame with a column containing the sheet of origin! 
```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_df.png"))
```

There are variations of `map()` that you should be aware of. For example - `map_dfr()` returns a data frame, not a list. Thus, we could have used it for the task above and not have to bind rows. But then we would not have been able to capture which sheet (hospital) each case came from.  

Other variations include `map_chr()`, `map_dbl()`, and `map_if()`. TO DO - MORE EXPLANATION  



### Mapping a function across columns {-}  

Another common use-case is to map a function across many columns. Below, we `map()` the function `t.test()` across numeric columns in the data frame `linelist`, comparing the numeric values by gender.  

Recall from the page on [Simple statistical tests] that `t.test()` can take inputs in a formula format, such as `t.test(numeric column ~ binary column)`. In this example, we do the following:    

* The numeric columns of interest are selected from `linelist` - these become the `.x` inputs to `map()`  
* The function `t.test()` is supplied as the `.f` function, which is applied to each numeric column  
* Within the parentheses of `t.test()`:  
  * the first `~` preceedes the `.f` that map will iterate over `.x`  
  * the `.x` represents the current column being supplied to the function `t.test()`  
  * the second `~` is part of the t-test equation described above  
  * the `t.test()` function expects a binary column on the right-hand side of the equation. We supply the vector `linelist$gender` independently and statically (note that it is not included in `select()`).  
  
`map()` returns a List, so the output is a list of t-test results - one list element for each numeric column analysed. **Below we show only the first one of six, for demonstration purposes.**   

```{r}
# Results are saved as a list
t.test_results <- linelist %>% 
  select(age, wt_kg, ht_cm, ct_blood, temp) %>%  # keep only the numeric columns to map across
  map(.f = ~t.test(.x ~ linelist$gender))              # t.test function, with equation NUMERIC ~ CATEGORICAL

t.test_results[[1]] # show first result 
```

If you wanted the p-values only, you can modify the `.f` function by appending `$p.value` to the `t.test()` output. This way, the value that `map()` returns to it's output list is only the p-value and not the entire t.test output.  

```{r}
linelist %>% 
  select(age, wt_kg, ht_cm, ct_blood, temp) %>% 
  map(.f = ~t.test(. ~ linelist$gender)$p.value)
```


Note:  
Remember that if you want to apply a function to only certain columns in a data frame, you can also use `mutate()` and `across()`, as explained in the [Cleaning data and core functions] page. Below is an example of applying `as.character()` to only the "age" columns. Note the placement of the parentheses and commas.  

```{r, eval=F}
# convert columns with column name containing "age" to class Character
linelist <- linelist %>% 
  mutate(across(.cols = contains("age"), .fns = as.character))  
```




### Custom functions {-}  

You will often want to create your own function to provide to `map()`. One example of making a purely custom plotting function to provide to `map()` is shown below.  

Let's say we want to create epidemic curves for each hospital's cases. To do this using **purrr**, our `.f` function can be `ggplot()` and extensions with `+` as usual. As the output of `map()` is always a list, the plots are stored in a list. They can be extracted and plotted with the `ggarrange()` function from the **ggpubr** package ([documentation](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html)).  


```{r, message = F, warning=F}

# load package for plotting elements from list
pacman::p_load(ggpubr)

# map across the vector of 6 hospital "names" (created earlier)
# use the ggplot function specified
# output is a list with 6 ggplots

hospital_names <- unique(linelist$hospital)

my_plots <- map(
  .x = hospital_names,
  .f = ~ggplot(data = linelist %>% filter(hospital == .x))+
                geom_histogram(aes(x = date_onset)) +
                labs(title = .x)
)

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```

If this code style looks too messy, you can achieve the same result by saving your specific `ggplot()` command as a custom user-defined function, for example we can name it `make_epicurve())`. This function is then used within the `map()`. `.x` will be iteratively replaced by the hospital name, and used as `hosp_name` in the `make_epicurve()` function. See the page on [Writing functions].

```{r, eval=F}
make_epicurve <- function(hosp_name){
  
  ggplot(data = linelist %>% filter(hospital == hosp_name)) +
    geom_histogram(aes(x = date_onset)) +
    theme_classic()+
    labs(title = hosp_name)
  
}
```

```{r, eval=F}
# mapping
my_plots <- map(hospital_names, ~make_epicurve(hosp_name = .x))

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```



### Split datasets {-}  

#### Split dataset and export CSV files {-}  

Here is a more complex **purrr** `map()` example that involves splitting a dataset and mapping functions to each part.  

Let's say we have the complete case `linelist` as a data frame, and  we now want to create a separate linelist for each hospital and export each as a separate CSV file. Below, we do the following steps:  

Use `group_split()` (from **dplyr**) to split the `linelist` data frame by unique values in column `hospital`. The output is a List containing one data frame per hospital subset.  

```{r}
linelist_split <- linelist %>% 
  group_split(hospital)
```

You can `View(linelsit_split)` and see that this list contains 6 data frames, each representing the cases from one hospital. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split.png"))
```

However, note that the data frames in the list do not have names by default! We want each to have a name, and then to use that name when saving the CSV file.  

So, we use `pull()` (from **purrr**) to extract the `hospital` column from each data frame in the list. Then, to be safe, we convert the values to character and then use `unique()` to get the name for that particular dataset. All of these steps are applied to each data frame via `map()`  


```{r}
names(linelist_split) <- linelist_split %>%   # Assign the names of each data frame in the list linelist_split 
                                              # Extract the names by doing the following to each data frame: 
  map(.f = ~pull(.x, hospital)) %>%             # Pull out hospital column
  map(.f = ~as.character(.x)) %>%               # Convert to character
  map(.f = ~unique(.x))                         # Take the unique hospital name
```

We can now see that each of the list elements has a name. These names can be accessed via `names(linelist_split)`.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split_named.png"))
```

```{r}
names(linelist_split)
```

Lastly, we will export each data frame as a .csv file, with a name specific to the hospital. Again we use `map()`: we take the vector of list element names (shown above) and use `map()` to iterate through them, applying `export()` (from **rio** package, see [Import and export] page) on the data frame in the list `linelist_split` that has that name. We also use the name to create a unique file name. Here is how it works:  

* We begin with the vector of character names, passed to `map()` as `.x`  
* The `.f` function is `export()` , which requires a data frame and a file path to write to  
* The input `.x` (the hospital name) is used *within* `.f` to extract/index that specific element of `linelist_split` list. This results in only one data frame at a time being provided to `export()`.  
  * For example, when `map()` iterates for "Military Hospital", then `linelist_split[[.x]]` is actually `linelist_split[["Military Hospital"]]`, thus returning the second element of `linelist_split` - which is all the cases from that Military Hospital.  
* The file path provided to `export()` is dynamic via use of `str_glue()` (see [Characters and strings] page):  
  * `here()` is used to get the base of the file path and specify the "data" folder (note single quotes to not interrupt the `str_glue()` double quotes)  
  * Then a slash `/`, and then again the `.x` which prints the current hospital name to make the file identifiable  
  * Finally the extension ".csv" which `export()` uses to create a CSV file  
  
```{r, eval=F, message = F, warning=F}
names(linelist_split) %>%
  map(.f = ~export(linelist_split[[.x]], file= str_glue("{here('data')}/{.x}.csv")))
```
Now you can see that each file is saved in the "data" folder of the R Project "Epi_R_handbook"!  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_csv.png"))
```


#### Split dataset and export as Excel sheets {-}  

To export the hospital linelists as *an Excel workbook with one linelist per sheet*, we can just provide the named list `linelist_split` to the `write_xlsx()` function from the **writexl** package. This has the ability to save one Excel workbook with multiple sheets. The list element names are automatically applied as the sheet names.  

```{r, eval=F}
linelist_split %>% 
  writexl::write_xlsx(path = here("data", "hospital_linelists.xlsx"))
```

You can now open the Excel file and see that each hospital has its own sheet.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_sheets.png"))
```

#### More than one `group_split()` column {-}  

If you wanted to split the linelist by *more than one grouping column*, such as to produce subset linelist by intersection of hospital AND gender, you will need a different approach to naming the list elements. This involves collecting the unique "group keys" using `group_keys()` from **dplyr** - they are returned as a data frame. Then you can combine the group keys into values with `unite()` as shown below, and assign these conglomerate names to `linelist_split`.  


```{r}
# split linelist by unique hospital-gender combinations
linelist_split <- linelist %>% 
  group_split(hospital, gender)

# extract group_keys() as a dataframe
groupings <- linelist %>% 
  group_by(hospital, gender) %>%       
  group_keys()

groupings      # show unique groupings 
```

Now we combine the groupings together, separated by dashes, and assign them as the names of list elements in `linelist_split`. This takes some extra lines as we replace `NA` with "Missing", use `unite()` from **dplyr** to combine the column values together (separated by dashes), and then convert into an un-named vector so it can be used as names of `linelist_split`.  

```{r, eval=F}
# Combine into one name value 
names(linelist_split) <- groupings %>% 
  mutate(across(everything(), replace_na, "Missing")) %>%  # replace NA with "Missing" in all columns
  unite("combined", sep = "-") %>%                         # Unite all column values into one
  setNames(NULL) %>% 
  as_vector() %>% 
  as.list()
```


### `pmap()` {-}

THIS SECTION IS UNDER CONSTRUCTION  




<!-- ======================================================= -->
## *for loops* {  }

As an epidemiologist, it is a common need to repeat analyses on sub-groups (e.g. jurisdictions or sub-populations). Iterating with a *for loop* is one method to automate this process.

A *for loop* has three core parts:  

1) The **container** for the results (optional)  
2) The **sequence** of items to iterate through  
3) The **operations** to conduct per item in the sequence  

The basic syntax is: `for (item in sequence) {do operations using item}`. Note the parentheses and the curly brackets. The results could be printed to console, or stored in a container R object.   


### Container {-}

Sometimes the results of your *for loop* will be printed to the console or Plots pane. Other times, you will want to store the outputs in a container for later use. Such a container could be a vector, a data frame, or even a list.  

It is most efficient to create the container for the results *before* even beginning the *for loop*. In practice, this means creating an empty vector, data frame, or list. These can be created with the functions `vector()` for vectors or lists, or with `matrix()` and `data.frame()` for a data frame. 

**Empty vector**  
Say you want to store the median delay-to-admission for each hospital in a new vector. Use `vector()` and specify the class as either "double" (to hold numbers), "character", or "logical". In this case we would use "double" and set the length to be the number of expected outputs (length of the *sequence*, or in this case the number of unique hospitals in the data set).  

```{r}
delays <- vector(mode = "double",
                 length = length(unique(linelist$hospital))) # this is the number of unique hospitals in the dataset
```

**Empty data frame**  

You can make an empty data frame by specifying the number of rows and columns like this:  

```{r, eval=F}
delays <- data.frame(matrix(ncol = 2, nrow = 3))
```


**Empty list**  

Say you want to store some plots created by a *for loop* in a list. You actually initialize the container using the same `vector()` command as above, but with `mode = "list"`. Specify the length however you wish.  

```{r, eval=F}
plots <- vector(mode = "list", length = 16)
```





### Sequence {-}  

This is the "for" part of a *for loop* - the operations will run for each item in the sequence. The sequence can be a series of character values (e.g. of jurisdictions, diseases, etc), or R object names (e.g. column names or list element names), or the sequence can be a series of consecutive numbers (e.g. 1,2,3,4,5). Each approach has their own utilities, described below.  

**Sequence of character values**  

In this case, the loop is applied for each value in a character vector.  
```{r}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)
hospital_names # print
```

The value of the "item", whose value changes each iteration of the loop, proceeds through each value in the character vector. In this example, the term `hosp` represents a value from the vector `hospital_names`. For the first iteration of the loop the value would be "Port Hospital". TFor the second loop it would be "St. Mark's Maternity Hospital (SMMH)". And so on...  

```{r, eval=F}
# 'for loop'
for (hosp in hospital_names){       # sequence
  
  # OPERATIONS HERE
  
}
```

**Sequence of names**  

This is a variation on the character sequence above, in which the names of an existing R object are extracted and become the character vector. For example, the column names of a data frame. This is useful because you know the names are exact matches to the column names and thus can be used to *index* the R object within the *for loop*.  

Below, the sequence is the `names()` (column names) of `linelist`. Inside the *for loop*, the column names are used to *index* (subset) `linelist` one-at-a-time. In this example, we demonstrate an *if* conditional statement as part of the operations code within the *for loop*. **If** the column of interest is class Numeric, then the mean of the column is printed to the console. If the column is not class Numeric then another statement is printed to the console.  

*A note on indexing with column names* - whenever referencing the column itself (e.g. within `mean()`) *do not just write "col"! `col` is just the character column name! To refer to the entire column you use the column name as an *index* on `linelist` via `linelist[[col]]`.  

```{r}
for (col in names(linelist)){ 
  
  # if column is class Numeric, print the mean value
  if(is.numeric(linelist[[col]])) {
    print(mean(linelist[[col]], na.rm=T))     # don't forget to index with [[col]]
    } else {        
    print("Column not numeric")            # if column is not numeric, print this
  }
  
}
```

**Sequence of numbers**  

Use this approach if you plan to do more complicated operations or to store the results of the *for loop*. In this approach, the sequence is a series of consecutive numbers. Thus, the value of the "item" is not a character value (e.g. "Central Hospital" or "date_onset") but is a number. This is useful for looping through data frames, as you can use the numeric item inside the *for loop* to index the dataframe by *row number*.  

For example, let's say that you want to loop over every row in your data frame and extract certain information. Your "items" would be numeric row numbers. The process could be explained as "for every item in a sequence of numbers from 1 to the total number of rows in my data frame, do X". The first iteration of the loop, `i` would be 1. For the second iteration, `i` would be 2, etc.    

Whew, that was a mouthful of words! Here is what it looks like in code: `for (i in seq_len(nrow(linelist)) {}` where `i` represents the item and `seq_len()` produces a sequence of consecutive numbers from 1 to the number of rows in `linelist`. If using this approach on a named vector (not a data frame), use `seq_along()`, like `for (i in seq_along(hospital_names) {}`.  

```{r, eval=F}
for (i in seq_len(nrow(linelist)) {  # use on a data frame
  # OPERATIONS HERE
}  
```

The below code actually returns numbers, which become the value of `i` in their respective loop.  

```{r}
seq_along(hospital_names)  # use on a named vector
```






### Operations  {-}  

This is code within the *for loop*. You want this to run for each item in the *sequence*. Therefore, be careful that every part of your code that changes by the item is correctly coded such that it changes! Remember to use `[[ ]]` for indexing. For example, 

Below, we use `seq_len()` on the linelist. The gender and age of each row is pasted together and stored the container character vector `cases_demographics`.  

```{r}
# create container to store results - a character vector
cases_demographics <- vector(mode = "character", length = nrow(linelist))

# the for loop
for (i in seq_len(nrow(linelist))){
  
  # OPERATIONS
  # extract values from linelist for i using indexing
  row_gender  <- linelist$gender[[i]]
  row_age     <- linelist$age_years[[i]]    # don't forget to index!
  
  # store the gender-age in container at indexed location
  cases_demographics[[i]] <- str_c(row_gender, row_age, sep = ", ") 

}  # end for loop

# display first 10 rows of container
head(cases_demographics, 10)
```


### Printing {-}  

Note that to print from within a *for loop* you will likely need to explicitly wrap with the function `print()`.  

In this example below, the sequence is an explicit character vector, which is used to subset the linelist by hospital.The results are not stored in a container, but rather print to console with the `print()` function.    

```{r}
for (hosp in hospital_names){ 
  hospital_cases <- linelist %>% filter(hospital == hosp)
  print(nrow(hospital_cases))
}
```


### Testing your for loop {-}

To test your loop, you can make a temporarily assignment of the item, such as `i <- 10` or `hosp <- "Central Hospital"` and run your operations code to see if the expected results are produced.  




### Looping plots {-}

To put all three components together (container, sequence, and operations) let's try to plot an epicurve for each hospital (see page on [Epidemic curves].  

Of course, we can make an epicurve of all the cases using the **incidence2** package as below:  

```{r, warning=F, message=F}
# create 'incidence' object
outbreak <- incidence2::incidence(   
     x = linelist,                   # dataframe - complete linelist
     date_index = date_onset,        # date column
     interval = "week",              # aggregate counts weekly
     groups = gender,                # group values by gender
     na_as_group = TRUE)             # missing gender is own group

# plot epi curve
plot(outbreak,                       # name of incidence object
     fill = "gender",                # color bars by gender
     color = "black",                # outline color of bars
     title = "Outbreak of ALL cases" # title
     )
```

To produce a separate plot for each hospital's cases, we can put this epicurve code within a *for loop*. 

First, we save a named vector of the unique hospital names, `hospital_names`. The *for loop* will run once for each of these names (`for (hosp in hospital_names)`). Each iteration of the *for loop*, the current hospital name from the vector will be represented as "hosp" for use within the loop.  

Within the loop, you can write R code as normal, but use the item (`hosp` in this case) knowing that its value will be changing. Within this loop:  

* A `filter()` is applied to `linelist`, such that column `hospital` must equal the current value of `hosp`  
* The incidence object is created on the filtered linelist  
* The plot for the current hospital is created, with an auto-adjusting title  
* The plot for the current hospital is temporarily saved and then printed  
* The loop then moves onward to repeat with the next hospital in `hospital_names`  

```{r, out.width='50%', message = F}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)

# for each name ("hosp") in hospital_names, create and print the epi curve
for (hosp in hospital_names) {
     
     # create incidence object specific to the current hospital
     outbreak_hosp <- incidence2::incidence(
                    x = linelist %>% filter(hospital == hosp),   # linelist is filtered to the current hospital
                    date_index = date_onset,
                    interval = "week", 
                    groups = gender,
                    na_as_group = TRUE
     )
     
     # Create and save the plot. Title automatically adjusts to the current hospital
     plot_hosp <- plot(outbreak_hosp,
                       fill = "gender",
                       color = "black",
                       title = stringr::str_glue("Epidemic of cases admitted to {hosp}")
                       )
     
     # print the plot for the current hospital
     print(plot_hosp)

} # end the for loop when it has been run for every hospital in hospital_names 
```



### Tracking progress of a loop {-} 

A loop with many iterations can run for many minutes or even hours. Thus, it can be helpful to print the progress to the R console. This code can be placed *within* the loop to print every 100th number.  

```{r, eval=F}
# loop with code to print progress every 100 iterations
for (row in 1:nrow(linelist)){

  # print progress
  if(row %% 100==0){    # The %% operator is the remainder
    print(row)

}
```








<!-- ======================================================= -->
## Resources { }

[for loops with Data Carpentry](https://datacarpentry.org/semester-biology/materials/for-loops-R/)  

The [R for Data Science page on iteration](https://r4ds.had.co.nz/iteration.html#iteration)  

[Vignette on write/read Excel files](https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/)  

A purrr [tutorial](https://jennybc.github.io/purrr-tutorial/index.html ) 

[purrr cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/thumbnails/purrr-cheatsheet-thumbs.png)



TO DO
group_split
collapse
pluck

set_names()
vars = linelist %>%
     select_if(is.numeric) %>%
     select(-cyl, - year) %>%
     names() %>%
     set_names()

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/iteration.Rmd-->

# (PART) Analysis {-}

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/cat_analysis.Rmd-->

# Descriptive tables { }


This page demonstrates the use of **janitor**, **dplyr**, **gtsummary**, and **base** R to produce tables and descriptive statistics. Each of these tools have advantages and disadvantages in the areas of code simplicity, accessibility of outputs, quality of printed outputs. Use this page to decide which approach works for your scenario.  

<!-- ======================================================= -->
## Preparation {  }


### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r, warning=F, message=F}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  janitor,      # adding totals and percents to tables
  scales,       # easily convert proportions to percents  
  flextable     # converting tables to HTML
  )
```

### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Browse data {  }

### `skimr` package {-}

Using the **skimr** package you can get a detailed and aesthetically pleasing overview of each of the variables in your dataset. Read more about **skimr** at its [github page](https://github.com/ropensci/skimr).  

Below, the function `skim()` is applied to the entire `linelist` data frame. An overview of the data frame and a summary of every column (by class) is produced.    

```{r eval=F}
## get information about each variable in a dataset 
skim(linelist)
```

```{r eval=T, echo=F}
# sparkline histograms not showing correctly, so avoiding them.
skim_without_charts(linelist)
```

You can also use the `summary()` function, from **base** R, to get information about an entire dataset, but this output can be more difficult to read than using **skimr**. Therefore the output is not shown below, to conserve page space.  

```{r, eval=F}
## get information about each column in a dataset 
summary(linelist)
```


### Summary statistics {-} 

You can use **base** R functions to return summary statistics on a numeric column. You can return most of the useful summary statistics for a numeric column using `summary()`, as below. Note that the data frame name must also be specified as shown below.  

```{r}
summary(linelist$age_years)
```

You can access and save one specific part of it with index brackets [ ]:  

```{r}
summary(linelist$age_years)[[2]]
```

You can return individual statistics with **base** R functions like `max()`, `min()`, `median()`, `mean()`, `quantile()`, `sd()`, and `range()`. See the [R Basics] page for a complete list.  

<span style="color: orange;">**_CAUTION:_** If your data contain missing values, R wants you to know this and so will return `NA` unless you specify to the above mathematical functions that you want R to ignore missing values, via the argument `na.rm = TRUE`.</span>



<!-- ======================================================= -->
## Descriptive tables {}

You have several choices when producing tabulation and cross-tabulation summary tables. Some of the factors to consider include code simplicity and ease, the desired output (printed to R console, or as pretty HTML), and what you can do with the data afterward. Consider the points below as you choose the tool for your situation.  

* Use `tabyl()` from **janitor** to produce and "adorn" tabulations and cross-tabulations  
* Use `summarise()` and `count()` from **dplyr** if calculating more complex statistics or preparing data for `ggplot()`  
* Use `tbl_summary()` from **gtsummary** to produce detailed publication-ready tables  
* Use `table()` from **base** R if you do not have access to the above packages  


## **janitor** package  

The **janitor** packages offers the `tabyl()` function to produce tabulations and cross-tabulations, which can be "adorned" or modified with helper functions to display percents, proportions, counts, etc.  

Below, we pipe the `linelist` data frame to **janitor** functions and print the result. If desired, you can also save the resulting tables with the assignment operator `<-`.  

### Simple tabyl {-}  

The default use of `tabyl()` on a specific column produces the unique values, counts, and column-wise "percents" (actually proportions). The proportions may have many digits. You can adjust the number of decimals with `adorn_rounding()` as described below.   

```{r}
linelist %>% tabyl(age_cat)
```
As you can see above, if there are missing values they display in a row labeled `<NA>`. You can suppress them with `show_na = FALSE`. If there are no missing values, this row will not appear. If there are missing values, proportions are given as both raw (denominator inclusive of `NA` counts) and "valid" (denominator excludes `NA` counts).  

If the column is class Factor and only certain levels are present in your data, all levels will still appear in the table. You can suppress this feature by specifying `show_missing_levels = FALSE`.  

### Cross-tabulation {-}  

Cross-tabulation counts are achieved by adding one or more additional columns within `tabyl()`. Note that only counts are returned - proportions and percents can be added with additional steps shown below.  

```{r}
linelist %>% tabyl(age_cat, gender)
```

### "Adorning" the tabyl {-}  

Use **janitor**'s "adorn" functions to add totals or convert to proportions, percents, or otherwise adjust the display. Often, you will pipe the tabyl through multiple of these functions.  


Function           | Outcome                          
-------------------|--------------------------------
`adorn_totals()`   | Adds totals (`where = ` "row", "col", or "both"). Set `name =` for "Total".  
`adorn_percentages()` | Convert counts to proportions, with `denominator = ` "row", "col", or "all"  
`adorn_pct_formatting()` | Converts proportions to percents. Specify `digits =`. Remove the "%" symbol with `affix_sign = FALSE`.  
`adorn_rounding()` | To round proportions to `digits =` places. To round percents use `adorn_pct_formatting()` with `digits = `.  
`adorn_ns()` | Add counts to a table of proportions or percents. Indicate `position =` "rear" to show counts in parentheses, or "front" to put the percents in parentheses.  
`adorn_title()` | Add string via arguments `row_name = ` and/or `col_name = `  

Be conscious of the order you apply the above functions. Below are some examples.  

A simple one-way table with percents instead of the default proportions.  

```{r}
linelist %>%               # case linelist
  tabyl(age_cat) %>%       # tabulate counts and proportions by age category
  adorn_pct_formatting()   # convert proportions to percents
```
A cross-tabulation with a total row and row percents.  

```{r}
linelist %>%                                  
  tabyl(age_cat, gender) %>%                  # counts by age and gender
  adorn_totals(where = "row") %>%             # add total row
  adorn_percentages(denominator = "row") %>%  # convert counts to proportions
  adorn_pct_formatting(digits = 1)            # convert proportions to percents
```
A cross-tabulation adjusted so that both counts and percents are displayed.  

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions
  adorn_pct_formatting() %>%                  # convert to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```



### Printing the tabyl {-}

By default, the tabyl will print raw to your R console. Alternatively, you can pass the tabyl to **flextable**  or other package to print as HTML in the RStudio Viewer. Note that if using `adorn_titles()`, you must specify `placement = "combined"` in order to print in this manner.

```{r}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% # this is necessary to print to HTML 
  flextable::flextable() %>%    # convert to HTML
  flextable::autofit()          # format to one line per row 

```
### Use on other tables {-}  

You can use **janitor**'s `adorn_*()` functions on other tables, such as those created by `summarise()`, `count()`, or `table()`.  


### Saving the tabyl {-}  

If you convert the table to HTML with a package like **flextable**, you can save with its functions `save_as_html()`, `save_as_word()`, `save_as_ppt()`, and `save_as_image()`, as discussed more extensively in the [HTML tables] page. Below, the table is saved as a Word document, in which it can be further hand-edited.  

```{r, eval=F}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% 
  flextable::flextable() %>%                     # convert to HTML flextable
  flextable::autofit() %>%                       # ensure only one line per row
  flextable::save_as_docx(path = "tabyl.docx")   # save as Word document
```

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "tabyl_word.png"))
```

### Statistics {-}  

You can apply statistical tests on tabyls, like `chisq.test()` or `fisher.test()` from the **stats** package, as shown below. Note missing values are not allowed so they are excluded from the tabyl with `show_na = FALSE`.  

```{r, warning=F, message=F}
age_by_outcome <- linelist %>% 
  tabyl(age_cat, outcome, show_na = FALSE) 

chisq.test(age_by_outcome)
```


### Other tips {-}  

* Include the argument `na.rm = TRUE` to exclude missing values from any of the above calculations.  
* If applying any `adorn_*()` helper functions to tables not created by `tabyl()`, you can specify particular column(s) to apply them to like  `adorn_percentage(,,,c(cases,deaths))` (specify them to the 4th unnamed argument). The syntax is not simple. Consider using `summarise()` instead.  
* You can read more detail in the [janitor page](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) and this [tabyl vignette](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html).  




## **dplyr** package   

**dplyr** is part of the **tidyverse** packages and is an very common data management tool. Creating tables with **dplyr** functions `summarise()` and `count()` is a useful approach to calculating summary statistics, summarize *by group*, or pass tables to `ggplot()`. 

`summarise()` creates a *new, summary data frame*. If the data are *ungrouped*, it will return a one-row dataframe with the specified summary statistics of the entire data frame. If the data are *grouped*, the new data frame will have one row per *group* (see [Grouping data] page).  

Within the `summarise()` parentheses, you provide the names of each new summary column followed by an equals sign and a statistical function to apply.  

<span style="color: darkgreen;">**_TIP:_** The summarise function works with both UK and US spelling (`summarise()` and `summarize()`).</span>

### Get counts {-}  

The most simple function to apply within `summarise()` is `n()`. Leave the parentheses empty to count the number of rows.  

```{r}
linelist %>%                 # begin with linelist
  summarise(n_rows = n())    # return new summary dataframe with column n_rows
```
This gets more interesting if we have grouped the data beforehand.  

```{r}
linelist %>% 
  group_by(age_cat) %>%     # group data by unique values in column age_cat
  summarise(n_rows = n())   # return number of rows *per group*
```
The above command can be shortened by using the `count()` function instead. `count()` groups the data by the columns provided to it, summarises them with `n()` (creating column `n`), and finishes by un-grouping the data.  

```{r}
linelist %>% 
  count(age_cat)
```
Tabulations of two or more columns in this manner are still returned in “long” format, with the counts in the `n` column.

```{r}
linelist %>% 
  count(age_cat, outcome)
```

### Proportions {-}  

Proportions can be added by piping the table to `mutate()` to create a new column. Define the new column are the counts column (`n` by default) divided by the `sum()` of the counts column (this will return a proportion). To easily get percents, you can wrap the result in the function `percent()` from the package **scales**.  

```{r}
age_summary <- linelist %>% 
  count(age_cat) %>%                     # group and count by gender (produces "n" column)
  mutate(                                # get percent of column - note the denominator
    percent = scales::percent(n / sum(n))) 

# print
age_summary
```

You can calculate proportions *within groups* by having two levels of aggregation prior to using `mutate()`. The table below first groups the data frame by `outcome` and then groups again and counts by column `age_cat`, achieving the breakdown of age *by outcome*. Note that you can add more stratifications by adding columns to the `group_by()` command.  

```{r}
age_by_outcome <- linelist %>% 
  group_by(outcome) %>%                  # group first by outcome 
  count(age_cat) %>%                     # group again and count by gender (produces "n" column)
  mutate(                                # calculate percent - note the denominator is by outcome group
    percent = scales::percent(n / sum(n))
    ) 
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```




### Plotting {-}  

To display a "long" table output like the above with `ggplot()` is relatively straight-forward. The data are naturally in "long" format, which is naturally accepted by `ggplot()`. See further examples in the pages [Plot categorical data] and [ggplot tips].  

```{r, warning=F, message=F}
linelist %>%                      # begin with linelist
  count(age_cat, outcome) %>%     # group and tabulate counts by two columns
  ggplot()+                       # pass new data frame to ggplot
    geom_bar(                     # create bar plot
      mapping = aes(   
        x = outcome,              # map outcome to x-axis
        fill = age_cat,           # map age_cat to the fill
        y = n),                   # map the counts column `n` to the height
      stat = "identity")          # set height from the y value, not the number of rows
```


### Summary statistics {-}  

One major advantage of **dplyr** and `summarise()` is the ability to return more advanced statistical summaries like `median()`, `mean()`, `max()`, `min()`, `sd()` (standard deviation), and percentiles. You can also use `sum()` to return the number of rows that meet certain logical criteria. As above, these outputs can be produced for the whole data frame set, or by group.  

As noted above, within the `summarise()` parentheses you provide the names of each new summary column followed by an equals sign and a statistical function to apply. Within the statistical function, give the column to be operated on and any relevant arguments (e.g. `na.rm = TRUE` for most mathematical functions). 

You can also use `sum()` to return the number of rows that meet a logical criteria. The expression within is counted if it evaluates to `TRUE`. For example: `sum(age_years < 18, na.rm=T)` or `sum(gender == "male", na.rm=T)`.    

Below, `linelist` data are summarised to describe the days delay from symptom onset to hospital admission (column `days_onset_hosp`), by hospital.  

```{r}
summary_table <- linelist %>%                                        # begin with linelist, save out as new object
  group_by(hospital) %>%                                             # group all calculations by hospital
  summarise(                                                         # only the below summary columns will be returned
    cases       = n(),                                                # number of rows per group
    delay_max   = max(days_onset_hosp, na.rm = T),                    # max delay
    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # mean delay, rounded
    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # standard deviation of delays, rounded
    delay_3     = sum(days_onset_hosp >= 3, na.rm = T),               # number of rows with delay of 3 or more days
    pct_delay_3 = scales::percent(delay_3 / cases)                    # convert previously-defined delay column to percent 
  )

summary_table  # print
```


Some tips:  

* Use `sum()` with a logic statement to "count" rows that meet certain criteria (`==`)  
* Note the use of `na.rm = TRUE` within mathematical functions like `sum()`, otherwise `NA` will be returned if there are any missing values  
* Use the function `percent()` from the **scales** package to easily convert to percents  
* Use `round()` from **base** R to specify decimals  
* To calculate these statistics on the entire dataset, use `summarise()` without `group_by()`  


### Glueing together {-}  

You can also use `str_glue()` from **stringr** to combine columns into one new column - typically used *after* the `summarise()` command.  

Below, the `summary_table` data frame created above is mutated such that columns `delay_mean` and `delay_sd` are combined, parentheses formating is added to the new column, and their respective old columns are removed.  

Then, to make the table more presentable, a total row is added with `adorn_totals()` from **janitor** (which ignores non-numeric columns). Lastly, we use `rename()` from **dplyr** to make the column names nicer.  

Now you could pass to **flextable** and print the table to Word, HTML, Powerpoint, RMarkdown, etc.! (see the **janitor** section above and the [HTML tables] page).  

```{r}
summary_table %>% 
  mutate(delay = str_glue("{delay_mean} ({delay_sd})")) %>%  # combine and format other values
  select(-c(delay_mean, delay_sd)) %>%                       # remove two old columns   
  adorn_totals(where = "row") %>%                            # add total row
  rename(                                                    # rename cols
    "Hospital Name"   = hospital,
    "Cases"           = cases,
    "Max delay"       = delay_max,
    "Mean (sd)"       = delay,
    "Delay 3+ days"   = delay_3,
    "% delay 3+ days" = pct_delay_3
    )
```

#### Percentiles {-}  

*Percentiles* deserve a special mention. To return percentiles, use `quantile()` with the defaults or specify the value(s) you would like with `probs = `.

```{r}
# get default percentile values of age (0%, 25%, 50%, 75%, 100%)
linelist %>% 
  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))

# get manually-specified percentile values of age (5%, 50%, 75%, 98%)
linelist %>% 
  summarise(
    age_percentiles = quantile(
      age_years,
      probs = c(.05, 0.5, 0.75, 0.98), 
      na.rm=TRUE)
    )
```



### On aggregated data {-}  

*If you begin with aggregated data*, use `sum()` on the data's counts column. For example, let's say you are beginning with the data frame of counts below, called `linelist_agg` - it shows in "long" format the case counts by outcome and gender.  

```{r, echo=F}
linelist_agg <- linelist %>% 
  filter(!is.na(gender),
         !is.na(outcome)) %>% 
  count(outcome, gender)

linelist_agg
```

To sum the counts (e.g. in column `n`) by group you can use `summarise()` as above but set the new column equal to `sum(n, na.rm=T)`. To add a subset criteria to the sum operation, you can use the subset bracket [ ] syntax on the counts column as shown below to create male and female columns.  

```{r}
linelist_agg %>% 
  group_by(outcome) %>% 
  summarise(
    total_cases  = sum(n, na.rm=T),
    male_cases   = sum(n[gender == "m"], na.rm=T),
    female_cases = sum(n[gender == "f"], na.rm=T))
```




### `across()` multiple columns {-}  

You can use summarise across multiple columns using `across()`. This makes life easier when you want to calculate same statistics for many columns. To specify which columns to operate across, either:  

* provide `.cols = ` as either a vector of column names `c()` or `select()` semantic helper functions (explained below)  
* provide `.fns = ` the function to perform (no parenthese) - you can provide multiple within a `list()`

Below, `mean()` is applied to several numeric columns. A vector of columns are named explicitly and a single function `mean` is specified (no parentheses). Any additional arguments for the function (e.g. `na.rm=TRUE`) are provided afterwards.  

It can be difficult to get the order of parentheses and commas correct when using `across()`. Remember that within `across()` you must include the columns, the functions, and any extra arguments needed for the functions. 

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columns
                   .fns = mean,                               # function
                   na.rm=T))                                  # extra arguments
```

Multiple functions can be run at once. Below the functions `mean` and `sd` are provided to `.fns = ` within a `list()`. You have the opportunity to provide character names (e.g. "mean" and "sd") which are appended in the new column names.  

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columns
                   .fns = list("mean" = mean, "sd" = sd),    # multiple functions 
                   na.rm=T))                                 # extra arguments
```

Here are those `select()` helper functions that you can place *within* `across()`:  

These are helper functions available to assist you in specifying columns:  

* `everything()`  - all other columns not mentioned  
* `last_col()`    - the last column  
* `where()`       - applies a function to all columns and selects those which are TRUE  
* `starts_with()` - matches to a specified prefix. Example: `starts_with("date")`
* `ends_with()`   - matches to a specified suffix. Example: `ends_with("_end")`  
* `contains()`    - columns containing a character string. Example: `contains("time")` 
* `matches()`     - to apply a regular expression (regex). Example: `contains("[pt]al")`  
* `num_range()`   - 
* `any_of()`      - matches if column is named. Useful if the name might not exist. Example: `any_of(date_onset, date_death, cardiac_arrest)`  


For example, to return the mean of every numeric column. The `where()` command takes the place of a vector of column names `c()`. Everything is all still within the `across()` command.  

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(
    .cols = where(is.numeric),  # all numeric columns in the data frame
    .fns = mean,
    na.rm=T))
```


### Pivot wider {-}

If you prefer your table in "wide" format you can transform it using the **tidyr** `pivot_wider()` function. You will likely need to re-name the columns with `rename()`. For more information see the page on [Pivoting data].  

The example below begins with the "long" table `age_by_outcome` from above. The new column names are specified to `names_from = ` while the values are specified to come from column `n`. The only column not mentioned is `outcome`, which remains on the far left.  

```{r}
age_by_outcome %>% 
  select(-percent) %>%   # keep only counts for simplicity
  pivot_wider(names_from = age_cat, values_from = n)  
```


## **gtsummary** package   

If you want to print your summary statistics in a pretty, publication-ready graphic, you can use the **gtsummary** package and its function `tbl_summary()`. The code can seem complex at first, but the outputs look very nice and print to your RStudio Viewer panel as HTML. Read a [vignette here](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html).    

To introduce `tbl_summary()` we will show the most basic behavior first, which actually produces a large and beautiful table. Then, we will examine in detail how to make adjustments and more tailored tables. 



### Summary table {-}

The default behavior of `tbl_summary()` is quite incredible - it takes the columns you provide and creates a summary table in one command. The function prints statistics appropriate to the column class: median and inter-quartile range (IQR) for numeric columns, and counts (%) for categorical columns. Missing values are converted to "Unknown". Footnotes are added to the bottom to explain the statistics, while the total N is shown at the top.  

```{r, warning=F, message=F}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>%  # keep only the columns of interest
  tbl_summary()                                                  # default
```


### Adjustments {-}  

Now we will explain how the function works and how to make adjustments. The key arguments are detailed below: 

**`by = `**  
You can stratify your table by a column (e.g. by `outcome`), creating a 2-way table.  

**`statistic = `**  
Indicate which statistics to show and how to display them with an equation. There are two sides to the equation, separated by a tilde `~`. On the right in quotes is the statistical display desired, and on the left are the columns to which that display will apply.  

* The right side of the equation uses the syntax of `str_glue()` from **stringr** (see [Characters and Strings]), with the desired display string in quotes and the statistics themselves within curly brackets. You can include statistics like "n" (for counts), "N" (for denominator), "mean", "median", "sd", "max", "min", percentiles as "p##" like "p25", or percent of total as "p". See `?tbl_summary` for details.  
* For the left side of the equation, you can specify columns by name (e.g. `age` or `c(age, gender)`) or using helpers such as `all_continuous()`, `all_categorical()`, `contains()`, `starts_with()`, etc.  

A simple example of a `statistic = ` equation might look like below, to only print the mean of column `age_years`:  

```{r}
linelist %>% 
  select(age_years) %>%         # keep only columns of interest 
  tbl_summary(                  # create summary table
    statistic = age_years ~ "{mean}") # print mean of age
```

A slightly more complex equation might look like `"({min}, {max})"`, incorporating the max and min values within parentheses and separated by a comma:  

```{r, eval=F}
linelist %>% 
  select(age_years) %>%                       # keep only columns of interest 
  tbl_summary(                                # create summary table
    statistic = age_years ~ "({min}, {max})") # print min and max of age
```

You can also differentiate syntax for separate columns or types of columns. In the more complex example below, the value provided to `statistc = ` is a **list** indicating that for all continuous columns the table should print mean with standard deviation in parentheses, while for all categorical columns it should print the n, denominator, and percent.  

**`digits = `**  
Adjust the digits and rounding. Optionally, this can be specified to be for continuous columns only (as below).  

**`label = `**  
Adjust how the column name should be displayed. Provide the column name and its desired label separated by a tilde. The default is the column name.  

**`missing_text = `**  
Adjust how missing values are displayed. The default is "Unknown".  

**`type = `**  
This is used to adjust how many levels of the statistics are shown. The syntax is similar to `statistic = ` in that you provide an equation with columns on the left and a value on the right. Two common scenarios include:  

* `type = all_categorical() ~ "categorical"` Forces dichotomous columns (e.g. fever) to show all levels instead of only the “yes” row  
* `type = all_continuous() ~ "continuous2"` Allows multi-line statistics per variable, as shown in a later section  

In the example below, each of these arguments is used to modify the original summary table:  

```{r}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>% # keep only columns of interest
  tbl_summary(     
    by = outcome,                                               # stratify entire table by outcome
    statistic = list(all_continuous() ~ "{mean} ({sd})",        # stats and format for continuous columns
                     all_categorical() ~ "{n} / {N} ({p}%)"),   # stats and format for categorical columns
    digits = all_continuous() ~ 1,                              # rounding for continuous columns
    type   = all_categorical() ~ "categorical",                 # force all categorical levels to display
    label  = list(                                              # display labels for column names
      outcome   ~ "Outcome",                           
      age_years ~ "Age (years)",
      gender    ~ "Gender",
      temp      ~ "Temperature",
      hospital  ~ "Hospital"),
    missing_text = "Missing"                                    # how missing values should display
  )
```



### Multi-line stats for continuous variables {-}  

If you want to print multiple lines of statistics for continuous variables, you can indicate this by setting the `type = ` to "continuous2".  You can combine all of the previously shown elements in one table by choosing which statistics you want to show. To do this you need to tell the function that you want to get a table back by entering the type as “continuous2”. The number of missing values is shown as "Unknown".

```{r}
linelist %>% 
  select(age_years, temp) %>%                      # keep only columns of interest
  tbl_summary(                                     # create summary table
    type = all_continuous() ~ "continuous2",       # indicate that you want to print multiple statistics 
    statistic = all_continuous() ~ c(
      "{mean} ({sd})",                             # line 1: mean and SD
      "{median} ({p25}, {p75})",                   # line 2: median and IQR
      "{min}, {max}")                              # line 3: min and max
    )
```
There are many other ways to modify these tables, including adding p-values, adjusting color and headings, etc. Many of these are described in the documentation (enter `?tbl_summary` in Console), and some are given in the section on statistical tests.  







## **base** R   

You can use the function `table()` to tabulate and cross-tabulate columns. Unlike the options above, you must specify the dataframe, as shown below.  

<span style="color: orange;">**_CAUTION:_** `NA` (missing) values will **not** be tabulated unless you include the argument `useNA = "always"` (which could also be set to "no" or "ifany").</span>

```{r}
table(linelist$outcome, useNA = "always")
```

Multiple columns can be cross-tabulated by listing them one after the other, separated by commas. Optionally, you can assign each column a "name" like `Outcome = linelist$outcome`.  

```{r}
age_by_outcome <- table(linelist$age_cat, linelist$outcome, useNA = "always") # save table as object
age_by_outcome   # print table
```

### Proportions {-}  

To return proportions, passing the above table to the function `prop.table()`. Use the `margins = ` argument to specify whether you want the proportions to be of rows (1), of columns (2), or of the whole table (3). For clarity, we pipe the table to the `round()` function from **base** R, specifying 2 digits.   

```{r}
# get proportions of table defined above, by rows, rounded
prop.table(age_by_outcome, 1) %>% round(2)
```

### Totals {-}  

To add row and column totals, pass the table to `addmargins()`. This works for both counts and proportions.  

```{r}
addmargins(age_by_outcome)
```

### Convert to data frame {-}  

Converting a `table()` object directly to a data frame is not straight-forward. One approach is demonstrated below:  

1) Create the table, *without using* `useNA = "always"`. Instead convert `NA` values to "(Missing)" with `fct_explicit_na()` from **forcats**.  
2) Add totals (optional) by piping to `addmargins()`  
3) Pipe to the **base** R function `as.data.frame.matrix()`  
4) Pipe the table to the **dplyr** function `add_rownames()`, specifying the name for the first column  
5) Print, View, or export as desired. In this example we use `flextable()` from package **flextable** as described in the [HTML tables] page. This will print to the RStudio viewer pane as a pretty HTML.  

```{r, warning=F, message=F}
table(fct_explicit_na(linelist$age_cat), fct_explicit_na(linelist$outcome)) %>% 
  addmargins() %>% 
  as.data.frame.matrix() %>% 
  dplyr::add_rownames(var = "Age Category") %>% 
  flextable::flextable()
```




<!-- ======================================================= -->

## Resources {  }

Much of the information in this page is adapted from these resources and vignettes online:  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)  

[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/descriptive_tables.Rmd-->

# Simple statistical tests { }


This page demonstrates the use of **base** R, **gtsummary**, and **dplyr** to conduct simple statistical tests.  

<!-- ======================================================= -->
## Preparation {  }


### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  janitor,      # adding totals and percents to tables
  flextable,    # converting tables to HTML
  corrr         # correlation analayis for numeric variables
  )
```

### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





## **base** R {-}

You can use **base** R functions to produce the results of statistical tests. The commands are relatively simple and results will print to the R Console for simple viewing. However, the outputs are usually lists and so are harder to manipulate if you want to use the results in subsequent code operations. 

### T-tests {-} 

**Syntax 1:** Best is your numeric and categorical columns are in the same data frame. Provide the numeric column on the left side of the equation and the categorical column on the right side. Specify the dataset to `data = `. Optionally, set `paired = TRUE`, and `conf.level = ` (0.95 default), and `alternative = ` (either "two.sided", "less", or "greater"). Enter `?t.test` for more details.  

```{r}
## compare mean age by outcome group with a t-test
t.test(age_years ~ outcome, data = linelist)
```

**Syntax 2:** You can compare two separate numeric vectors using this alternative syntax. For example, if the two columns are in different data sets.  

```{r, eval=F}
t.test(df1$age_years, df2$age_years)
```

Conduct a one-sample t-test with the known/hypothesized populaton mean on the right side of the equation:  

```{r, eval=F}
t.test(linelist$age_years, mu = 45)
```

### Shapiro-Wilk's test {-}  

```{r, eval=F}
shapiro.test(linelist$age_years)
```

### Wilcoxon rank sum test {-}

```{r wilcox_base}

## compare age distribution by outcome group with a wilcox test
wilcox.test(age_years ~ outcome, data = linelist)

```

### Kruskal-wallis test {-}


```{r }

## compare age distribution by outcome group with a kruskal-wallis test
kruskal.test(age_years ~ outcome, linelist)

```

### Chi-squared test {-} 

```{r}

## compare the proportions in each group with a chi-squared test
chisq.test(linelist$gender, linelist$outcome)

```





## `gtsummary` package {-}

Use **gtsummary** if you are looking to add the results of a statistical test to a pretty table (described in section above). 
Performing statistical tests of comparison with `tbl_summary` is done by adding the 
`add_p` function to a table and specifying which test to use. It is possible to get p-values corrected for multiple testing by using the
`add_q` function. Run `?tbl_summary` for details.  

### Chi-squared test 

Compare the proportions of a categorical variable in two groups. The default statistical test for 
`add_p()` is to perform a chi-squared test of independence with continuity correction, but if 
any expected call count is below 5 then a Fisher's exact test is used. 

```{r chi_gt}
linelist %>% 
  select(gender, outcome) %>%    # keep variables of interest
  tbl_summary(by = outcome) %>%  # produce summary table and specify grouping variable
  add_p()                        # specify what test to perform
```


### T-tests {-} 

Compare the difference in means for a continuous variable in two groups. 
For example, compare the mean age by patient outcome. 

```{r ttest_gt}

linelist %>% 
  select(age_years, outcome) %>%             # keep variables of interest
  tbl_summary(                               # produce summary table
    statistic = age_years ~ "{mean} ({sd})", # specify what statistics to show
    by = outcome) %>%                        # specify the grouping variable
  add_p(age_years ~ "t.test")                # specify what tests to perform


```

### Wilcoxon rank sum test{-}

Compare the distribution of a continuous variable in two groups. The default 
is to use the Wilcoxon rank sum test and the median (IQR) when comparing two 
groups. However for non-normally distributed data or comparing multiple groups, 
the Kruskal-wallis test is more appropriate. 

```{r wilcox_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (this is default so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "wilcox.test")                     # specify what test to perform (default so could leave brackets empty)


```

### Kruskal-wallis test {-}

Compare the distribution of a continuous variable in two or more groups, 
regardless of whether the data is normally distributed. 

```{r kruskal_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (default, so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "kruskal.test")                    # specify what test to perform


```




## `dplyr` package {-}

Performing statistical tests in `dplyr` alone is very dense, again because it 
does not fit within the tidy-data framework. It requires using `purrr` to create
a list of dataframes for each of the subgroups you want to compare. See the page on [Iteration and loops] to learn about **purrr**.  

An easier alternative may be the `rstatix` package. 

### T-tests {-} 

```{r ttest_dplyr}

linelist %>% 
  ## only keep variables of interest
  select(age, outcome) %>% 
  ## drop those missing outcome 
  filter(!is.na(outcome)) %>% 
  ## specify the grouping variable
  group_by(outcome) %>% 
  ## create a subset of data for each group (as a list)
  nest() %>% 
  ## spread in to wide format
  pivot_wider(names_from = outcome, values_from = data) %>% 
  mutate(
    ## calculate the mean age for the death group
    Death_mean = map(Death, ~mean(.x$age, na.rm = TRUE)),
    ## calculate the sd among dead 
    Death_sd = map(Death, ~sd(.x$age, na.rm = TRUE)),
    ## calculate the mean age for the recover group
    Recover_mean = map(Recover, ~mean(.x$age, na.rm = TRUE)), 
    ## calculate the sd among recovered 
    Recover_sd = map(Recover, ~sd(.x$age, na.rm = TRUE)),
    ## using both grouped data sets compare mean age with a t-test
    ## keep only the p.value
    t_test = map2(Death, Recover, ~t.test(.x$age, .y$age)$p.value)
  ) %>% 
  ## drop datasets 
  select(-Death, -Recover) %>% 
  ## return a dataset with the medians and p.value (drop missing)
  unnest(cols = everything())

```


### Wilcoxon rank sum test {-}

```{r wilcox_dplyr}

linelist %>% 
  ## only keep variables of interest
  select(age, outcome) %>% 
  ## drop those missing outcome 
  filter(!is.na(outcome)) %>% 
  ## specify the grouping variable
  group_by(outcome) %>% 
  ## create a subset of data for each group (as a list)
  nest() %>% 
  ## spread in to wide format
  pivot_wider(names_from = outcome, values_from = data) %>% 
  mutate(
    ## calculate the median age for the death group
    Death_median = map(Death, ~median(.x$age, na.rm = TRUE)),
    ## calculate the sd among dead 
    Death_iqr = map(Death, ~str_c(
      quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE), 
      collapse = ", "
      )),
    ## calculate the median age for the recover group
    Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)), 
    ## calculate the sd among recovered 
    Recover_iqr = map(Recover, ~str_c(
      quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE), 
      collapse = ", "
      )),
    ## using both grouped data sets compare age distribution with a wilcox test
    ## keep only the p.value
    wilcox = map2(Death, Recover, ~wilcox.test(.x$age, .y$age)$p.value)
  ) %>% 
  ## drop datasets 
  select(-Death, -Recover) %>% 
  ## return a dataset with the medians and p.value (drop missing)
  unnest(cols = everything())

```

### Kruskal-wallis test {-}


```{r kruskal_dplyr}

linelist %>% 
  ## only keep variables of interest
  select(age, outcome) %>% 
  ## drop those missing outcome 
  filter(!is.na(outcome)) %>% 
  ## specify the grouping variable
  group_by(outcome) %>% 
  ## create a subset of data for each group (as a list)
  nest() %>% 
  ## spread in to wide format
  pivot_wider(names_from = outcome, values_from = data) %>% 
  mutate(
    ## calculate the median age for the death group
    Death_median = map(Death, ~median(.x$age, na.rm = TRUE)),
    ## calculate the sd among dead 
    Death_iqr = map(Death, ~str_c(
      quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE), 
      collapse = ", "
      )),
    ## calculate the median age for the recover group
    Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)), 
    ## calculate the sd among recovered 
    Recover_iqr = map(Recover, ~str_c(
      quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE), 
      collapse = ", "
      )),
    ## using the original data set compare age distribution with a kruskal test
    ## keep only the p.value
    kruskal = kruskal.test(linelist$age, linelist$outcome)$p.value
  ) %>% 
  ## drop datasets 
  select(-Death, -Recover) %>% 
  ## return a dataset with the medians and p.value (drop missing)
  unnest(cols = everything())

```

### Chi-squared test {-} 


```{r}
linelist %>% 
  ## do everything by gender 
  group_by(outcome) %>% 
  ## count the variable of interest
  count(gender) %>% 
  ## calculate proportion 
  ## note that the denominator here is the sum of each gender
  mutate(percentage = n / sum(n) * 100) %>% 
  pivot_wider(names_from = outcome, values_from = c(n, percentage)) %>% 
  filter(!is.na(gender)) %>% 
  mutate(pval = chisq.test(linelist$gender, linelist$outcome)$p.value)
```


<!-- ======================================================= -->

## Correlations 

Correlation between numeric variables can be investigated using the tidyverse  
`corrr` package. It allows you to compute correlations using Pearson, Kendall
tau or Spearman rho. The package creates a table and also has a function to 
automatically plot the values. 

```{r, warning=F, message=F}

correlation_tab <- linelist %>% 
  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %>%   # keep numeric variables of interest
  correlate()      # create correlation table (using default pearson)

correlation_tab    # print

## remove duplicate entries (the table above is mirrored) 
correlation_tab <- correlation_tab %>% 
  shave()

## view correlation table 
correlation_tab

## plot correlations 
rplot(correlation_tab)
```


<!-- ======================================================= -->

## Resources {  }

Much of the information in this page is adapted from these resources and vignettes online:  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)
[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)
[corrr](https://corrr.tidymodels.org/articles/using-corrr.html)
[sthda correlation](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/stat_tests.Rmd-->

# Univariate and multivariate regression { }

<!-- ======================================================= -->

## Overview {  }

This page demonstrates the use of **base** R regression function such as `glm()` and the **gtsummary** package to 
look at associations between variables (e.g. odds ratios, risk ratios and hazard
ratios). It also uses functions like `tidy()` from the **broom** package to clean-up regression outputs.  

1.  Univariate: two-by-two tables 
2.  Stratified: mantel-haenszel estimates 
3.  Multivariable: variable selection, model selection, final table
4.  Forest plots


<!-- ======================================================= -->

## Preparation {  }


### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse,    # data management + ggplot2 graphics, 
  stringr,      # manipulate text strings 
  purrr,        # loop over objects in a tidy way
  gtsummary,    # summary statistics and tests 
  broom,        # tidy up results from regressions
  lmtest,       # likelihood-ratio tests
  parameters,   # alternative to tidy up results from regressions
  see          # alternative to visualise forest plots
  )
```

### Import data {-}
We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

### Clean data {-}

**Store the explanatory variables**  

```{r}
## define variables of interest 
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")
```


**Convert to 1's and 0's**  

Below we convert the explanatory columns from "yes"/"no" "m"/"f", and "dead"/"alive" to **1 / 0**, to cooperate with the expectations of logistic regression models. To do this efficiently, we define a vector of the column names of our explanatory variables. 

We apply the function `case_when()` to convert specified values to 1's and 0's. This function is applied all the `explanatory_vars`  columns using `across()` (see page on [Grouping data]).  

```{r}
## convert dichotomous variables to 0/1 
linelist <- linelist %>% 
  mutate(
    ## for each of the variables listed and "outcome"
    across(
      all_of(c(explanatory_vars, "outcome")), 
      ## recode male, yes and death to 1; female, no and recover to 0
      ## otherwise set to missing
           ~case_when(
             . %in% c("m", "yes", "Death")   ~ 1,
             . %in% c("f", "no",  "Recover") ~ 0, 
             TRUE                            ~ NA_real_
           ))
  )
```

**Drop rows with missing values**  

To drop rows with missing values, we add the column `age` to the `explanatory_vars` (`age` would have produced an error in the previous `case_when()` operation). Then we pipe the `linelist` to `drop_na()` to remove any rows with missing values in the `outcome` column or any of the `explanatory_vars` columns.  

```{r}
## add in age_category to the explanatory vars 
explanatory_vars <- c(explanatory_vars, "age_cat")

## drop rows with missing information for variables of interest 
linelist <- linelist %>% 
  drop_na(any_of(c("outcome", explanatory_vars)))

```

The number of rows remaining in `linelist` is `r nrow(linelist)`.  


<!-- ======================================================= -->

## Univariate {  }

Just like in the page on [Descriptive tables], your use case will determine which R package you use. We present two options for doing univariate analysis:  

* Use functions available in **base** to quickly print results to the console. Use the **broom** package to tidy up the outputs.  
* Use the **gtsummary** package to model and get publication-ready outputs  



<!-- ======================================================= -->

### **base** R {-}

The function `glm()` from the **stats** package (part of **base** R) is used to fit Generalized Linear Models (GLM).  

`glm()` can be used for univariate and multivariate logistic regression (e.g. to get Odds Ratios). Here are the core parts:  

```{r, eval=F}
# arguments for glm()
glm(formula, family, data, weights, subset, ...)
```

* `formula = ` The model is provided to `glm()` as an equation, with the outcome on the left and explanatory variables on the right of a tilde `~`. In this example we are assessing the association between different age categories and the outcome of death (now coded as 1, see Preparation section).  
* `family = ` This determines the type of model to run. For logistic regression, use `family = "binomial"`, for poisson use `family = "poisson"`. Other examples are in the table below.  
* `data = ` Specify your data frame  


If necessary, you can also specify the link function via the syntax `family = familytype(link = "linkfunction"))`. You can read more in the documentation about other families and optional arguments such as `weights = ` and `subset = ` (`?glm`).  



Family                 | Default link function 
-----------------------|-------------------------------------------  
`"binomial"` | `(link = "logit")`  
`"gaussian"` | `(link = "identity")`  
`"Gamma"` | `(link = "inverse")`  
`"inverse.gaussian"` | `(link = "1/mu^2")`  
`"poisson"` | `(link = "log")`  
`"quasi"` | `(link = "identity", variance = "constant")`  
`"quasibinomial"` | `(link = "logit")`  
`"quasipoisson"` | `(link = "log")`  


When running `glm()` it is most common to save the results as a named R object. Then you can print the results to your console using `summary()` as shown below, or perform other operations on the results (e.g. exponentiate).  


#### Univariate `glm()` {-}

Below is a univariate model of `outcome` by age category. We save the model output as `model` and then print it within `summary()` to the console. Note the Estimates provided are the *log odds*. The baseline level is the first factor level of `age_cat` (0-4).  

```{r}
model <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
summary(model)
```

To alter the baseline level of a given variable, ensure the column is class Factor and set the first level (see page on [Factors]. For example below we take `linelist` column `age_cat` and set "20-29" as the baseline before piping the modified dataset into `glm()`.  

```{r}
linelist %>% 
  mutate(age_cat = fct_relevel(age_cat, "20-29", after = 0)) %>% 
  glm(formula = outcome ~ age_cat, family = "binomial") %>% 
  summary()
```

#### Printing results {-}

For most uses, several modifications must be made to the above outputs. The function `tidy()` from the package **broom** is very convenient for making the model results presentable. Here we demonstrate how to combine model outputs with a table of counts.  

1) Get the *exponentiated* log odds ratio estimates and confidence intervals by passing the model to `tidy()` and setting `exponentiate = TRUE` and `conf.int = TRUE`.  

```{r odds_base_single}

model <- glm(outcome ~ age_cat, family = "binomial", data = linelist) %>% 
  # clean up the outputs of the regression (exponentiate and produce CIs)
  tidy(exponentiate = TRUE, conf.int = TRUE) %>% 
  # round all numeric columns
  mutate(across(is.numeric, round, digits = 2))
```

Below is the outputted tibble `model`:  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(model, rownames = FALSE, options = list(pageLength = nrow(model), scrollX=T), class = 'white-space: nowrap' )
```

2) Combine these model results with a table of counts. Below, we create the counts table by applying **dplyr** functions on the `linelist` (see page on [Grouping data]).  

* Group rows by outcome, and get counts by age category  
* Pivot wider so the column are `age_cat`, `0`, and `1`  
* Remove row for `NA` `age_cat`, if applicable, to align with the model results  

```{r}

counts_table <- linelist %>% 
  ## remove cases with missing outcome or age category
  filter(!is.na(outcome) & !is.na(age_cat)) %>% 
  ## get counts of variable of interest grouped by outcome
  group_by(outcome) %>% 
  ## gets number or rows by unique outcome-age category combinations  
  count(age_cat) %>% 
  ## spread data to wide format (as in cross-tabulation)
  pivot_wider(names_from = outcome, values_from = n) 
```

Here is what this `counts_table` data frame looks like:  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(counts_table, rownames = FALSE, options = list(pageLength = nrow(counts_table), scrollX=T), class = 'white-space: nowrap' )
```

Now we can bind the `counts_table` and the `model` results together horizontally with `bind_cols()` (**dplyr**). In this code, the `.` represents the piped object `counts_table` and we bind it to `model`. To finish the process, we use `select()` to pick the desired columns and their order, and apply the **base** R `round()` function to all numeric columns specifying 2 decimal places.  

```{r, message=F, warning=F}
combined <- counts_table %>% 
  ## merge with the outputs of the regression 
  bind_cols(., model) %>% 
  ## only keep columns interested in 
  select(term, 2:3, estimate, conf.low, conf.high, p.value) %>% 
  ## round values to 2 decimal places
  mutate(across(is.numeric, round, digits = 2))
```

Here is what the combined data frame looks like:  


```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(combined, rownames = FALSE, options = list(pageLength = nrow(combined), scrollX=T), class = 'white-space: nowrap' )
```


#### Looping multiple univariate models {-}  

Below we present a method using `glm()` and `tidy()` for a more simple approach, see the section on **gtsummary**.  

To run the models on several exposure variables to produce univariate odds ratios (i.e. 
not controlling for each other), you can use the approach below. It uses `str_c()` from **stringr** to create univariate formulas,, runs the `glm()` regression on each formula, passes each `glm()` output to `tidy()` and finally collapses all the model outputs together with `bind_rows()` from **tidyr***. This approach uses `map()` from the package **purrr** to iterate - see the page on [Iteration and loops] for more information on this tool.  

1) Create a vector of column names of the explanatory variables. We already have this as `explanatory_vars` from the data Preparation section of this page.  

2) Use `str_c()` to create multiple string formulas, with `outcome` on the left, and a column name from `explanatory_vars` on the right. The period `.` substitutes for the column name in `explanatory_vars`.  

```{r}
explanatory_vars %>% str_c("outcome ~ ", .)
```

3) Pass these string formulas to `map()` and set `~glm()` as the function to apply to each input. Within `glm()`, set the regression formula as `as.formula(.x)` where `.x` will be replaced by the string formula defined in the step above. `map()` will loop over each of the string formulas, running regressions for each one.  

4) The outputs of this first `map()` are passed to a second `map()` command, which applied `tidy()` to the regression outputs.  

5) Finally the output of the second `map()` (a list of tidied data frames) is condensed with `bind_rows()`, resulting in one data frame with all the univariate results.  


```{r odds_base_multiple}

models <- explanatory_vars %>%       # begin with variables of interest
  str_c("outcome ~ ", .) %>%         # combine each variable into formula ("outcome ~ variable of interest")
  
  # iterate through each univariate formula
  map(                               
    .f = ~glm(                       # pass the formulas one-by-one to glm()
      formula = as.formula(.x),      # within glm(), the string formula is .x
      family = "binomial",           # specify type of glm (logistic)
      data = linelist)) %>%          # dataset
  
  # tidy up each of the glm regression outputs from above
  map(
    .f = ~tidy(
      .x, 
      exponentiate = TRUE,           # exponentiate 
      conf.int = TRUE)) %>%          # return confidence intervals
  
  # collapse the list of regression outputs in to one data frame
  bind_rows() %>% 
  
  # round all numeric columns
  mutate(across(is.numeric, round, digits = 2))
```

This time, the end object `models` is longer because it now represents the combined results of several univariate regressions. Click through to see all the rows of `model`.  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(models, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

As before, we can create a counts table from the `linelist` for each explanatory variable, bind it to `models`, and make a nice table. We begin with the variables, and iterate through them with `map()`. We iterate through a user-defined function which involves creating a counts table with **dplyr** functions. Then the results are combined and bound with the `models` model results.  


```{r, warning=F, message=F}

## for each explanatory variable
univ_tab_base <- explanatory_vars %>% 
  map(.f = 
    ~{linelist %>%                ## begin with linelist
        group_by(outcome) %>%     ## group data set by outcome
        count(.data[[.x]]) %>%    ## produce counts for variable of interest
        pivot_wider(              ## spread to wide format (as in cross-tabulation)
          names_from = outcome,
          values_from = n) %>% 
        filter(!is.na(.data[[.x]])) %>%  ## drop rows with missings
        rename("variable" = .x) %>%      ## change variable of interest column to "variable"
        mutate(variable = as.character(variable))} ## convert to character, else non-dichotomous (categorical) variables come out as factor and cant be merged
      ) %>% 
  
  ## collapse the list of count outputs in to one data frame
  bind_rows() %>% 
  
  ## merge with the outputs of the regression 
  bind_cols(., models) %>% 
  
  ## only keep columns interested in 
  select(term, 2:3, estimate, conf.low, conf.high, p.value) %>% 
  
  ## round decimal places
  mutate(across(is.numeric, round, digits = 2))

```

Below is what the data frame looks like. See the page on [HTML tables] for ideas on how to further convert this table to pretty HTML output (e.g. with **flextable**).  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(univ_tab_base, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### **gtsummary** package {-}

Below we present the use of `tbl_uvregression()` from the **gtsummary** package. Just like in the page on [Descriptive tables], **gtsummary** functions do a good job of running statistics *and* producing professional-looking outputs. This function produces a table of univariate regression results.  

We select only the necessary columns from the `linelist` (explanatory variables and the outcome variable) and  pipe them into `tbl_uvregression()`. We are going to run univariate regression on each of the columns we defined as `explanatory_vars` in the data Preparation section (gender, fever, chills, cough, aches, vomit, and age_cat).  

Within the function itself, we provide the `method = ` as `glm` (no quotes), the `y = ` outcome column (`outcome`), specify to `method.args = ` that we want to run logistic regression via `family = binomial`, and we tell it to exponentiate the results.  

The output is HTML and contains the counts

```{r odds_gt, message=F, warning=F}

univ_tab <- linelist %>% 
  dplyr::select(explanatory_vars, outcome) %>% ## select variables of interest

  tbl_uvregression(                         ## produce univariate table
    method = glm,                           ## define regression want to run (generalised linear model)
    y = outcome,                            ## define outcome variable
    method.args = list(family = binomial),  ## define what type of glm want to run (logistic)
    exponentiate = TRUE                     ## exponentiate to produce odds ratios (rather than log odds)
  )

## view univariate results table 
univ_tab
```

There are many modifications you can make to this table output, such as adjusting the text labels, bolding rows by their p-value, etc. See tutorials [here](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html) and elsewhere online.  



<!-- ======================================================= -->

## Stratified {  }

Stratified analysis is currently still being worked on for **gtsummary**, 
this page will be updated in due course. 


<!-- ======================================================= -->

### **gtsummary** package {-}

TODO

<!-- ======================================================= -->

### **base** R {-}

TODO

<!-- ======================================================= -->



## Multivariate  

For multivariate analysis, we again present two approaches:  

* `glm()` and `tidy()`  
* **gtsummary** package  

The workflow is similar for each, as shown below, and only the last step of pulling a final table together is different.


### Conduct multivariate {-}  

Use `glm()` but add more variables to the right side of the equation, separated by plus symbols (`+`). 


To run the model with all of our explanatory variables we would run:  

```{r}
mv_reg <- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = "binomial", data = linelist)

summary(mv_reg)
```

If you want to include two variables and an interaction between them you can separate them with an asterisk `*` instead of a `+`. Separate them with a colon `:` if you are only specifying the interaction. For example:  

```{r, eval=F}
glm(outcome ~ gender + age_cat * fever, family = "binomial", data = linelist)
```


*Optionally*, you can leverage the pre-defined vector of column names and re-create the above command using `str_c()` as shown below. This might be useful if your explanatory variable names are changing, or you don't want to type them all out again.  

```{r mv_regression}

## run a regression with all variables of interest 
mv_reg <- explanatory_vars %>%  ## begin with vector of explanatory column names
  str_c(collapse = "+") %>%     ## combine all names of the variables of interest separated by a plus
  str_c("outcome ~ ", .) %>%    ## combine the names of variables of interest with outcome in formula style
  glm(family = "binomial",      ## define type of glm as logistic,
      data = linelist)          ## define your dataset
```


#### Building the model {-}  

You can build you model step-by-step, saving various models that include certain explanatory variables. You can compare these models with likelihood-ratio tests using `lrtest()` from the package **lmtest**, as below:  

```{r}
model1 <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
model2 <- glm(outcome ~ age_cat + gender, family = "binomial", data = linelist)

lmtest::lrtest(model1, model2)
```

Another option is to take the model object and apply the `step()` function from the **stats** package. Specify which variable selection direction you want use when building the model.      

```{r}
## choose a model using forward selection based on AIC
## you can also do "backward" or "both" by adjusting the direction
final_mv_reg <- mv_reg %>%
  step(direction = "forward", trace = FALSE)
```


You can also turn off scientific notation in your R session, for clarity:  

```{r}
options(scipen=999)
```

As described in the section on univariate analysis, pass the model output to `tidy()` to exponentiate the log odds and CIs. Finally we round all numeric columns to two decimal places. Scroll through to see all the rows.  

```{r mv_regression_base}

mv_tab_base <- final_mv_reg %>% 
  ## get a tidy dataframe of estimates 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %>% 
  ## round 
  mutate(across(is.numeric, round, digits = 2))
```

Here is what the resulting data frame looks like: 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(mv_tab_base, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### Combine univariate and multivariate {-}

#### Combine with **gtsummary**  {-}  

The `gtsummary` package provides the `tbl_regression` function, which will 
take the outputs from a regression (`glm()` in this case) and produce an easy 
summary table. You can also combine several different output tables produced by `gtsummary` with 
the `tbl_merge` function. 

```{r mv_regression_gt}
## show results table of final regression 
mv_tab <- tbl_regression(final_mv_reg, exponentiate = TRUE)
```

And now combine the univariate and multivariate results:  

```{r}
## combine with univariate results 
tbl_merge(
  tbls = list(univ_tab, mv_tab), 
  tab_spanner = c("**Univariate**", "**Multivariable**")) # set header names
```



#### Combine with **dplyr** {-}  

An alternative way of combining the `glm()`/`tidy()` univariate and multivariate outputs is with the **dplyr** join functions.  

* Join the univariate results from earlier (which contains counts) with the tidied multivariate results  
* Use `select()` to keep only the columns we want, specify their order, and re-name them  
* Use `round()` with two decimal places on all the column that are class Double  

```{r, warning=F, message=F}
## combine univariate and multivariable tables 
left_join(univ_tab_base, mv_tab_base, by = "term") %>% 
  ## choose columns and rename them
  select( # new name =  old name
    "characteristic" = term, 
    "recovered"      = "0", 
    "dead"           = "1", 
    "univ_or"        = estimate.x, 
    "univ_ci_low"    = conf.low.x, 
    "univ_ci_high"   = conf.high.x,
    "univ_pval"      = p.value.x, 
    "mv_or"          = estimate.y, 
    "mvv_ci_low"     = conf.low.y, 
    "mv_ci_high"     = conf.high.y,
    "mv_pval"        = p.value.y 
  ) %>% 
  mutate(across(is.double, round, 2))

```




<!-- ======================================================= -->

## Forest plot {  }

This section shows how to produce a plot with the outputs of your regression.
There are two options, you can build a plot yourself using `ggplot2` or use a 
meta-package called **easystats** (package that includes many packages).  


<!-- ======================================================= -->

### **ggplot2** package {-}

You can build a forest plot with `ggplot()` by plotting elements of the multivariate regression results. Add the layers:  

* estimates with `geom_point()`  
* confidence intervals with `geom_errorbar()`  
* a vertical line at OR = 1 with `geom_vline()`  

You may want to re-arrange the order of the variables/levels on the y-axis (see how the order of age_cat levels is alphabetical and not sensical). To do this, use `fct_relevel()` from the **forcats** package to classify the column `term` as a factor and specify the order manually. See the page on [Factors] for more details.  

```{r ggplot_forest}

## remove the intercept term from your multivariable results
mv_tab_base %>% 
  filter(term != "(Intercept)") %>% 
  ## plot with variable on the y axis and estimate (OR) on the x axis
  ggplot(aes(x = estimate, y = term)) +
  ## show the estimate as a point
  geom_point() + 
  ## add in an error bar for the confidence intervals
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  ## show where OR = 1 is for reference as a dashed line
  geom_vline(xintercept = 1, linetype = "dashed")
  
```


<!-- ======================================================= -->

### **easystats** packages {-}

The alternative if you do not want to decide all of the different things required
for a `ggplot`, is to use a combination of **easystats** packages. 
In this case the **parameters** package function `model_parameters()` does the equivalent
of **broom** package function `tidy()`. The **see** package then accepts those outputs
and creates a default forest plot as a `ggplot()` object. 

```{r easystats_forest}

## remove the intercept term from your multivariable results
final_mv_reg %>% 
  model_parameters(exponentiate = TRUE) %>% 
  plot()
  
```


<!-- ======================================================= -->

## Resources {  }

Much of the information in this page is adapted from these resources and vignettes online:  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)  

[sthda stepwise regression](http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/)   

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/regression.Rmd-->


# Missing data { }

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "missingness.png"))
knitr::include_graphics(here::here("images", "missingness_overview.png"))
```

This page will cover:  

1) Useful functions for assessing missingness  
2) Assess missingness in a dataframe  
3) How to filter out rows by missingness  
3) Plotting missingness over time  
4) Handling how `NA` is displayed in plots  
5) Missing value imputation, MCAR, MAR, MNAR  



<!-- ======================================================= -->
## Preparation { }

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # import/export
  tidyverse,     # data mgmt and viz
  naniar         # assess and visualize missingness
)
```


### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


When importing your data, be aware of values that should be classified as missing. For example, 99, 999, "Missing", blank cells (""), or cells with an empty space (" "). You can convert these to `NA` (R's version of missing data) during the data import. See the page on [Import and export] for details.  


<!-- ======================================================= -->
## `NA` { }

**`NA`**  

In R, missing values are represented by a reserved (special) value - `NA`. Note that this is typed *without* quotes. "NA" is different and is just a normal character value (also a Beatles lyric from the song Hey Jude).  

Your data may have other ways of representing missingness, such as "99", or "Missing", or "Unknown" - you may even have empty character value "" which looks "blank", or a single space " ". Be aware of these and consider whether to convert them to `NA` during importation or data cleaning (e.g. with `na_if()`). You may also want to convert the other way - changing all `NA` to "Missing" or similar (e.g. with `replace_na()` or `fct_explicit_na()`)  

**Versions of `NA`**  

Most of the time, all you need to know/use is `NA` and `is.na()`. However sometimes you may encounter the need for variations on `NA` listed below. One example is when creating a new column with `case_when()` and deciding to assign `NA` as the outcome for some logical criteria (see [Cleaning data and core functions] page for tips on `case_when()`).  

There may be circumstances where `NA` on the right-hand side (RHS) of `case_when()` is rejected because the other RHS values are a class such as Character. The RHS values in a `case_when()` command must all be of the same class. Thus, if you have character outcomes on the RHS like "Confirmed", "Suspect", "Probable" and then `NA` - you will get an error. Instead of `NA`, put "Missing", or else you must put `NA_character_`. Likewise for integers, use `NA_integer_` instead of `NA`. `NA` should work for dates and logical. See the [R documentation on NA](https://stat.ethz.ch/R-manual/R-devel/library/base/html/NA.html) for more information.  

* `NA`  
* `NA_character_`  
* `NA_integer_`  
* `NA_real_`  
* `NA_complex_`  


**`na.rm = TRUE`**  

When you apply mathematical functions such as `max()`, `min()`, `sum()` or `mean()`, any `NA` value present over-rides the calculation and `NA` is returned. You must specify the arguemnt `na.rm = TRUE` within the function to remove any `NA` values from the calculation. This default behavior is intentional, so that you are alerted if any of your data are missing.  



**`NULL`**  

`NULL` is another reserved value in R. It is the logical representation of a statement that is neither true nor false. It is returned by expressions or functions whose values are undefined. Generally do not assign NULL as a value, unless writing functions or perhaps writing Shiny to return `NULL` in specific scenarios. Null-ness can be assessed using `is.null()` and conversion can made with `as.null()`.  

See this [blog post](https://www.r-bloggers.com/2010/04/r-na-vs-null/) on the difference between `NULL` and `NA`.  

**`NaN`**  

Impossible values are represented by the special value - `NaN`. An example of this is when you force R to divide 0 by 0. It has an infinite value. You can assess this with `is.nan()`. You may also encounter complementary functions include `is.infinite()` and `is.finite()`


**`Inf`**  

Say you have a column `z` that contains these values: `z <- c(1, 22, NA, Inf, NaN, 5)`  

If you want to use `max()` on the column, you can use `na.rm = TRUE` as described above to remove the `NA` from the calculation, but the `Inf` and `NaN` remain and `Inf` will be returned. You can use brackets [ ] to subset the vector such that only finite values are used for the calculation: `max(z[is.finite(z)])`.  

```{r, eval=F}
z <- c(1, 22, NA, Inf, NaN, 5)
max(z)                           # returns NA
max(z, na.rm=T)                  # returns Inf
max(z[is.finite(z)])             # returns 22
```


**Examples**  

R command | Outcome
----------|--------------
`5 / 0` | `Inf`  
`0 / 0` | `NaN`  
`5 / NA` | `NA`  
`5 / Inf | `0`  
`NA - 5` | `NA`  
`Inf / 5` | `Inf`  
`class(NA)` | "logical"  
`class(NaN)` | "numeric"  
`class(Inf)` | "numeric"  
`class(NULL)` | "NULL"  

"NAs introduced by coercion" is a common warning message. This can happen if you attempt to make an illegal conversion like inserting a character value into a vector that is otherwise numeric.  

```{r}
as.numeric(c("10", "20", "thirty", "40"))
```

`NULL` is ignored in a vector.  

```{r}
my_vector <- c(25, NA, 10, NULL)  # define
my_vector                         # print
```


Variance of one number results in `NA`.  

```{r}
var(22)
```


<!-- ======================================================= -->
## Useful functions { }

The following are useful **base** R functions when assessing or handling missing values:  


**`is.na()` and `!is.na()`**  

Use `is.na()`to identify missing values, or use its opposite (with `!` in front) to identify non-missing values. These both return a logical value (`TRUE` or `FALSE`). Remember that you can `sum()` the resulting vector to count the number `TRUE`, e.g. `sum(is.na(linelist$date_outcome))`.    

```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)
is.na(my_vector)
!is.na(my_vector)
sum(is.na(my_vector))
```


**`na.omit()`**  

This function, if applied to a dataframe, will remove rows with *any* missing values. It is also from **base** R.  
If applied to a vector, it will remove `NA` values from the vector it is applied to. For example:  

```{r}
sum(na.omit(my_vector))
```


**`na.rm = TRUE`**  

In R, most mathematical functions will by default *include* `NA` in calculations, *which results in the function returning `NA`*. This is designed intentionally, in order to make you aware that you have missing data.  

You can avoid this by removing missing values from the calculation, by including the argument `na.rm = TRUE` (na.rm stands for "remove `NA`").  

```{r}
mean(my_vector)

mean(my_vector, na.rm = TRUE)
```



<!-- ======================================================= -->
## Assess missingness in a dataframe { }

You can use the package **naniar** to assess and visualize missingness in the dataframe `linelist`.  

```{r}
# install and/or load package
pacman::p_load(naniar)
```

### Quantifying missingness {-}

To find the percent of all values that are missing use `pct_miss()`. Use `n_miss()` to get the number of missing values.  

```{r}
pct_miss(linelist)
```

The two functions below return the percent of rows with any missing value, or that are entirely complete, respectively. Remember that `NA` means missing, and that ``""` or `" "` will register as non-missing.  

```{r}
pct_miss_case(linelist)   # also see n_complete() for counts

pct_complete_case(linelist) # see n_complete
```



### Visualizing missingness {-}  

The `gg_miss_var()` function will show you the number of missing values in each column. You can add a bare column name to the argument `facet = ` if desired to see the plot by groups. By default, counts are shown instead of percents (`show_pct = FALSE`). You can also add labs as a normal ggplot with `+labs()`.  


```{r}
gg_miss_var(linelist, show_pct = TRUE)
```

You can use `vis_miss()` to visualize the dataframe as a heatmap, showing whether each value is missing or not. As usual, you can also `select()` certain columns from the dataframe and the provide that to the function.    

```{r}
vis_miss(linelist)
```


### Explore and visualize missingness relationships {-} 

How do you visualize something that is not there??? By default, ggplot removes points with missing values from plots.  

**naniar** offers a solution via `geom_miss_point()`. When creating a scatterplot of two columns, records with one of the values missing and the other present are shown by setting the missing values to 10% lower than the lowest value in the column, and coloring them distinctly.  

In the scatterplot below, the red dots are records where the value for one column is present but the value for the other column is missing.  


```{r}
ggplot(
  linelist, 
  aes(x = age_years,             
      y = temp)) +     # column to show missingness
  geom_miss_point()
```

To assess missingness in the dataframe *stratified by another column*, consider `gg_miss_fct()`, which returns a heatmap of percent missingness in the dataframe by a factor/categorical (or date) column:  

```{r}
gg_miss_fct(linelist, age_cat5)
```


This function can also be used with a date column to see missingness over time:  

```{r}
gg_miss_fct(linelist, date_onset)
```




### "Shadow" columns {-}

Another way to visualize missingness in one column by values in a second column is using the "shadow" that **naniar** can create. `bind_shadow()` creates a binary `NA`/not `NA` column for every existing column, and binds all these new columns to the original dataset with the appendix "_NA". This doubles the number of columns - see below:  


```{r}
shadowed_linelist <- linelist %>% 
  bind_shadow()

names(shadowed_linelist)
```

These "shadow" columns can be used to plot the density of proportion of values that are missing, by any another column X. For example, the plot below shows the proportion of records missing `days_onset_hosp` (number of days from symptom onset to hospitalisation), by that record's value in `date_hospitalisation`. Essentially, you are plotting the density of the x-axis column, but stratifying the results (`color = `) by a shadow column of interest. This analysis works best if the x-axis is a numeric or date column.  


```{r, message = F}
ggplot(
  shadowed_linelist,                   # dataframe with shadow columns
  aes(x = date_hospitalisation,        # numeric or date column
      colour = age_years_NA)) +        # shadow column of interest
  geom_density()                       # plots the density curves
```

You can also use these "shadow" columns to stratify a statistical summary, as shown below:

```{r}
linelist %>%
  bind_shadow() %>%                # create the shows cols
  group_by(date_outcome_NA) %>%    # shadow col for stratifying
  summarise_at(.vars = c("age_years"),                        # variable of interest for calculations
               .funs = c("mean", "sd", "var", "min", "max"),  # stats to calculate
               na.rm = TRUE)       # other arguments for the stat calculations
```


An alternative way to plot the proportion of values in one column, including missingness, is given below. It does *not* involve **naniar**. This example shows percent of weekly observations that are missing in a column):  

1) Aggregate the data into a useful time unit (days, weeks, etc.), summarizing the proportion of observations with `NA` (and any other values of interest)  
2) Plot the proportion missing as a line using `ggplot()`  

Below, we take the linelist, add a new column for week, group the data by week, and then calculate the percent of that week's records where the value is missing. (note: if you want % of 7 days the calculation would be slightly different).  

```{r}
outcome_missing <- linelist %>%
  mutate(week = lubridate::floor_date(date_onset, "week")) %>%   # create new week column
  group_by(week) %>%                                             # group the rows by week
  summarize(                                                     # summarize each week
    n_obs = n(),                                                     # number of records
    
    outcome_missing = sum(is.na(outcome) | outcome == ""),       # number of records missing the value
    outcome_p_miss  = outcome_missing / n_obs,                   # proportion of records missing the value
  
    outcome_dead    = sum(outcome == "Death", na.rm=T),          # number of records as dead
    outcome_p_dead  = outcome_dead / n_obs) %>%                  # proportion of records as dead
  
  tidyr::pivot_longer(-week, names_to = "statistic") %>%         # pivot all columns except week, to long format for ggplot
  filter(stringr::str_detect(statistic, "_p_"))                  # keep only the proportion values
```

Then we plot the proportion missing as a line, by week

```{r, message=F, warning=F}
ggplot(data = outcome_missing)+
    geom_line(
      aes(x = week, y = value, group = statistic, color = statistic),
      size = 2,
      stat = "identity")+
    labs(title = "Weekly outcomes",
         x = "Week",
         y = "Proportion of weekly records") + 
     scale_color_discrete(
       name = "",
       labels = c("Died", "Missing outcome"))+
    scale_y_continuous(breaks = c(seq(0,1,0.1)))+
  theme_minimal()+
  theme(
    legend.position = "bottom"
  )
```





<!-- ======================================================= -->
## Using data with missing values  


### Filter out rows with missing values {-}

To quickly remove rows with missing values, use the **dplyr** function `drop_na()`.  

The original `linelist` has `r `nrow(linelist)` rows. The adjusted number of rows is shown below:  

```{r}
linelist %>% 
  drop_na() %>%     # remove rows with ANY missing values
  nrow()
```

You can specify to drop rows with missingness in certain columns:  

```{r}
linelist %>% 
  drop_na(date_onset) %>% # remove rows missing date_onset 
  nrow()
```

Multiple columns can be specified one after the other, or using this standard syntax:  

```{r}
linelist %>% 
  drop_na(contains("date")) %>% # remove rows missing values in any "date" column 
  nrow()
```



<!-- ======================================================= -->
### Handling `NA` in `ggplot()` {-}

It is often wise to report the number of values excluded from a plot in a caption. Below is an example:  

In `ggplot()`, you can add `labs()` and within it a `caption = `. In the caption, you can use `str_glue()` from **stringr** package to paste values together into a sentence dynamically so they will adjust to the data. An example is below:  

* Note the use of `\n` for a new line.  
* Note that if multiple column would contribute to values not being plotted (e.g. age or sex if those are reflected in the plot), then you must filter on those columns as well to correctly calculate the number not shown.  

```{r, eval=F}
labs(
  title = "",
  y = "",
  x = "",
  caption  = stringr::str_glue(
  "n = {nrow(central_data)} from Central Hospital;
  {nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown."))  
```

Sometimes, it can be easier to save the string as an object in commands prior to the `ggplot()` command, and simply reference the named string object within the `str_glue()`.  


<!-- ======================================================= -->
### `NA` in factors {-}

If your column of interest is a factor, use `fct_explicit_na()` from the **forcats** package to convert `NA` values to a character value. By default the character value is "(Missing)" but this can be adjusted via the `na_level =` arguement.   

```{r}
pacman::p_load(forcats)   # load package

linelist <- linelist %>% 
  mutate(gender = fct_explicit_na(gender, na_level = "Missing"))

levels(linelist$gender)
```



<!-- ======================================================= -->
## Imputation { }


Sometimes, when analyzing your data, it will be important to "fill in the gaps" and impute missing data While you can always simply analyze a dataset after removing all missing values, this can cause problems in many ways. Here are two examples: 

1) By removing all observations with missing values or variables with a large amount of missing data, you might reduce your power or ability to do some types of analysis. For example, as we discovered earlier, only a small fraction of the observations in our linelist dataset have no missing data across all of our variables. If we removed the majority of our dataset we'd be losing a lot of information! And, most of our variables have some amount of missing data--for most analysis it's probably not reasonable to drop every variable that has a lot of missing data either.

2) Depending on why your data is missing, analysis of only non-missing data might lead to biased or misleading results. For example, as we learned earlier we are missing data for some patients about whether they've had some important symptoms like fever or cough. But, as one possibility, maybe that information wasn't recorded for people that just obviously weren't very sick. In that case, if we just removed these observations we'd be excluding some of the healthiest people in our dataset and that might really bias any results.

It's important to think about why your data might be missing in addition to seeing how much is missing. Doing this can help you decide how important it might be to impute missing data, and also which method of imputing missing data might be best in your situation.

### Types of missing data {-}

Here are three general types of missing data:

1) **Missing Completely at Random** (MCAR). This means that there is no relationship between the probability of data being missing and any of the other variables in your data. The probability of being missing is the same for all cases This is a rare situation. But, if you have strong reason to believe your data is MCAR analyzing only non-missing data without imputing won't bias your results (although you may lose some power). [TODO: consider discussing statistical tests for MCAR]

2) **Missing at Random** (MAR). This name is actually a bit misleading as MAR means that your data is missing in a systematic, predictable way based on the other information you have. For example, maybe every observation in our dataset with a missing value for fever was actually not recorded because every patient with chills and and aches was just assumed to have a fever so their temperature was never taken. If true, we could easily predict that every missing observation with chills and aches has a fever as well and use this information to impute our missing data. In practice, this is more of a spectrum. Maybe if a patient had both chills and aches they were more likely to have a fever as well if they didn't have their temperature taken, but not always. This is still predictable even if it isn't perfectly predictable. This is a common type of missing data 

3) **Missing not at Random** (MNAR). Sometimes, this is also called **Not Missing at Random** (NMAR). This assumes that the probability of a value being missing is NOT systematic or predictable using the other information we have but also isn't missing randomly. In this situation data is missing for unknown reasons or for reasons you don't have any information about. For example, in our dataset maybe information on age is missing because some very elderly patients either don't know or refuse to say how old they are. In this situation, missing data on age is related to the value itself (and thus isn't random) and isn't predictable based on the other information we have. MNAR is complex and often the best way of dealing with this is to try to collect more data or information about why the data is missing rather than attempt to impute it. 

In general, imputing MCAR data is often fairly simple, while MNAR is very challenging if not impossible. Many of the common data imputation methods assume MAR. 

### Useful packages {-}

Some useful packages for imputing missing data are Mmisc, missForest (which uses random forests to impute missing data), and mice (Multivariate Imputation by Chained Equations). For this section we'll just use the mice package, which implements a variety of techniques. The maintainer of the mice package has published an online book about imputing missing data that goes into more detail here (https://stefvanbuuren.name/fimd/).  

Here is the code to load the mice package:

```{r}
pacman::p_load(mice)
```

### Mean Imputation {-}

Sometimes if you are doing a simple analysis or you have strong reason to think you can assume MCAR, you can simply set missing numerical values to the mean of that variable. Perhaps we can assume that missing temperature measurements in our dataset were either MCAR or were just normal values. Here is the code to create a new variable that replaces missing temperature values with the mean temperature value in our dataset. However, in many situations replacing data with the mean can lead to bias, so be careful.

```{r}
linelist <- linelist %>%
  mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))
```

You could also do a similar process for replacing categorical data with a specific value. For our dataset, imagine you knew that all observations with a missing value for their outcome (which can be "Death" or "Recover") were actually people that died (note: this is not actually true for this dataset):

```{r}
linelist <- linelist %>%
  mutate(outcome_replace_na_with_death = replace_na(outcome, "Death"))
```

### Regression imputation {-}

A somewhat more advanced method is to use some sort of statistical model to predict what a missing value is likely to be and replace it with the predicted value. Here is an example of creating predicted values for all the observations where temperature is missing, but age and fever are not, using simple linear regression using fever status and age in years as predictors. In practice you'd want to use a better model than this sort of simple approach.

```{r, warning=F, message=F}
simple_temperature_model_fit <- lm(temp ~ fever + age_years, data = linelist)

#using our simple temperature model to predict values just for the observations where temp is missing
predictions_for_missing_temps <- predict(simple_temperature_model_fit,
                                        newdata = linelist %>% filter(is.na(temp))) 
```

Or, using the same modeling approach through the mice package to create imputed values for the missing temperature observations:

```{r}
model_dataset <- linelist %>%
  select(temp, fever, age_years)  

temp_imputed <- mice(model_dataset,
                            method = "norm.predict",
                            seed = 1,
                            m = 1,
                            print = F)

temp_imputed_values <- temp_imputed$imp$temp

```


This is the same type of approach by some more advanced methods like using the missForest package to replace missing data with predicted values. In that case, the prediction model is a random forest instead of a linear regression. You can use other types of models to do this as well. However, while this approach works well under MCAR you should be a bit careful if you believe MAR or MNAR more accurately describes your situation. The quality of your imputation will depend on how good your prediction model is and even with a very good model the variability of your imputed data may be underestimated. 

### LOCF and BOCF {-}

Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) are  imputation methods for time series/longitudinal data. The idea is to take the previous observed value as a replacement for the missing data. When multiple values are missing in succession, the method searches for the last observed value.

[TO BE COMPLETED]

### Multiple Imputation {-}

The online book we mentioned earlier by the author of the mice package (https://stefvanbuuren.name/fimd/) contains a detailed explanation of multiple imputation and why you'd want to use it. But, here is a basic explanation of the method:

When you do multiple imputation, you create multiple datasets with the missing values imputed to plausible data values (depending on your research data you might want to create more or less of these imputed datasets, but the mice package sets the default number to 5). The difference is that rather than a single, specific value each imputed value is drawn from an estimated distribution (so it includes some randomness). As a result, each of these datasets will have slightly different different imputed values (however, the non-missing data will be the same in each of these imputed datasets). You still use some sort of predictive model to do the imputation in each of these new datasets (mice has many options for prediction methods including *Predictive Mean Matching*, *logistic regression*, and *random forest*) but the mice package can take care of many of the modeling details. 

Then, once you have created these new imputed datasets, you can apply then apply whatever statistical model or analysis you were planning to do for each of these new imputed datasets and pool the results of these models together. This works very well to reduce bias in both MCAR and many MAR settings and often results in more accurate standard error estimates.

Here is an example of applying the Multiple Imputation process to predict temperature in our linelist dataset using a age and fever status (our simplified model_dataset from above):
[Note from Daniel: this is not a very good model example and I'll change it later]

```{r}
# imputing missing values for all variables in our model_dataset, and creating 10 new imputed datasets
multiple_imputation = mice(
  model_dataset,
  seed = 1,
  m = 10,
  print = FALSE) 

model_fit <- with(multiple_imputation, lm(temp ~ age_years + fever))

base::summary(mice::pool(model_fit))
```

Here we used the mice default method of imputation, which is Predictive Mean Matching. We then used these imputed datasets to separately estimate and then pool results from simple linear regressions on each of these datasets. There are many details we've glossed over and many settings you can adjust during the Multiple Imputation process while using the mice package. For example, you won't always have numerical data and might need to use other imputation methods (you can still use the mice package for many other types of data and methods). But, for a more robust analysis when missing data is a significant concern, Multiple Imputation is good solution that isn't always much more work than doing a complete case analysis. 





<!-- ======================================================= -->
## Resources { }

Vignette on the [naniar package](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)

Gallery of [missing value visualizations](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/missing_data.Rmd-->


# Standardization { }  

This page will show you two ways to standardize an outcome, such as hospitalizations or mortality, by characteristics such as age and sex. 

* Using **dsr** package 
* Using **PHEindicatormethods** package  

We begin by extensively demonstrating the processes of data preparation/cleaning/joining, as this is common when combining population data from multiple countries, standard population data, deaths, etc.  

<!-- ======================================================= -->
## Overview {  }

There are two main ways to standardize: direct and indirect standardization.
Let's say we would like to standardize mortality by age and sex for country A and country B, and compare the standardized rates between these countries.

* For direct standardization, you will have to know the number of the at-risk population and the number of deaths for each stratum of age and sex, for country A and country B. One stratum in our example could be females between ages 15-44.  
* For indirect standardization, you only need to know the total number of deaths and the age- and sex structure of each country. This option is therefore feasible if age- and sex-specific mortality rates or population numbers are not available. Indirect standardization is furthermore preferable in case of small numbers per stratum, as estimates in direct standardization would be influenced by substantial sampling variation. 

<!-- ======================================================= -->
## Preparation {  }

To show how standardization is done, we will use fictitious population counts and death counts from  country A and country B, by age (in 5 year categories) and sex (female, male). To make the datasets ready for use, we will perform the following preparation steps:  

1. Load packages  
2. Load datasets  
3. Join the populaton and death data from the two countries
4. Pivot longer so there is one row per age-sex stratum
5. Clean the reference population (world standard population) and join it to the country data  

In your scenario, your data may come in a different format. Perhaps your data are by province, city, or other catchment area. You may have one row for each death and information on age and sex for each (or a significant proportion) of these deaths. In this case, see the pages on [Grouping data] and [Pivoting data] to create a dataset with event and population counts per age-sex stratum.  

We also need a reference population, the standard population. There are several standard populations available, for the purpose of this exercise we will use the `world_standard_population_by_sex`. The World standard population is based on the populations of 46 countries and was developed in 1960. There are many "standard" populations - as one example, the website of [NHS Scotland](https://www.opendata.nhs.scot/dataset/standard-populations) is quite informative on the European Standard Population, World Standard Population and Scotland Standard Population. 

<!-- ======================================================= -->
### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
     rio,         # to import data
     here,        # to locate files
     tidyverse,   # to clean, handle, and plot the data (includes ggplot2 package)
     stringr,     # cleaning characters and strings
     frailtypack, # needed for dsr, for frailty models
     dsr,  
     PHEindicatormethods)
```


<span style="color: orange;">**_CAUTION:_** If you have a newer version of R, the **dsr** package cannot be directly downloaded from CRAN. However, it is still available from the CRAN archive. You can install and use this one. </span>

For non-Mac users:  

```{r, eval=F} 
require(Rtools)
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

```{r, eval=FALSE}
# Other solution that may work
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="http:/cran.us.r.project.org")
```

For Mac users:  

```{r, eval=FALSE}
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="https://mac.R-project.org")
```




### Load population data {-}  

First we load the demographic data (counts of males and females by 5-year age category) for the two countries that we will be comparing, "Country A" and "Country B".  

```{r, echo=F}
# Country A
countryA_demo <- rio::import(here::here("data", "country_demographics.csv")) %>% 
     mutate(Country = "A") %>% 
     select(Country, everything()) %>% # re-arrange
     mutate(age_cat5 = str_replace_all(age_cat5, "\\+", "")) # remove + symbols
```

```{r, eval=F}
# Country A
countryA_demo <- import("country_demographics.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(countryA_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
```{r, echo=F}
# Country B
countryB_demo <- rio::import(here::here("data", "country_demographics_2.csv")) %>% 
     mutate(Country = "B") %>% 
     select(Country, everything()) # re-arrange
```

```{r, eval=F}
# Country B
countryB_demo <- import("country_demographics_2.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(countryB_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





### Load death counts {-}  

Conveniently, we also have the counts of deaths during the time period of interest, by age and sex. Each country's counts are in a separate file, shown below.   

```{r, echo=F}
A_males <- c(224, 257, 251, 245, 334, 245, 154, 189, 334, 342, 565, 432, 543, 432, 245, 543, 234, 354) # for males of country A
B_males <- c(34, 37, 51, 145, 434, 120, 100, 143, 307, 354, 463, 639, 706, 232, 275, 543, 234, 274) # for males of country B
A_females <- c(194, 254, 232, 214, 316, 224, 163, 167, 354, 354, 463, 574, 493, 295, 175, 380, 177, 392) # for females of country A
B_females <- c(54, 24, 32, 154, 276, 254, 123, 164, 254, 354, 453, 654, 435, 354, 165, 432, 287, 395) # for females of country B

age_cat5 <- c("0-4", "5-9", "10-14", "15-19", "20-24", "25-29",  "30-34", "35-39", "40-44",
                                                                                "45-49", "50-54", "55-59",
                                                                                "60-64", "65-69", "70-74",
                                                                                "75-79", "80-84", "85")
A_deaths <- data.frame(Country = "A", AgeCat = age_cat5, Male = A_males, Female = A_females)
B_deaths <- data.frame(Country = "B", AgeCat = age_cat5, Male = B_males, Female = B_females)
```

Deaths in Country A
```{r message=FALSE, echo=F}
DT::datatable(A_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Deaths in Country B

```{r message=FALSE, echo=F}
DT::datatable(B_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```






### Clean populations and deaths {-}  


We need to join and transform these data in the following ways:  

* Combine country populations into one dataset and pivot "long" so that each age-sex stratum is one row  
* Combine country death counts into one dataset and pivot "long" so each age-sex stratum is one row  
* Join the deaths to the populations  

First, we combine the country populations datasets, pivot longer, and do minor cleaning. See the page on [Pivoting data] for more detail.  

```{r}
pop_countries <- countryA_demo %>%  # begin with country A dataset
     bind_rows(countryB_demo) %>%        # bind rows, because cols are identically named
     pivot_longer(                       # pivot longer
          cols = c(m, f),                   # columns to combine into one
          names_to = "Sex",                 # name for new column containing the category ("m" or "f") 
          values_to = "Population") %>%     # name for new column containing the numeric values pivoted
     mutate(Sex = recode(Sex,            # re-code values for clarity
          "m" = "Male",
          "f" = "Female"))
```

The population data now look like this:  

```{r message=FALSE, echo=F}
DT::datatable(pop_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

And now we perform similar operations on the two deaths datasets.

```{r}
deaths_countries <- A_deaths %>%    # begin with country A deaths dataset
     bind_rows(B_deaths) %>%        # bind rows with B dataset, because cols are identically named
     pivot_longer(                  # pivot longer
          cols = c(Male, Female),        # column to transform into one
          names_to = "Sex",              # name for new column containing the category ("m" or "f") 
          values_to = "Deaths") %>%      # name for new column containing the numeric values pivoted
     rename(age_cat5 = AgeCat)    # rename for clarity
```

The deaths data now look like this, and contains data from both countries: 

```{r message=FALSE, echo=F}
DT::datatable(deaths_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


We now join the deaths and population data based on common columns `Country`, `age_cat5`, and `Sex`.  

```{r}
country_data <- pop_countries %>% 
     left_join(deaths_countries, by = c("Country", "age_cat5", "Sex"))
```

We can now classify `Sex`, `age_cat5`, and `Country` as factors so their ordering is specified correctly. We use the `as_factor()` function from the **forcats** package, as described in the page on [Factors]. Note, classifying the factor levels doesn't visibly change the data, but the `arrange()` command does sort it by Country, age category, and sex.  

```{r, warning=F, message=F}
country_data <- country_data %>% 
     mutate(
          Country = as_factor(Country),
          Country = fct_relevel(Country, "A", "B"),
          
          Sex = as_factor(Sex),
          Sex = fct_relevel(Sex, "Male", "Female"),
          
          age_cat5 = as_factor(age_cat5),
          age_cat5 = fct_relevel(age_cat5,
                                 "0-4", "5-9", "10-14", "15-19",
                                 "20-24", "25-29",  "30-34", "35-39",
                                 "40-44", "45-49", "50-54", "55-59",
                                 "60-64", "65-69", "70-74",
                                 "75-79", "80-84", "85")) %>% 
          arrange(Country, age_cat5, Sex)

```

```{r message=FALSE, echo=F}
DT::datatable(country_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">**_CAUTION:_** NB. If you have few deaths per stratum, use 10-, or 15-year categories, instead of 5-year categories for age, or combine categories</span>




### Load reference population {-}  

Lastly, we import the reference population (world "standard population" by sex)

```{r, echo=F}
# Reference population
standard_pop_data <- rio::import(here::here("data", "world_standard_population_by_sex.csv")) %>% 
     rename(age_cat5 = AgeGroup)
```

```{r, eval=F}
# Reference population
standard_pop_data <- import("world_standard_population_by_sex.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(standard_pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Clean reference population {-}

The values of the column `age_cat5` from the `standard_pop_data` contain the word "years" and "plus", while those of the `country_data` do not. We will have to remove this string to make the age category values match. We use `str_replace_all()` from the **stringr** package, as described in the page on [Characters and strings].  

Furthermore, the package **dsr** expects that in the standard population, the column containing counts will be called `"pop"`. So we rename that column accordingly.  

```{r}
# Remove specific string from column values
standard_pop_clean <- standard_pop_data %>%
     mutate(
          age_cat5 = str_replace_all(age_cat5, "years", ""),   # remove "year"
          age_cat5 = str_replace_all(age_cat5, "plus", ""),    # remove "plus"
          age_cat5 = str_replace_all(age_cat5, " ", "")) %>%   # remove " " space
     
     rename(pop = WorldStandardPopulation)   # change col name to "pop", as this is expected by dsr package
```

<span style="color: orange;">**_CAUTION:_** If you try to use `str_replace_all()` to remove a plus *symbol*, it won't work because it is a special symbol. "Escape" the specialnes by putting two back slashes in front, as in `str_replace_call(column, "\\+", "")`. </span>

Finally, the package **PHEindicatormethods**, detailed below, expects the standard populations joined to the country event and population counts. So, we will create a dataset `all_data` for that purpose.  

```{r}
all_data <- left_join(country_data, standard_pop_clean, by=c("age_cat5", "Sex"))
```

This complete dataset looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(all_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## **dsr** package {  }
 
Below we demonstrate calculating and comparing directly standardized rates using the **dsr** package. The **dsr** package allows you to calculate and compare directly standardized rates (no indirectly standardized rates!).
  
In the data Preparation section, we made separate datasets for country counts and standard population:  

1) the `country_data` object, which is a population table with the number of population and number of deaths per stratum per country  
2) the `standard_pop_clean` object, containing the number of population per stratum for our reference population, the World Standard Population  

We will use these separate datasets for the **dsr** approach.  


<!-- ======================================================= -->
### Standardized rates {-}

Below, we calculate rates per country directly standardized for age and sex. We use the `dsr()` function. 

Of note - `dsr()` expects one data frame for the country populations and event counts (deaths), *and a **separate** data frame with the reference population*. It also expects that in this reference population dataset the unit-time column name is "pop" (we assured this in the data Preparation section).  

There are many arguments, as annotated in the code below. Notably, `event = ` is set to the column `Deaths`, and the `fu = ` ("follow-up") is set to the `Population` column. We set the subgroups of comparison as the column `Country` and we standardize based on `age_cat5` and `Sex`. These last two columns are not assigned a particular named argument. See `?dsr` for details. 

```{r, warning=F, message=F}
# Calculate rates per country directly standardized for age and sex
mortality_rate <- dsr::dsr(
     data = country_data,  # specify object containing number of deaths per stratum
     event = Deaths,       # column containing number of deaths per stratum 
     fu = Population,      # column containing number of population per stratum
     subgroup = Country,   # units we would like to compare
     age_cat5,             # other columns - rates will be standardized by these
     Sex,
     refdata = standard_pop_clean, # reference population data frame, with column called pop
     method = "gamma",      # method to calculate 95% CI
     sig = 0.95,            # significance level
     mp = 100000,           # we want rates per 100.000 population
     decimals = 2)          # number of decimals)


# Print output as nice-looking HTML table
knitr::kable(mortality_rate) # show mortality rate before and after direct standardization
```

Above, we see that while country A had a lower crude mortality rate than country B, it has a higher standardized rate after direct age and sex standardization.




<!-- ======================================================= -->
### Standardized rate ratios {-}

```{r,warning=F, message=F}
# Calculate RR
mortality_rr <- dsr::dsrr(
     data = country_data, # specify object containing number of deaths per stratum
     event = Deaths,      # column containing number of deaths per stratum 
     fu = Population,     # column containing number of population per stratum
     subgroup = Country,  # units we would like to compare
     age_cat5,
     Sex,                 # characteristics to which we would like to standardize 
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",      # reference for comparison
     estimate = "ratio",  # type of estimate
     sig = 0.95,          # significance level
     mp = 100000,         # we want rates per 100.000 population
     decimals = 2)        # number of decimals

# Print table
knitr::kable(mortality_rr) 
```

The standardized mortality rate is 1.22 times higher in country A compared to country B (95% CI 1.17-1.27).

<!-- ======================================================= -->
### Standardized rate difference {-}

```{r, warning=F, message=F}
# Calculate RD
mortality_rd <- dsr::dsrr(
     data = country_data,       # specify object containing number of deaths per stratum
     event = Deaths,            # column containing number of deaths per stratum 
     fu = Population,           # column containing number of population per stratum
     subgroup = Country,        # units we would like to compare
     age_cat5,                  # characteristics to which we would like to standardize
     Sex,                        
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",            # reference for comparison
     estimate = "difference",   # type of estimate
     sig = 0.95,                # significance level
     mp = 100000,               # we want rates per 100.000 population
     decimals = 2)              # number of decimals

# Print table
knitr::kable(mortality_rd) 
```

Country A has 4.24 additional deaths per 100.000 population (95% CI 3.24-5.24) compared to country A.







<!-- ======================================================= -->
## **PHEindicatormethods** package {  }

Another way of calculating standardized rates is with the **PHEindicatormethods** package. This package allows you to calculate directly as well as indirectly standardized rates. We will show both.

<!-- ======================================================= -->
### Directly standardized rates {-}

Below, we first group the data by Country and then pass it to the function `phe_dsr()` to get directly standardized rates per country.

Of note - the reference (standard) population can be provided as a **column within the country-specific data frame** or as a **separate vector**. If provided within the country-specific data frame, you have to set `stdpoptype = "field"`. If provided as a vector, set `stdpoptype = "vector"`. In the latter case, you have to make sure the ordering of rows by strata is similar in both the country-specific data frame and the reference population, as records will be matched by position. In our example below, we provided the reference population as a column within the country-specific data frame.

See the help with `?phr_dsr` or the links in the References section for more information.  

```{r}
# Calculate rates per country directly standardized for age and sex
mortality_ds_rate_phe <- all_data %>%
     group_by(Country) %>%
     PHEindicatormethods::phe_dsr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          stdpop = pop,               # standard populations for each stratum
          stdpoptype = "field")       # either "vector" for a standalone vector or "field" meaning std populations are in the data  

# Print table
knitr::kable(mortality_ds_rate_phe)
```

<!-- ======================================================= -->
### Indirectly standardized rates {-}

For indirect standardization, you need a reference population with the number of deaths and number of population per stratum. In this example, we will be calculating rates for country A using country B as the reference population, as our previous `standard_pop_clean` reference population does not include number of deaths per stratum. 

Below, we first create the reference population from country B. Then, we pass mortality and population data for country A, combine it with the reference population, and pass it to the function `phe_isr()`, to get indirectly standardized rates. Of course, you can do it also vice versa.

Of note - in our example below, the reference population is provided as a separate data frame. In this case, we make sure that `x = `, `n = `, `x_ref = ` and `n_ref = ` vectors are all ordered by the same standardization category (stratum) values as that in our country-specific data frame, as records will be matched by position.

See the help with `?phr_isr` or the links in the References section for more information.  

```{r}
# Create reference population
refpopCountryB <- country_data %>% 
  filter(Country=="B") 

# Calculate rates for country A indirectly standardized by age and sex
mortality_is_rate_phe_A <- country_data %>%
     filter(Country=="A") %>%
     PHEindicatormethods::phe_isr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          x_ref = refpopCountryB$Deaths,  # reference number of deaths for each stratum
          n_ref = refpopCountryB$Population)  # reference population for each stratum

# Print table
knitr::kable(mortality_is_rate_phe_A)
```

<!-- ======================================================= -->
## Resources {  }

<span style="color: darkgreen;">**_TIP:_** If you would like to see another reproducible example using **dsr** than listed in this Handbook, please go to https://mran.microsoft.com/snapshot/2020-02-12/web/packages/dsr/vignettes/dsr.html.</span>

<span style="color: darkgreen;">**_TIP:_** For another example using **PHEindicatormethods**, please go to https://mran.microsoft.com/snapshot/2018-10-22/web/packages/PHEindicatormethods/vignettes/IntroductiontoPHEindicatormethods.html</span>

**PHEindicatormethods** reference file: https://cran.r-project.org/web/packages/PHEindicatormethods/PHEindicatormethods.pdf 


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/standardization.Rmd-->


# Moving averages { }  

```{r, out.width=c("100%"), echo=F}
knitr::include_graphics(here::here("images", "moving_avg_epicurve.png"))
```


This page will cover two methods to calculate and visualize moving averages:  

1) Calculate with the **slider** package  
2) Calculate *within* a `ggplot()` command with the **tidyquant** package  



<!-- ======================================================= -->
## Preparation {  }

### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,      # for data management and viz
  slider,         # for calculating moving averages
  tidyquant       # for calculating moving averages within ggplot
)
```


### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
## Calculate with **slider** {  }

**Use this approach to calculate a moving average in a data frame prior to plotting.**  

The **slider** package provides several "sliding window" functions to compute rolling averages, cumulative sums, rolling regressions, etc. It treats a dataframe as a vector of rows, allowing iteration row-wise over a dataframe.   

Here are some of the common functions:  

* `slide_dbl()` - iterates through a numeric column performing an operation using a sliding window  
  * `slide_sum()` - rolling sum shortcut  
  * `slide_mean()` - rolling average shortcut  
* `slide_index_dbl()` - applies the rolling window using a separate index column (useful if using dates or there are missing rows)  


**Core arguments**  

* `.x`, the first argument by default, is the vector to iterate over and to apply the function to  
* `.f`, the second argument by default, either:  
  * A function, written without parentheses, like `mean`, or  
  * A formula, which will be converted into a function. For example `~ .x - mean(.x)` will return the result of the current value minus the mean of the window's value  
  
* For more details see this [reference material](https://davisvaughan.github.io/slider/reference/slide.html)



**Window size**  

Specify the size of the window by using either `.before`, `.after`, or both arguments:   

* `.before = ` - Provide an integer  
* `.after = ` - Provide an integer  
* `.complete = ` - Set this to `TRUE` if you only want calculation performed on complete windows  

For example, to achieve a 7-day window including the current value and the six previous, use `.before = 6`. To achieve a "centered" window provide the same number to both `.before = ` and `.after = `.    

By default, `.complete = ` will be FALSE so if the full window of rows does not exist, the functions will use available rows to perform the calculation. Setting to TRUE restricts so calculations are only performed on complete windows.  

**Expanding window**  

To achieve *cumulative* operations, set the `.before = ` argument to `Inf`. This will conduct the operation on the current value and all coming before.  




### Rolling operations {-}  

Use `slide_dbl()`, which is made specifically to slide across a numeric vector. This operates across the data frame from row 1 onwards, so be careful the ordering of the rows. For example, below we arrange the dataset by date of onset, and then calculate a rolling mean of the days delay from symptom onset to hospital admission (`days_onset_hosp` column). We set the window as the value and the two values before.  

```{r}
rolled <- linelist %>%  
  
  arrange(date_onset) %>%             # arrange rows by ascending date of onset
  
  select(                             # select only some columns, for visibility
    case_id,                     
    date_onset,
    days_onset_hosp) %>% 
  
  mutate(
    delay_roll = slider::slide_dbl(   # define column delay_roll 
      .x        = days_onset_hosp,    # apply function to delays column
      .f        = mean,               # use mean()
      .before   = 2,                  # use value and 2 previous values
      .complete = FALSE)) %>%         # calculate even if three values not present

  mutate(delay_roll = round(delay_roll, 2)) # round values to 2 decimal places
```

```{r, echo = F}
DT::datatable(rolled, rownames = FALSE, options = list(pageLength = 15, scrollX=T), class = 'white-space: nowrap' )
```



**Grouped data**  

You can group your data so that the moving average is calculated within groups.  

If you do this *and* you have set `.complete = TRUE` be careful of how the rows are arranged. Every change in the grouping variable will start over the minimum window to allow a calculation.

See handbook page on [Grouping data] for details on grouping data.  


```{r}
grouped_roll <- linelist %>%
  
  select(                             # select only some columns, for example clarity
    case_id,                     
    hospital,
    date_onset,
    days_onset_hosp) %>%  
  
  arrange(hospital, date_onset) %>%   # arrange rows by hospital and then by date of onset

  group_by(hospital) %>%              # group by month of onset 
    
  
  mutate(                             # rolling mean, as before  
    delay_roll_hosp = slider::slide_dbl(
      .x = days_onset_hosp,
      .f = mean,
      .before = 30,                   # 30-day sliding window
      .complete = TRUE)
    )

```

You can now plot the moving averages by group. See the page on [ggplot tips] for more information on facetting and using **gghighlight**.    

```{r, warning=F, message=F}
grouped_roll %>% 
  ggplot()+
  geom_line(aes(x = date_onset, y = delay_roll_hosp, color = hospital))+
  theme_classic()+
  gghighlight::gghighlight()+
  facet_wrap(~hospital, ncol = 2)+
  labs(
    title = "Monthly rolling average of delay to care",
    x = "Date of symptom onset",
    y = "Days onset to admission")
```


<span style="color: red;">**_DANGER:_** If you get an error saying *"slide() was deprecated in tsibble 0.9.0 and is now defunct. Please use slider::slide() instead."*, it means that the `slide()` function from the **tsibble** package is masking the `slide()` function from **slider** package. Fix this by specifying the package in the command, such as `slider::slide_dbl()`.</span>



### Indexed rolling  {-}  

Often when conducting rolling operations *by date* (common with epidemiological linelists), we can encounter problems like:  

* Dates are missing from the dataframe, but should be included in a window  

To solve this, use `slide_index()` from **slider**. It uses a separate column as an *index* for the rolling window. If this column is a date, it will know which dates are not present in the data and include them in the window as `NA`. Below is an example to return a 7-day rolling average of new cases reported per day:  

* First we count the number of cases reported each day with `count()` from **dplyr** (see page on [Grouping data]).  

```{r}
# make dataset of daily counts and 7-day moving average
counts_7day <- linelist %>% 
  
  # get counts
  count(
    date_onset,        # count rows per unique onset_date
    name = "new_cases" # name of new column
    ) %>%
  
  # remove counts with missing onset_date
  filter(!is.na(date_onset))
```

The new dataset now looks like this. Note how some days are not present (no cases on those days). A simple `slide_dbl()` would incorrectly include the first seven rows in the first window.    

```{r, echo=F}
DT::datatable(counts_7day, rownames = FALSE, options = list(pageLength = 6, scrollX=T) )
```

We use the function `slide_index()` specifically because we recognize that *there are missing days* in the above dataframe, and they must be accounted for when creating windows of time. We set our "index" column (`.i` argument) as the column `date_onset`. Because `date_onset` is a column of class Date, the function accounts for the days that do not appear in the dataframe. For the arguments `.before` and `.after` we can use integers, or use **lubridate** functions like `days()` and `months()`.  

```{r}
## calculate 7-day rolling average, accounting for missing days
rolling <- counts_7day %>% 
  mutate(
    avg_7day = slider::slide_index_dbl(  # create new column
        new_cases,                       # calculate avg based on value in new_cases column
        .i = date_onset,                 # index column is date_onset, so non-present dates are included in 7day window 
        .f = ~mean(.x, na.rm = TRUE),    # function is mean() with missing values removed
        .before = days(6),               # window is the day and 6-days before
        .complete = TRUE))               # fills in first days with NA
```

You can see below that the time windows account for days that do not appear in the data.  

```{r, echo=F}
DT::datatable(rolling, rownames = FALSE, options = list(pageLength = 6, scrollX=T) )
```

We can now plot the linelist, with the 7-day moving average overlaid. If needed, see the page on [ggplot tips].    

```{r, warning = F, message=F}
ggplot(data = rolling, aes(x = date_onset))+
  geom_histogram(         # plot histogram of daily cases
    aes(y = new_cases),
    fill   ="#92a8d1",    # bar color
    stat   = "identity",  # height = value
    colour = "#92a8d1")+  # color around bars
  geom_line(              # overlay line
    aes(y = avg_7day),    # use 7-day average column
    color="red",         
    size = 1) +           # line thickness  
  scale_x_date(           # x-axis by months
    date_breaks = "1 month",
    date_labels = '%d/%m',
    expand = c(0,0)) +
  scale_y_continuous(
    expand = c(0,0),
    limits = c(0, NA)) + 
  labs(
    x="",
    y ="Number of confirmed cases")+ 
  theme_minimal() 
```



If you rolling average by months, you can use **lubridate** to group the data by month, and then apply `slide_index_dbl()` as below shown for a three-month rolling average:  

```{r}
ll_months <- linelist %>%
  mutate(
    month_onset = floor_date(date_onset, "month")) %>% 
  count(month_onset) %>% 
  filter(!is.na(month_onset)) %>% 
  mutate(
    monthly_roll = slider::slide_index_dbl(
      n,                                # calculate avg based on value in new_cases column
      .i = month_onset,                 # index column is date_onset, so non-present dates are included in 7day window 
      .f = ~mean(.x, na.rm = TRUE),     # function is mean() with missing values removed
      .before = months(2),              # window is the day and 6-days before
      .complete = TRUE))                # fills in first days with NA
  
```




<!-- ======================================================= -->
## Calculate with **tidyquant** within `ggplot()` {  }

The package **tidyquant** offers another approach to calculating moving averages - this time from *within* a `ggplot()` command itself.  

Below the `linelist` data are counted by date of onset, and this is plotted as a faded line (`alpha` < 1). Overlaid on top is a line created with `geom_ma()`, with a window of 7 days (`n = 7`) with specified color and thickness.  

By default `geom_ma()` uses a simple moving average (`ma_fun = "SMA"`), but other types can be specified, such as:  

* "EMA" - exponential moving average (more weight to recent observations)  
* "WMA" - weighted moving average (`wts` are used to weight observations in the moving average)  
* Others can be found in the function documentation  

```{r}
linelist %>% 
  count(date_onset) %>%                 # count cases per day
  filter(!is.na(date_onset)) %>%        # remove cases missing onset date
  ggplot(aes(x = date_onset, y = n))+   # start ggplot
    geom_line(                          # plot raw values
      size = 1,
      alpha = 0.2                       # semi-transparent line
      )+             
    tidyquant::geom_ma(                 # plot moving average
      n = 7,           
      size = 1,
      color = "blue")+ 
  theme_minimal()                       # simple background
```

See this [vignette](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html) for more details on the options available within **tidyquant**.  

<!-- ======================================================= -->
## Resources {  }


See the helpful online [vignette for the **slider** package](https://cran.r-project.org/web/packages/slider/vignettes/slider.html)  

The **slider** [github page](https://github.com/DavisVaughan/slider)

A **slider** [vignette](https://davisvaughan.github.io/slider/articles/slider.html)  

[tidyquant vignette](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html)

If your use case requires that you “skip over” weekends and even holidays, you might like **almanac** package.



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/moving_average.Rmd-->


# Time series and outbreak detection { }  

<!-- ======================================================= -->
## Overview {  }

This tab demonstrates the use of several packages for time series analysis. 
It primarily relies on packages from the [**tidyverts**](https://tidyverts.org/) 
family, but will also use the RECON [**trending**](https://github.com/reconhub/trending) 
package to fit models that are more appropriate for infectious disease epidemiology. 

1.  Time series data 
2.  Descriptive analysis 
3.  Fitting regressions
4.  Relation of two time series 
5.  Outbreak detection
6.  Interrupted time series


<!-- ======================================================= -->
## Preparation {  }

### Packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r load_packages}
pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets
               slider,       # for calculating moving averages
               imputeTS,     # for filling in missing values
               feasts,       # for time series decomposition and autocorrelation
               forecast,     # fit sin and cosin terms to data (note: must load after feasts)
               trending,     # fit and assess models 
               tmaptools,    # for getting geocoordinates (lon/lat) based on place names
               ecmwfr,       # for interacting with copernicus sateliate CDS API
               stars,        # for reading in .nc (climate data) files
               units,        # for defining units of measurement (climate data)
               yardstick,    # for looking at model accuracy
               surveillance  # for aberration detection
               )
``` 

### Load data {-}

The example dataset used in this section:

-   Weekly counts of campylobacter cases reported in Germany between 2001 and 2011. 

This dataset is a reduced version of the dataset available in the [**surveillance**](https://cran.r-project.org/web/packages/surveillance/) package. 
(for details load the surveillance package and see `?campyDE`)

The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r read_data_hide, echo=F}
# import the counts into R
counts <- rio::import(here::here("data", "campylobacter_germany.xlsx"))

```

```{r read_data_show, eval=F}
# import the linelist
counts <- rio::import("campylobacter_germany.xlsx")
```

The first 10 rows of the counts are displayed below.

```{r inspect_data, message=FALSE, echo=F}
# display the counts data as a table
DT::datatable(head(counts, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Clean data {-}

The below makes sure that the date column is in the appropriate format. 
For this tab we will be using the **tsibble** package and so the `yearweek` 
function will be used to create a calendar week variable. There are several other
ways of doing this (see the [Working with dates] page for details), however for 
time series its best to keep within one framework. 

```{r clean_data}

## ensure the date column is in the appropriate format
counts$date <- as.Date(counts$date)

## create a calendar week variable 
## fitting ISO definitons of weeks starting on a monday
counts <- counts %>% 
     mutate(epiweek = yearweek(date, week_start = 1))

```

### Download climate data {-} 

In the *relation of two time series* section of this tab, we will be comparing 
campylobacter case counts to climate data. 

Climate data for anywhere in the world can be downloaded from the EU's Copernicus 
Satellite. These are not exact measurements, but based on a model (similar to 
interpolation), however the benefit is global hourly coverage as well as forecasts. 

We will be using the **ecmwfr** package to pull data from the Copernicus 
climate data store. You will need to create a free account in order for this to 
work. The package website has a useful [walkthrough](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds)
of how to do this. Below is example code of how to go about doing this, once you 
have the appropriate API keys. You have to replace the X's below with your account
IDs.
You will need to download one year of data at a time otherwise the server times-out. 

If you are not sure of the coordinates for a location you want to download data 
for, you can use the **tmaptools** package to pull the coordinates off open street
maps. An alternative option is the [**photon**](https://github.com/rCarto/photon)
package, however this has not been released on to CRAN yet; the nice thing about 
**photon** is that it provides more contextual data for when there are several 
matches for your search.

```{r weather_data, eval = FALSE}

## retrieve location coordinates
coords <- geocode_OSM("Germany", geometry = "point")

## pull together long/lats in format for ERA-5 querying (bounding box) 
## (as just want a single point can repeat coords)
request_coords <- str_glue_data(coords$coords, "{y}/{x}/{y}/{x}")


## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)
## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app
## https://github.com/bluegreen-labs/ecmwfr

## set up key for weather data 
wf_set_key(user = "XXXXX",
           key = "XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX",
           service = "cds") 

## run for each year of interest (otherwise server times out)
for (i in 2002:2011) {
  
  ## pull together a query 
  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax
  ## change request to a list using addin button above (python to list)
  ## Target is the name of the output file!!
  request <- request <- list(
    product_type = "reanalysis",
    format = "netcdf",
    variable = c("2m_temperature", "total_precipitation"),
    year = c(i),
    month = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    day = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12",
            "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
            "25", "26", "27", "28", "29", "30", "31"),
    time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00", "06:00", "07:00",
             "08:00", "09:00", "10:00", "11:00", "12:00", "13:00", "14:00", "15:00",
             "16:00", "17:00", "18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
    area = request_coords,
    dataset_short_name = "reanalysis-era5-single-levels",
    target = paste0("germany_weather", i, ".nc")
  )
  
  ## download the file and store it in the current working directory
  file <- wf_request(user     = "XXXXX",  # user ID (for authentication)
                     request  = request,  # the request
                     transfer = TRUE,     # download the file
                     path     = here::here("data", "Weather")) ## path to save the data
  }

```

### Load climate data {-}

```{r read_climate, warning = FALSE, message = FALSE}

## define path to weather folder 
file_paths <- list.files(
  here::here("data", "weather"), 
  full.names = TRUE)

## only keep those with the current name of interest 
file_paths <- file_paths[str_detect(file_paths, "germany")]


## read in as a stars object 
data <- stars::read_stars(file_paths)

## change to a data frame 
temp_data <- as_tibble(data) %>% 
  ## add in variables and correct units
  mutate(
    ## create an calendar week variable 
    epiweek = tsibble::yearweek(time), 
    ## create a date variable (start of calendar week)
    date = as.Date(epiweek),
    ## change temperature from kelvin to celsius
    t2m = set_units(t2m, celsius), 
    ## change precipitation from metres to millimetres 
    tp  = set_units(tp, mm)) %>% 
  ## group by week (keep the date too though)
  group_by(epiweek, date) %>% 
  ## get the average per week
  summarise(t2m = as.numeric(mean(t2m)), 
            tp = as.numeric(mean(tp)))

```




<!-- ======================================================= -->
## Time series data {  }

There are a number of different packages for structuring and handling time series
data. As said, we will focus on the **tidyverts** family of packages and so will
use the **tsibble** package to define our time series object. Having a data set
defined as a time series object means it is much easier to structure our analysis. 

To do this we use the `tsibble()` function and specify the "index", i.e. the variable
specifying the time unit of interest. In our case this is the `epiweek` variable. 

If we had a data set with weekly counts by province, for example, we would also 
be able to specify the grouping variable using the `key = ` argument. 
This would allow us to do analysis for each group. 


```{r ts_object}

## define time series object 
counts <- tsibble(counts, index = epiweek)

```

Looking at `class(counts)` tells you that on top of being a tidy data frame 
("tbl_df", "tbl", "data.frame"), it has the additional properties of a time series
data frame ("tbl_ts"). 

You can take a quick look at your data by using ggplot2. We see from the plot that
there is a clear seasonal pattern, and that there are no missings. However, there
seems to be an issue with reporting at the beginning of each year; cases drop 
in the last week of the year and then increase for the first week of the next year. 

```{r basic_plot}

## plot a line graph of cases by week
ggplot(counts, aes(x = epiweek, y = case)) + 
     geom_line()

```


<span style="color: red;">**_DANGER:_** Most datasets aren't as clean as this example. 
You will need to check for duplicates and missings as below. </span>

<!-- ======================================================= -->
### Duplicates {-}

**tsibble** does not allow duplicate observations. So each row will need to be
unique, or unique within the group (`key` variable). 
The package has a few functions that help to identify duplicates. These include
`are_duplicated()` which gives you a TRUE/FALSE vector of whether the row is a 
duplicate, and `duplicates()` which gives you a data frame of the duplicated rows. 

See the page on [De-duplication] for more details on how to select rows you want. 

```{r duplicates, eval = FALSE}

## get a vector of TRUE/FALSE whether rows are duplicates
are_duplicated(counts, index = epiweek) 

## get a data frame of any duplicated rows 
duplicates(counts, index = epiweek) 

```

<!-- ======================================================= -->
### Missings {-}

We saw from our brief inspection above that there are no missings, but we also 
saw there seems to be a problem with reporting delay around new year. 
One way to address this problem could be to set these values to missing and then 
to impute values. The simplest form of time series imputation is to draw
a straight line between the last non-missing and the next non-missing value. 
To do this we will use the **imputeTS** package function `na_interpolation()`. 

See the [Missing data] page for other options for imputation.  

Another alternative would be to calculating a moving average, to try and smooth
over these apparent reporting issues (see next section, and the page on [Moving averages]). 

```{r missings}

## create a variable with missings instead of weeks with reporting issues
counts <- counts %>% 
     mutate(case_miss = if_else(
          ## if epiweek contains 52, 53, 1 or 2
          str_detect(epiweek, "W51|W52|W53|W01|W02"), 
          ## then set to missing 
          NA_real_, 
          ## otherwise keep the value in case
          case
     ))

## alternatively interpolate missings by linear trend 
## between two nearest adjacent points
counts <- counts %>% 
  mutate(case_int = na_interpolation(case_miss)
         )

## to check what values have been imputed compared to the original
ggplot_na_imputations(counts$case_miss, counts$case_int) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```




<!-- ======================================================= -->
## Descriptive analysis {  }



<!-- ======================================================= -->
### Moving averages {-}

If data is very noisy (counts jumping up and down) then it can be helpful to 
calculate a moving average. In the example below, for each week we calculate the 
average number of cases from the four previous weeks. This smooths the data, to 
make it more interpretable. In our case this does not really add much, so we will
stick to the interpolated data for further analysis. 
See the [Moving averages] page for more detail. 

```{r moving_averages}

## create a moving average variable (deals with missings)
counts <- counts %>% 
     ## create the ma_4w variable 
     ## slide over each row of the case variable
     mutate(ma_4wk = slider::slide_dbl(case, 
                               ## for each row calculate the name
                               ~ mean(.x, na.rm = TRUE),
                               ## use the four previous weeks
                               .before = 4))

## make a quick visualisation of the difference 
ggplot(counts, aes(x = epiweek)) + 
     geom_line(aes(y = case)) + 
     geom_line(aes(y = ma_4wk), colour = "red")

```


<!-- ======================================================= -->
### Periodicity {-}

```{r periodogram}

## x is a dataset
## counts is variable with count data or rates within x 
## start_week is the first week in your dataset
## period is how many units in a year 
## output is whether you want return spectral periodogram or the peak weeks
  ## "periodogram" or "weeks"
periodogram <- function(x, 
                        counts, 
                        start_week = c(2002, 1), 
                        period = 52, 
                        output = "weeks") {
  

    ## make sure is not a tsibble, filter to project and only keep columns of interest
    prepare_data <- dplyr::as_tibble(x)
    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
    prepare_data <- dplyr::select(prepare_data, {{counts}})
    
    ## create an intermediate "zoo" time series to be able to use with spec.pgram
    zoo_cases <- zoo::zooreg(prepare_data, 
                             start = start_week, frequency = period)
    
    ## get a spectral periodogram not using fast fourier transform 
    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)
    
    ## return the peak weeks 
    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period
    
    if (output == "weeks") {
      periodo_weeks
    } else {
      periodo
    }
    
}

## get spectral periodogram for extracting weeks with the highest frequencies 
## (checking of seasonality) 
periodo <- periodogram(counts, 
                       case_int, 
                       start_week = c(2002, 1),
                       output = "periodogram")

## pull spectrum and frequence in to a dataframe for plotting
periodo <- data.frame(periodo$freq, periodo$spec)

## plot a periodogram showing the most frequently occuring periodicity 
ggplot(data = periodo, 
                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + 
  geom_line() + 
  labs(x = "Period (Weeks)", y = "Log(density)")


## get a vector weeks in ascending order 
peak_weeks <- periodogram(counts, 
                          case_int, 
                          start_week = c(2002, 1), 
                          output = "weeks")

```

<span style="color: black;">**_NOTE:_** It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) </span>

<!-- ======================================================= -->
### Decomposition {-}

Classical decomposition is used to break a time series down several parts, which
when taken together make up for the pattern you see. 
These different parts are:  

* The trend-cycle (the long-term direction of the data)  
* The seasonality (repeating patterns)  
* The random (what is left after removing trend and season)  


```{r decomposition, warning=F, message=F}

## decompose the counts dataset 
counts %>% 
  # using an additive classical decomposition model
  model(classical_decomposition(case_int, type = "additive")) %>% 
  ## extract the important information from the model
  components() %>% 
  ## generate a plot 
  autoplot()

```

<!-- ======================================================= -->
### Autocorrelation {-}

Autocorrelation tells you about the relation between the counts of each week 
and the weeks before it (called lags).  

Using the `ACF()` function, we can produce a plot which shows us a number of lines 
for the relation at different lags. Where the lag is 0 (x = 0), this line would 
always be 1 as it shows the relation between an observation and itself (not shown here). 
The first line shown here (x = 1) shows the relation between each observation 
and the observation before it (lag of 1), the second shows the relation between 
each observation and the observation before last (lag of 2) and so on until lag of
52 which shows the relation between each observation and the observation from 1 
year (52 weeks before).  

Using the `PACF()` function (for partial autocorrelation) shows the same type of relation 
but adjusted for all other weeks between. This is less informative for determining
periodicity. 

```{r autocorrelation}

## using the counts dataset
counts %>% 
  ## calculate autocorrelation using a full years worth of lags
  ACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

## using the counts data set 
counts %>% 
  ## calculate the partial autocorrelation using a full years worth of lags
  PACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

```

You can formally test the null hypothesis of independence in a time series (i.e. 
that it is not autocorrelated) using the Ljung-Box test (in the **stats** package). 
A significant p-value suggests that there is autocorrelation in the data.

```{r ljung_box}

## test for independance 
Box.test(counts$case_int, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Fitting regressions {  }

It is possible to fit a large number of different regressions to a time series, 
however, here we will demonstrate how to fit a negative binomial regression - as 
this is often the most appropriate for counts data in infectious diseases. 

<!-- ======================================================= -->
### Fourier terms {-}

Fourier terms are the equivalent of sin and cosin curves. The difference is that 
these are fit based on finding the most appropriate combination of curves to explain
your data.  

If only fitting one fourier term, this would be the equivalent of fitting a sin 
and a cosin for your most frequently occurring lag seen in your periodogram (in our 
case 52 weeks). We use the `fourier()` function from the **forecast** package.  

In the below code we assign using the `$`, as `fourier()` returns two columns (one 
for sin one for cosin) and so these are added to the dataset as a list, called 
"fourier" - but this list can then be used as a normal variable in regression. 

```{r fourier}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  fourier(K = 1)
```

<!-- ======================================================= -->
### Negative binomial {-}

It is possible to fit regressions using base **stats** or **MASS**
functions (e.g. `lm()`, `glm()` and `glm.nb()`). However we will be using those from 
the **trending** package, as this allows for calculating appropriate confidence
and prediction intervals (which are otherwise not available). 
The syntax is the same, and you specify an outcome variable then a tilde (~) 
and then add your various exposure variables of interest separated by a plus (+). 

The other difference is that we first define the model and then `fit()` it to the 
data. This is useful because it allows for comparing multiple different models 
with the same syntax. 

<span style="color: darkgreen;">**_TIP:_** If you wanted to use rates, rather than 
counts you could include the population variable as a logarithmic offset term, by adding 
`offset(log(population)`. You would then need to set population to be 1, before 
using `predict()` in order to produce a rate. </span>

<span style="color: darkgreen;">**_TIP:_** For fitting more complex models such 
as ARIMA or prophet, see the [**fable**](https://fable.tidyverts.org/index.html) package.</span>

```{r nb_reg, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier)

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model)

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```

<!-- ======================================================= -->
### Residuals {-}

To see how well our model fits the observed data we need to look at the residuals. 
The residuals are the difference between the observed counts and the counts 
estimated from the model. We could calculate this simply by using `case_int - estimate`, 
but the `residuals()` function extracts this directly from the regression for us.

What we see from the below, is that we are not explaining all of the variation 
that we could with the model. It might be that we should fit more fourier terms, 
and address the amplitude. However for this example we will leave it as is. 
The plots show that our model does worse in the peaks and troughs (when counts are
at their highest and lowest) and that it might be more likely to underestimate 
the observed counts. 

```{r, warning=F, message=F}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = residuals(fitted_model$fitted_model, type = "response"))

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Relation of two time series {  }

Here we look at using weather data (specifically the temperature) to explain 
campylobacter case counts. 

<!-- ======================================================= -->
### Merging datasets {-}

We can join our datasets using the week variable. For more on merging see the 
handbook section on [joining].

```{r join}

## left join so that we only have the rows already existing in counts
## drop the date variable from temp_data (otherwise is duplicated)
counts <- left_join(counts, 
                    select(temp_data, -date),
                    by = "epiweek")

```

<!-- ======================================================= -->
### Descriptive analysis {-}

First plot your data to see if there is any obvious relation. 
The plot below shows that there is a clear relation in the seasonality of the two
variables, and that temperature might peak a few weeks before the case number.
For more on pivoting data, see the handbook section on [cleaning data]. 

```{r basic_plot_bivar}

counts %>% 
  ## keep the variables we are interested 
  select(epiweek, case_int, t2m) %>% 
  ## change your data in to long format
  pivot_longer(
    ## use epiweek as your key
    !epiweek,
    ## move column names to the new "measure" column
    names_to = "measure", 
    ## move cell values to the new "values" column
    values_to = "value") %>% 
  ## create a plot with the dataset above
  ## plot epiweek on the x axis and values (counts/celsius) on the y 
  ggplot(aes(x = epiweek, y = value)) + 
    ## create a separate plot for temperate and case counts 
    ## let them set their own y-axes
    facet_grid(measure ~ ., scales = "free_y") +
    ## plot both as a line
    geom_line()

```

<!-- ======================================================= -->
### Lags and cross-correlation {-}

To formally test which weeks are most highly related between cases and temperature. 
We can use the cross-correlation function (`CCF()`) from the **feasts** package. 
You could also visualise (rather than using `arrange`) using the `autoplot()` function. 

```{r cross_correlation}

counts %>% 
  ## calculate cross-correlation between interpolated counts and temperature
  CCF(case_int, t2m,
      ## set the maximum lag to be 52 weeks
      lag_max = 52, 
      ## return the correlation coefficient 
      type = "correlation") %>% 
  ## arange in decending order of the correlation coefficient 
  ## show the most associated lags
  arrange(-ccf) %>% 
  ## only show the top ten 
  slice_head(n = 10)

```

We see from this that a lag of 4 weeks is most highly correlated, 
so we make a lagged temperature variable to include in our regression. 

```{r lag_tempvar}

counts <- counts %>% 
  ## create a new variable for temperature lagged by four weeks
  mutate(t2m_lag4 = lag(t2m, n = 4))

```


<!-- ======================================================= -->
### Negative binomial with two variables {-}

We fit a negative binomial regression as done previously. This time we add the 
temperature variable lagged by four weeks. 

```{r nb_reg_bivar, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier + 
    ## use the temperature lagged by four weeks 
    t2m_lag4
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model)

```


To investigate the individual terms, we can pull the original negative binomial
regression out of the **trending** format using `get_model()` and pass this to the
**broom** package `tidy()` function to retrieve exponentiated estimates and associated
confidence intervals.  

What this shows us is that lagged temperature, after controlling for trend and seasonality, 
is similar to the case counts (estimate ~ 1) and significantly associated. 
This suggests that it might be a good variable for use in predicting future case
numbers (as climate forecasts are readily available). 

```{r results_nb_reg_bivar}

fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE)
```

A quick visual inspection of the model shows that it might do a better job of 
estimating the observed case counts. 

```{r plot_nb_reg_bivar, warning=F, message=F}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```


#### Residuals {-}

We investigate the residuals again to see how well our model fits the observed data. 
The results and interpretation here are similar to those of the previous regression, 
so it may be more feasible to stick with the simpler model without temperature. 

```{r}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = case_int - estimate)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Outbreak detection {  }

We will demonstrate two (similar) methods of detecting outbreaks here. 
The first builds on the sections above. 
We use the **trending** package to fit regressions to previous years, and then
predict what we expect to see in the following year. If observed counts are above
what we expect, then it could suggest there is an outbreak. 
The second method is based on similar principles but uses the **surveillance** package,
which has a number of different algorithms for aberration detection.

<span style="color: orange;">**_CAUTION:_** Normally, you are interested in the current year (where you only know counts up to the present week). So in this example we are pretending to be in week 52 of 2011.</span>

<!-- ======================================================= -->
### **trending** package {-}

For this method we define a baseline (which should usually be about 5 years of data). 
We fit a regression to the baseline data, and then use that to predict the estimates
for the next year. 

<!-- ======================================================= -->
#### Cut-off date { -}

It is easier to define your dates in one place and then use these throughout the
rest of your code.  

Here we define a start date (when our observations started) and a cut-off date 
(the end of our baseline period - and when the period we want to predict for starts). 
We also define how many weeks are in your year of interest (the one you are going to
be predicting). 

```{r cut_off}

## define start date (when observations began)
start_date <- min(counts$epiweek)

## define a cut-off week (end of baseline, start of prediction period)
cut_off <- yearweek("2010-12-31")

## get the year which want to predict for 
## add one week to cut_off (to push in to next year) and change to only have Year
pred_year <- format(cut_off + 1, format = "%Y") %>% 
  ## change to numeric
  as.numeric()

## find how many weeks in year of interest
num_weeks <- ifelse(
  ## true/false of whether pred_year is a 53 week year
  is_53weeks(pred_year),
  ## if true then return 53 
  53, 
  ## otherwise return 52
  52)

```

<!-- ======================================================= -->
#### Fourier terms {  }

We need to redefine our fourier terms - as we want to fit them to the baseline 
date only and then predict (extrapolate) those terms for the next year. 
To do this we need to combine two output lists from the `fourier()` function together; 
the first one is for the baseline data, and the second one predicts for the 
year of interest (by defining the `h` argument).  

*N.b.* to bind rows we have to use `rbind()` (rather than tidyverse `bind_rows`) as
the fourier columns are a list (so not named individually). 

```{r fourier_terms_pred}

## define fourier terms (sincos) 
counts <- counts %>% 
  mutate(
    ## combine fourier terms for weeks prior to  and after 2010 cut-off date
    ## (nb. 2011 fourier terms are predicted)
    fourier = rbind(
      ## get fourier terms for previous years
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for 2011 (using baseline data)
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = num_weeks
        )
      )
    )

```

<!-- ======================================================= -->
#### Split data and fit regression {-}

We now have to split our dataset in to the baseline period and the prediction 
period. This is done using the **dplyr** `group_split()` function after `group_by()`, 
and will create a list with two data frames, one for before your cut-off and one 
for after.  

We then use the **purrr** package `pluck()` function to pull the datasets out of the
list (equivalent of using square brackets, e.g. `dat[[1]]`), and can then fit 
our model to the baseline data, and then use the `predict()` function for our data
of interest after the cut-off.  

See the page on [Iteration] to learn more about **purrr**.  

```{r forecast_regression, warning = FALSE}
# split data for fitting and prediction
dat <- counts %>% 
  group_by(epiweek <= cut_off) %>%
  group_split()

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier
)

# define which data to use for fitting and which for predicting
fitting_data <- pluck(dat, 2)
pred_data <- pluck(dat, 1) %>% 
  select(case_int, epiweek, fourier)

# fit model 
fitted_model <- trending::fit(model, fitting_data)

# get confint and estimates for fitted data
observed <- fitted_model %>% 
  predict()

# forecast with data want to predict with 
forecasts <- fitted_model %>% 
  predict(pred_data)

## combine baseline and predicted datasets
observed <- bind_rows(observed, forecasts)

```

As previously, we can visualise our model with **ggplot**. We highlight alerts with
red dots for observed counts above the 95% prediction interval. 
This time we also add a vertical line to label when the forecast starts. 

```{r forecast_plot}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "grey") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## plot in points for the observed counts above expected
  geom_point(
    data = filter(observed, case_int > upper_pi), 
    aes(y = case_int), 
    colour = "red", 
    size = 2) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(cut_off), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Forecast", 
           x = cut_off, 
           y = max(observed$upper_pi), 
           angle = 90, 
           vjust = 1
           ) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```



<!-- ======================================================= -->
#### Prediction validation {-}

Beyond inspecting residuals, it is important to investigate how good your model is
at predicting cases in the future. This gives you an idea of how reliable your 
threshold alerts are.  

The traditional way of validating is to see how well you can predict the latest 
year before the present one (because you don't yet know the counts for the "current year"). 
For example in our data set we would use the data from 2002 to 2009 to predict 2010, 
and then see how accurate those predictions are. Then refit the model to include
2010 data and use that to predict 2011 counts.  

As can be seen in the figure below by *Hyndman et al* in "Forecasting principles 
and practice". 

![](`r "https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png"`)

The downside of this is that you are not using all the data available to you, and 
it is not the final model that you are using for prediction. 

An alternative is to use a method called cross-validation. In this scenario you 
roll over all of the data available to fit multiple models to predict one year ahead. 
You use more and more data in each model, as seen in the figure below from the 
same *Hyndman et al* text. 
For example, the first model uses 2002 to predict 2003, the second uses 2002 and 
2003 to predict 2004, and so on. 
![](`r "https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png"`)

In the below we use **purrr** package `map()` function to loop over each dataset. 
We then put estimates in one data set and merge with the original case counts, 
to use the **yardstick** package to compute measures of accuracy. 
We compute four measures including: Root mean squared error (RMSE), Mean absolute error	
(MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).

```{r cross_validation, warning = FALSE}

## Cross validation: predicting week(s) ahead based on sliding window

## expand your data by rolling over in 52 week windows (before + after) 
## to predict 52 week ahead
## (creates longer and longer chains of observations - keeps older data)

## define window want to roll over
roll_window <- 52

## define weeks ahead want to predict 
weeks_ahead <- 52

## create a data set of repeating, increasingly long data
## label each data set with a unique id
## only use cases before year of interest (i.e. 2011)
case_roll <- counts %>% 
  filter(epiweek < cut_off) %>% 
  ## only keep the week and case counts variables
  select(epiweek, case_int) %>% 
    ## drop the last x observations 
    ## depending on how many weeks ahead forecasting 
    ## (otherwise will be an actual forecast to "unknown")
    slice(1:(n() - weeks_ahead)) %>%
    as_tsibble(index = epiweek) %>% 
    ## roll over each week in x after windows to create grouping ID 
    ## depending on what rolling window specify
    stretch_tsibble(.init = roll_window, .step = 1) %>% 
  ## drop the first couple - as have no "before" cases
  filter(.id > roll_window)


## for each of the unique data sets run the code below
forecasts <- purrr::map(unique(case_roll$.id), 
                        function(i) {
  
  ## only keep the current fold being fit 
  mini_data <- filter(case_roll, .id == i) %>% 
    as_tibble()
  
  ## create an empty data set for forecasting on 
  forecast_data <- tibble(
    epiweek = seq(max(mini_data$epiweek) + 1,
                  max(mini_data$epiweek) + weeks_ahead,
                  by = 1),
    case_int = rep.int(NA, weeks_ahead),
    .id = rep.int(i, weeks_ahead)
  )
  
  ## add the forecast data to the original 
  mini_data <- bind_rows(mini_data, forecast_data)
  
  ## define the cut off based on latest non missing count data 
  cv_cut_off <- mini_data %>% 
    ## only keep non-missing rows
    filter(!is.na(case_int)) %>% 
    ## get the latest week
    summarise(max(epiweek)) %>% 
    ## extract so is not in a dataframe
    pull()
  
  ## make mini_data back in to a tsibble
  mini_data <- tsibble(mini_data, index = epiweek)
  
  ## define fourier terms (sincos) 
  mini_data <- mini_data %>% 
    mutate(
    ## combine fourier terms for weeks prior to  and after cut-off date
    fourier = rbind(
      ## get fourier terms for previous years
      forecast::fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for following year (using baseline data)
      fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = weeks_ahead
        )
      )
    )
  
  
  # split data for fitting and prediction
  dat <- mini_data %>% 
    group_by(epiweek <= cv_cut_off) %>%
    group_split()

  ## define the model you want to fit (negative binomial) 
  model <- glm_nb_model(
    ## set number of cases as outcome of interest
    case_int ~
      ## use epiweek to account for the trend
      epiweek +
      ## use the furier terms to account for seasonality
      fourier
  )

  # define which data to use for fitting and which for predicting
  fitting_data <- pluck(dat, 2)
  pred_data <- pluck(dat, 1)
  
  # fit model 
  fitted_model <- trending::fit(model, fitting_data)
  
  # forecast with data want to predict with 
  forecasts <- fitted_model %>% 
    predict(pred_data) %>% 
    ## only keep the week and the forecast estimate
    select(epiweek, estimate)
    
  }
  )

## make the list in to a data frame with all the forecasts
forecasts <- bind_rows(forecasts)

## join the forecasts with the observed
forecasts <- left_join(forecasts, 
                       select(counts, epiweek, case_int),
                       by = "epiweek")

## using {yardstick} compute metrics
  ## RMSE: Root mean squared error
  ## MAE:  Mean absolute error	
  ## MASE: Mean absolute scaled error
  ## MAPE: Mean absolute percent error
model_metrics <- bind_rows(
  ## in your forcasted dataset compare the observed to the predicted
  rmse(forecasts, case_int, estimate), 
  mae( forecasts, case_int, estimate),
  mase(forecasts, case_int, estimate),
  mape(forecasts, case_int, estimate),
  ) %>% 
  ## only keep the metric type and its output
  select(Metric  = .metric, 
         Measure = .estimate) %>% 
  ## make in to wide format so can bind rows after
  pivot_wider(names_from = Metric, values_from = Measure)

## return model metrics 
model_metrics

```


<!-- ======================================================= -->
### **surveillance** package {-}

In this section we use the **surveillance** package to create alert thresholds 
based on outbreak detection algorithms. There are several different methods 
available in the package, however we will focus on two options here. 
For details, see these papers on the [application](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf)
and [theory](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf)
of the alogirthms used. 

The first option uses the improved Farrington method. This fits a negative 
binomial glm (including trend) and down-weights past outbreaks (outliers) to 
create a threshold level. 

The second option use the glrnb method. This also fits a negative binomial glm 
but includes trend and fourier terms (so is favoured here). The regression is used
to calculate the "control mean" (~fitted values) - it then uses a computed 
generalized likelihood ratio statistic to assess if there is shift in the mean 
for each week. Note that the threshold for each week takes in to account previous
weeks so if there is a sustained shift an alarm will be triggered. 
(Also note that after each alarm the algorithm is reset)

In order to work with the **surveillance** package, we first need to define a 
"surveillance time series" object (using the `sts()` function) to fit within the 
framework. 

```{r surveillance_obj}

## define surveillance time series object
## nb. you can include a denominator with the population object (see ?sts)
counts_sts <- sts(observed = counts$case_int,
                  start = c(
                    ## subset to only keep the year from start_date 
                    as.numeric(str_sub(start_date, 1, 4)), 
                    ## subset to only keep the week from start_date
                    as.numeric(str_sub(start_date, 7, 8))), 
                  ## define the type of data (in this case weekly)
                  freq = 52)

## define the week range that you want to include (ie. prediction period)
## nb. the sts object only counts observations without assigning a week or 
## year identifier to them - so we use our data to define the appropriate observations
weekrange <- cut_off - start_date

```

<!-- ======================================================= -->
#### Farrington method {-}

We then define each of our parameters for the Farrington method in a `list`. 
Then we run the algorithm using `farringtonFlexible()` and then we can extract the 
threshold for an alert using `farringtonmethod@upperbound`to include this in our 
dataset. It is also possible to extract a TRUE/FALSE for each week if it triggered 
an alert (was above the threshold) using `farringtonmethod@alarm`. 

```{r farrington}

## define control
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  b = 9, ## how many years backwards for baseline
  w = 2, ## rolling window size in weeks
  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)
  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)
  trend = TRUE,
  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)
  thresholdMethod = "nbPlugin",
  populationOffset = TRUE
  )

## apply farrington flexible method
farringtonmethod <- farringtonFlexible(counts_sts, ctrl)

## create a new variable in the original dataset called threshold
## containing the upper bound from farrington 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off),
              "threshold"] <- farringtonmethod@upperbound
```

We can then visualise the results in ggplot as done previously. 

```{r plot_farrington, warning=F, message=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
#### GLRNB method {-}

Similarly for the GLRNB method we define each of our parameters for the in a `list`, 
then fit the algorithm and extract the upper bounds.

<span style="color: orange;">**_CAUTION:_** This method uses "brute force" (similar to bootstrapping) for calculating thresholds, so can take a long time!</span>

See the [GLRNB vignette](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) 
for details. 

```{r glrnb, warning = FALSE, message=F}

## define control options
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2020)
  range = which(counts_sts@epoch > weekrange),
  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include
  trend = TRUE,   ## whether to include trend or not
  refit = FALSE), ## whether to refit model after each alarm
  ## cARL = threshold for GLR statistic (arbitrary)
     ## 3 ~ middle ground for minimising false positives
     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)
   c.ARL = 2,
   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak
   ret = "cases"     ## return threshold upperbound as case counts
  )

## apply the glrnb method
glrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)

## create a new variable in the original dataset called threshold
## containing the upper bound from glrnb 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off),
              "threshold_glrnb"] <- glrnbmethod@upperbound

```

Visualise the outputs as previously. 

```{r plot_glrnb, message=F, warning=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold_glrnb, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
## Interrupted timeseries {  }

Interrupted timeseries (also called segmented regression or intervention analysis), 
is often used in assessing the impact of vaccines on the incidence of disease. 
But it can be used for assessing impact of a wide range of interventions or introductions. 
For example changes in hospital procedures or the introduction of a new disease 
strain to a population. 
In this example we will pretend that a new strain of Campylobacter was introduced
to Germany at the end of 2008, and see if that affects the number of cases. 
We will use negative binomial regression again. The regression this time will be 
split in to two parts, one before the intervention (or introduction of new strain here) 
and one after (the pre and post-periods). This allows us to calculate an incidence rate ratio comparing the
two time periods. Explaining the equation might make this clearer (if not then just
ignore!). 

The negative binomial regression can be defined as follows: 

$$\log(Y_t)= β_0 + β_1 \times t+ β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+ + log(pop_t) + e_t$$

Where:
$Y_t$is the number of cases observed at time $t$  
$pop_t$ is the population size in 100,000s at time $t$ (not used here)  
$t_0$ is the last year of the of the pre-period (including transition time if any)  
$δ(x$ is the indicator function (it is 0 if x≤0 and 1 if x>0)  
$(x)^+$ is the cut off operator (it is x if x>0 and 0 otherwise)  
$e_t$ denotes the residual 
Additional terms trend and season can be added as needed. 

$β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+$ is the generalised linear 
part of the post-period and is zero in the pre-period. 
This means that the $β_2$ and $β_3$ estimates are the effects of the intervention. 

We need to re-calculate the fourier terms without forecasting here, as we will use
all the data available to us (i.e. retrospectively). Additionally we need to calculate
the extra terms needed for the regression. 

```{r define_terms_interrupted}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  as_tsibble(index = epiweek) %>% 
  fourier(K = 1)

## define intervention week 
intervention_week <- yearweek("2008-12-31")

## define variables for regression 
counts <- counts %>% 
  mutate(
    ## corresponds to t in the formula
      ## count of weeks (could probably also just use straight epiweeks var)
    # linear = row_number(epiweek), 
    ## corresponds to delta(t-t0) in the formula
      ## pre or post intervention period
    intervention = as.numeric(epiweek >= intervention_week), 
    ## corresponds to (t-t0)^+ in the formula
      ## count of weeks post intervention
      ## (choose the larger number between 0 and whatever comes from calculation)
    time_post = pmax(0, epiweek - intervention_week + 1))

```

We then use these terms to fit a negative binomial regression, and produce a 
table with percentage change. What this example shows is that there was no 
significant change. 

```{r interrupted_regression, warning = FALSE}


## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier + 
    ## add in whether in the pre- or post-period 
    intervention + 
    ## add in the time post intervention 
    time_post
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model)



## show estimates and percentage change in a table
fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE) %>% 
  ## only keep the intervention value 
  filter(term == "intervention") %>% 
  ## change the IRR to percentage change for estimate and CIs 
  mutate(
    ## for each of the columns of interest - create a new column
    across(
      all_of(c("estimate", "conf.low", "conf.high")), 
      ## apply the formula to calculate percentage change
            .f = function(i) 100 * (i - 1), 
      ## add a suffix to new column names with "_perc"
      .names = "{.col}_perc")
    ) %>% 
  ## only keep (and rename) certain columns 
  select("IRR" = estimate, 
         "95%CI low" = conf.low, 
         "95%CI high" = conf.high,
         "Percentage change" = estimate_perc, 
         "95%CI low (perc)" = conf.low_perc, 
         "95%CI high (perc)" = conf.high_perc,
         "p-value" = p.value)
```

As previously we can visualise the outputs of the regression. 

```{r plot_interrupted}

ggplot(observed, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate, col = "Estimate")) + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(intervention_week), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Intervention", 
           x = intervention_week, 
           y = max(observed$upper_pi), 
           angle = 90, 
           vjust = 1
           ) + 
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Estimate" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```


<!-- ======================================================= -->
## Resources {  }

[forecasting: principles and practice textbook](https://otexts.com/fpp3/)  
[EPIET timeseries analysis case studies](https://github.com/EPIET/TimeSeriesAnalysis)  
[Penn State course](https://online.stat.psu.edu/stat510/lesson/1) 
[Surveillance package manuscript](https://www.jstatsoft.org/article/view/v070i10)





```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/time_series.Rmd-->


# Epidemic modeling { }  


<!-- ======================================================= -->
## Overview {  }

There exists a growing body of tools for epidemic modelling that lets us conduct
fairly complex analyses with minimal effort. This section will provide an
overview on how to use these tools to:

* estimate the effective reproduction number R<sub>t</sub> and related statistics
  such as the doubling time
* produce short-term projections of future incidence

It is *not* intended as an overview of the methodologies and statistical methods
underlying these tools, so please refer to the Resources tab for links to some
papers covering this. Make sure you have an understanding of
the methods before using these tools; this will ensure you can accurately
interpret their results.

Below is an example of one of the outputs we'll be producing in this section.

```{r out.width=c('100%', '100%'), fig.show='hold', echo=F, fig.width = 12, fig.height = 9, message=F, warning=F}

## install and load packages
pacman::p_load(tidyverse, EpiNow2, EpiEstim, here, incidence2, epicontacts, rio, projections)

## load linelist
linelist <- import(here::here("data", "linelist_cleaned.rds"))

## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## ## estimate gamma generation time
## generation_time <- bootstrapped_dist_fit(
##   get_pairwise(epic, "date_infection"),
##   dist = "gamma",
##   max_value = 20,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   generation_time,
##   here("data/cache/epidemic_models/generation_time.rds")
## )

## import cached generation time
generation_time <- import(here("data/cache/epidemic_models/generation_time.rds"))

## ## estimate incubation period
## incubation_period <- bootstrapped_dist_fit(
##   linelist$date_onset - linelist$date_infection,
##   dist = "lognormal",
##   max_value = 100,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   incubation_period,
##   here("data/cache/epidemic_models/incubation_period.rds")
## )

## import cached incubation period
incubation_period <- import(here("data/cache/epidemic_models/incubation_period.rds"))

## get incidence from onset date
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())

## ## run epinow
## epinow_res <- epinow(
##   reported_cases = cases,
##   generation_time = generation_time,
##   delays = delay_opts(incubation_period),
##   target_folder = here("data/cache/epidemic_models"),
##   return_output = TRUE,
##   output = "samples",
##   verbose = TRUE,
##   stan = stan_opts(samples = 750, chains = 4),
##   horizon = 21
## )

## ## export for caching
## export(
##   epinow_res,
##   here("data/cache/epidemic_models/epinow_res.rds")
## )

## import cached epinow results
epinow_res <- import(here("data/cache/epidemic_models/epinow_res.rds"))

## plot summary figure
plot(epinow_res)

```

<!-- ======================================================= -->
## Preparation {  }

We will use two different methods and packages for R<sub>t</sub> estimation,
namely **EpiNow** and **EpiEstim**, as well as the **projections** package for
forecasting case incidence.  

This code chunk shows the loading of packages required for the analyses. 
In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. 
You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

	
```{r epidemic_models_packages, eval = TRUE}
pacman::p_load(
   rio,          # File import
   here,         # File locator
   tidyverse,    # Data management + ggplot2 graphics
   epicontacts,  # Analysing transmission networks
   EpiNow2,      # Rt estimation
   EpiEstim,     # Rt estimation
   projections,  # Incidence projections
   incidence2,   # Handling incidence data
   epitrix,      # Useful epi functions
   distcrete     # Discrete delay distributions
)
```
	
We will use the standard, cleaned linelist for all analyses in this section. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r eval=F}
# import the cleaned linelist
linelist <- import("linelist_cleaned.xlsx")
```


<!-- ======================================================= -->
## Estimating R<sub>t</sub> {  }

### EpiNow2 vs. EpiEstim {-}

The reproduction number R is a measure of the transmissibility of a disease and
is defined as the expected number of secondary cases per infected case. In a
fully susceptible population, this value represents the basic reproduction
number R<sub>0</sub>. However, as the number of susceptible individuals in a
population changes over the course of an outbreak or pandemic, and as various
response measures are implemented, the most commonly used measure of
transmissibility is the effective reproduction number R<sub>t</sub>; this is
defined as the expected number of secondary cases per infected case at a given
time _t_.

The **EpiNow2** package provides the most sophisticated framework for estimating
R<sub>t</sub>. It has two key advantages over the other commonly used package,
**EpiEstim**:

* It accounts for delays in reporting and can therefore estimate R<sub>t</sub>
  even when recent data is incomplete.
* It estimates R<sub>t</sub> on _dates of infection_ rather than the dates of
  onset of reporting, which means that the effect of an intervention will
  be immediately reflected in a change in R<sub>t</sub>, rather than with a
  delay.

However, it also has two key disadvantages:

* It requires knowledge of the generation time distribution (i.e. distribution
  of delays between infection of a primary and secondary cases), incubation
  period distribution (i.e. distribution of delays between infection and symptom
  onset) and any further delay distribution relevant to your data (e.g. if you
  have dates of reporting, you require the distribution of delays from symptom
  onset to reporting). While this will allow more accurate estimation of
  R<sub>t</sub>, **EpiEstim** only requires the serial interval distribution
  (i.e. the distribution of delays between symptom onset of a primary and a
  secondary case), which may be the only distribution available to you.
* **EpiNow2** is significantly slower than **EpiEstim**, anecdotally by a factor
  of about 100-1000! For example, estimating R<sub>t</sub> for the sample outbreak
  considered in this section takes about four hours (this was run for a large
  number of iterations to ensure high accuracy and could probably be reduced if
  necessary, however the points stands that the algorithm is slow in
  general). This may be unfeasible if you are regularly updating your
  R<sub>t</sub> estimates.
  
Which package you choose to use will therefore depend on the data, time and
computational resources available to you.

### EpiNow2 {-}

#### Estimating delay distributions {-}

The delay distributions required to run **EpiNow2** depend on the data you
have. Essentially, you need to be able to describe the delay from the date of
infection to the date of the event you want to use to estimate R<sub>t</sub>. If
you are using dates of onset, this would simply be the incubation period
distribution. If you are using dates of reporting, you require the
delay from infection to reporting. As this distribution is unlikely to be known
directly, **EpiNow2** lets you chain multiple delay distributions together; in
this case, the delay from infection to symptom onset (e.g. the incubation
period, which is likely known) and from symptom onset to reporting (which you
can often estimate from the data).

As we have the dates of onset for all our cases in the example linelist, we will
only require the incubation period distribution to link our data (e.g. dates of
symptom onset) to the date of infection. We can either estimate this distribution
from the data or use values from the literature.

A literature estimate of the incubation period of Ebola (taken
from [this paper](https://www.nejm.org/doi/full/10.1056/nejmoa1411100)) with a
mean of 9.1, standard deviation of 7.3 and maximum value of 30 would be
specified as follows:

```{r epidemic_models_incubation_literature, eval=F}
incubation_period_lit <- list(
  mean = log(9.1),
  mean_sd = log(0.1),
  sd = log(7.3),
  sd_sd = log(0.1),
  max = 30
)
```
Note that **EpiNow2** requires these delay distributions to be provided on a **log**
scale, hence the `log` call around each value (except the `max` parameter which,
confusingly, has to be provided on a natural scale). The `mean_sd` and `sd_sd`
define the standard deviation of the mean and standard deviation estimates. As
these are not known in this case, we choose the fairly arbitrary value of 0.1.

In this analysis, we instead estimate the incubation period distribution
from the linelist itself using the function `bootstrapped_dist_fit`, which will
fit a lognormal distribution to the observed delays between infection and onset
in the linelist.

```{r epidemic_models_incubation_estimate, eval=F}
## estimate incubation period
incubation_period <- bootstrapped_dist_fit(
  linelist$date_onset - linelist$date_infection,
  dist = "lognormal",
  max_value = 100,
  bootstraps = 1
)
```

The other distribution we require is the generation time. As we have data on
infection times __and__ transmission links, we can estimate this
distribution from the linelist by calculating the delay between infection times
of infector-infectee pairs. To do this, we use the handy `get_pairwise` function
from the package **epicontacts**, which allows us to calculate pairwise
differences of linelist properties between transmission pairs. We first create an
epicontacts object (see [Transmission chains] page for further
details):

```{r epidemic_models_epicontacts, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in infection times between transmission pairs,
calculated using `get_pairwise`, to a gamma distribution:

```{r epidemic_models_generation_estimate, eval=F}
## estimate gamma generation time
generation_time <- bootstrapped_dist_fit(
  get_pairwise(epic, "date_infection"),
  dist = "gamma",
  max_value = 20,
  bootstraps = 1
)
```

#### Running **EpiNow2** {-}

Now we just need to calculate daily incidence from the linelist, which we can do
easily with the **dplyr** functions `group_by()` and `n()`. Note
that **EpiNow2** requires the column names to  be `date` and `confirm`.

```{r epidemic_models_cases, eval=F}
## get incidence from onset dates
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())
```

We can then estimate R<sub>t</sub> using the `epinow` function. Some notes on
the inputs:

* We can provide any number of 'chained' delay distributions to the `delays`
  argument; we would simply insert them alongside the `incubation_period` object
  within the `delay_opts` function.
* `return_output` ensures the output is returned within R and not just saved to
  a file.
* `verbose` specifies that we want a readout of the progress.
* `horizon` indicates how many days we want to project future incidence for.
* We pass additional options to the `stan` argument to specify how long
  we want to run the inference for. Increasing `samples` and `chains` will give
  you a more accurate estimate that better characterises uncertainty, however
  will take longer to run.

```{r epidemic_models_run_epinow, eval=F}
## run epinow
epinow_res <- epinow(
  reported_cases = cases,
  generation_time = generation_time,
  delays = delay_opts(incubation_period),
  return_output = TRUE,
  verbose = TRUE,
  horizon = 21,
  stan = stan_opts(samples = 750, chains = 4)
)
```

#### Analysing outputs {-}

Once the code has finished running, we can plot a summary very easily as follows:

```{r epidemic_models_plot_epinow, eval=T, fig.width = 12, fig.height = 12}
## plot summary figure
plot(epinow_res)
```

We can also look at various summary statistics:

```{r epidemic_models_epinow_summary, eval=T}
## summary table
epinow_res$summary
```

For further analyses and custom plotting, you can access the summarised daily
estimates via `$estimates$summarised`. We will convert this from the default
`data.table` to a `tibble` for ease of use with **dplyr**.

```{r epidemic_models_to_tibble, eval=F}
## extract summary and convert to tibble
estimates <- as_tibble(epinow_res$estimates$summarised)
estimates
```

```{r epidemic_models_tibble_show, eval=T, echo = F}
## show outputs
estimates <- as_tibble(epinow_res$estimates$summarised)
DT::datatable(
  estimates,
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap'
)
```

As an example, let's make a plot of the doubling time and R<sub>t</sub>. We will
only look at the first few months of the outbreak when R<sub>t</sub> is well
above one, to avoid plotting extremely high doublings times.

We use the formula `log(2)/growth_rate` to calculate the doubling time from the
estimated growth rate.

```{r epidemic_models_plot_epinow_cusotom, eval=T, fig.width = 12, fig.height = 8}

## make wide df for median plotting
df_wide <- estimates %>%
  filter(
    variable %in% c("growth_rate", "R"),
    date < as.Date("2014-09-01")
  ) %>%
  ## convert growth rates to doubling times
  mutate(
    across(
      c(median, lower_90:upper_90),
      ~ case_when(
        variable == "growth_rate" ~ log(2)/.x,
        TRUE ~ .x
      )
    ),
    ## rename variable to reflect transformation
    variable = replace(variable, variable == "growth_rate", "doubling_time")
  )

## make long df for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(R = "R[t]", doubling_time = "Doubling~time"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credibel\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )

```

<!-- ======================================================= -->
### EpiEstim {-}

To run **EpiEstim**, we need to provide data on daily incidence and specify the
serial interval (i.e. the distribution of delays between symptom onset of
primary and secondary cases). 

Incidence data can be provided as a vector, a dataframe or an `incidence`
object from the **incidence2** package, and you can even distinguish between imports
and locally acquired infections; see the documentation at `?estimate_R` for
further details.  

We will create an incidence object. See the page on [Epidemic curves] for more examples with the **incidence** package. The incidence object consists of a tibble with dates and their respective case counts. We use `complete()` to ensure all dates are included (even those with no cases), and then `rename()` the columns to what is expected by `estimate_R()` in a later step.  

```{r epidemic_models_epiestim_incidence, eval=T}
## get incidence from onset date
cases <- incidence2::incidence(linelist, date_index = date_onset) %>% 
  rename(I = count,
         dates = date)
```

The package provides several options for specifying the serial interval, the
details of which are provided in the documentation at `?estimate_R`. We will
cover two of them here.

#### Using serial interval estimates from the literature {-}

Using the option `method = "parametric_si"`, we can manually specify the mean and
standard deviation of the serial interval in a `config` object created using the
function `make_config`. We use a mean and standard deviation of 12.0 and 5.2, respectively, defined in
[this paper](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0):

```{r epidemic_models_epiestim_config, eval=T}
## make config
config_lit <- make_config(
  mean_si = 12.0,
  std_si = 5.2
)
```

We can then estimate R<sub>t</sub> with the `estimate_R` function:

```{r epidemic_models_epiestim_lit, eval=T, warning = FALSE}
epiestim_res_lit <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_lit
)
```

and plot a summary of the outputs:

```{r epidemic_models_epiestim_lit_plot, eval=T, warning = FALSE}
plot(epiestim_res_lit)
```

#### Using serial interval estimates from the data {-}

As we have data on dates of symptom onset _and_ transmission links, we can
also estimate the serial interval from the linelist by calculating the delay
between onset dates of infector-infectee pairs. As we did in the **EpiNow2**
section, we will use the `get_pairwise` function from the **epicontacts**
package, which allows us to calculate pairwise differences of linelist
properties between transmission pairs. We first create an epicontacts object
(see [Transmission chains] page for further details):

```{r epidemic_models_epicontacts_epiestim, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in onset dates between transmission pairs, calculated
using `get_pairwise`, to a gamma distribution. We use the handy `fit_disc_gamma`
from the **epitrix** package for this fitting procedure, as we require a
_discretised_ distribution.

```{r epidemic_models_incubation_estimate_epiestim, eval=T, warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))
```

We then pass this information to the `config` object, run **EpiEstim**
again and plot the results:

```{r epidemic_models_epiestim_emp, eval=T, warning = FALSE}
## make config
config_emp <- make_config(
  mean_si = serial_interval$mu,
  std_si = serial_interval$sd
)

## run epiestim
epiestim_res_emp <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_emp
)

## plot outputs
plot(epiestim_res_emp)
```

#### Specifying estimation time windows {-}

These default options will provide a weekly sliding estimate and might act as a
warning that you are estimating R<sub>t</sub> too early in the outbreak for a
precise estimate. You can change this by setting a later start date for the
estimation as shown below. Unfortunately, **EpiEstim** only provides a very
clunky way of specifying these estimations times, in that you have to provide a
vector of __integers__ referring to the start and end dates for each time
window.

```{r epidemic_models_epiestim_config_late, eval=T}

## define a vector of dates starting on June 1st
start_dates <- seq.Date(
  as.Date("2014-06-01"),
  max(cases$dates) - 7,
  by = 1
) %>%
  ## subtract the starting date to convert to numeric
  `-`(min(cases$dates)) %>%
  ## convert to integer
  as.integer()

## add six days for a one week sliding window
end_dates <- start_dates + 6
  
## make config
config_partial <- make_config(
  mean_si = 12.0,
  std_si = 5.2,
  t_start = start_dates,
  t_end = end_dates
)
```
Now we re-run **EpiEstim** and can see that the estimates only start from June:

```{r epidemic_models_epiestim_config_late_run, eval=T}

## run epiestim
epiestim_res_partial <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_partial
)

## plot outputs
plot(epiestim_res_partial)

```

#### Analysing outputs {-}

The main outputs can be accessed via `$R`. As an example, we will create a plot of
R<sub>t</sub> and a measure of "transmission potential" given by the product of
R<sub>t</sub> and the number of cases reported on that day; this represents the
expected number of cases in the next generation of infection.

```{r epidemic_models_epiestim_plot_full, eval=T, warning = FALSE, fig.width = 12, fig.height = 8}

## make wide dataframe for median
df_wide <- epiestim_res_lit$R %>%
  rename_all(clean_labels) %>%
  rename(
    lower_95_r = quantile_0_025_r,
    lower_90_r = quantile_0_05_r,
    lower_50_r = quantile_0_25_r,
    upper_50_r = quantile_0_75_r,
    upper_90_r = quantile_0_95_r,
    upper_95_r = quantile_0_975_r,
    ) %>%
  mutate(
    ## extract the median date from t_start and t_end
    dates = epiestim_res_emp$dates[round(map2_dbl(t_start, t_end, median))],
    var = "R[t]"
  ) %>%
  ## merge in daily incidence data
  left_join(cases, "dates") %>%
  ## calculate risk across all r estimates
  mutate(
    across(
      lower_95_r:upper_95_r,
      ~ .x*I,
      .names = "{str_replace(.col, '_r', '_risk')}"
    )
  ) %>%
  ## seperate r estimates and risk estimates
  pivot_longer(
    contains("median"),
    names_to = c(".value", "variable"),
    names_pattern = "(.+)_(.+)"
  ) %>%
  ## assign factor levels
  mutate(variable = factor(variable, c("risk", "r")))

## make long dataframe from quantiles
df_long <- df_wide %>%
  select(-variable, -median) %>%
  ## seperate r/risk estimates and quantile levels
  pivot_longer(
    contains(c("lower", "upper")),
    names_to = c(".value", "quantile", "variable"),
    names_pattern = "(.+)_(.+)_(.+)"
  ) %>%
  mutate(variable = factor(variable, c("risk", "r")))

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = dates, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = dates, y = median),
    alpha = 0.2
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(r = "R[t]", risk = "Transmission~potential"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`50` = 0.7, `90` = 0.4, `95` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )
  
```

<!-- ======================================================= -->
## Projecting incidence {  }

### EpiNow2 {-}

Besides estimating R<sub>t</sub>, **EpiNow2** also supports forecasting of
R<sub>t</sub> and projections of case numbers by integration with the
**EpiSoon** package under the hood. All you need to do is specify the `horizon`
argument in your `epinow` function call, indicating how many days you want to
project into the future; see the **EpiNow2** section under the "Estimating
R<sub>t</sub>" for details on how to get **EpiNow2** up and running. In this
section, we will just plot the outputs from that analysis, stored in the
`epinow_res` object.

```{r epidemic_models_episoon, eval=T, fig.width = 12, fig.height = 8, warning = FALSE}

## define minimum date for plot
min_date <- as.Date("2015-03-01")

## extract summarised estimates
estimates <-  as_tibble(epinow_res$estimates$summarised)

## extract raw data on case incidence
observations <- as_tibble(epinow_res$estimates$observations) %>%
  filter(date > min_date)

## extract forecasted estimates of case numbers
df_wide <- estimates %>%
  filter(
    variable == "reported_cases",
    type == "forecast",
    date > min_date
  )

## convert to even longer format for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_histogram(
    data = observations,
    aes(x = date, y = confirm),
    stat = 'identity',
    binwidth = 1
  ) +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  geom_vline(xintercept = min(df_long$date), linetype = 2) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = "Daily reported cases",
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14)

```

### projections {-}

The **projections** package developed by RECON makes it very easy to make short
term incidence forecasts, requiring only knowledge of the effective reproduction
number R<sub>t</sub> and the serial interval. Here we will cover how to use
serial interval estimates from the literature and how to use our own estimates
from the linelist.

#### Using serial interval estimates from the literature {-}

**projections** requires a discretised serial interval distribution of the class
`distcrete` from the package **distcrete**. We will use a gamma distribution
with a mean of 12.0 and and standard deviation of 5.2 defined in
[this paper](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0). To
convert these values into the shape and scale parameters required for a gamma
distribution, we will use the function `gamma_mucv2shapescale` from the
**epitrix** package.

```{r epidemic_models_projections_distcrete, eval=T}

## get shape and scale parameters from the mean mu and the coefficient of
## variation (e.g. the ratio of the standard deviation to the mean)
shapescale <- epitrix::gamma_mucv2shapescale(mu = 12.0, cv = 5.2/12)

## make distcrete object
serial_interval_lit <- distcrete::distcrete(
  name = "gamma",
  interval = 1,
  shape = shapescale$shape,
  scale = shapescale$scale
)

```

Here is a quick check to make sure the serial interval looks correct. We
access the density of the gamma distribution we have just defined by `$d`, which
is equivalent to calling `dgamma`:

```{r epidemic_models_projections_distcrete_plot, eval=T}

## check to make sure the serial interval looks correct
qplot(
  x = 0:50, y = serial_interval_lit$d(0:50), geom = "area",
  xlab = "Serial interval", ylab = "Density"
)

```

#### Using serial interval estimates from the data {-}

As we have data on dates of symptom onset _and_ transmission links, we can
also estimate the serial interval from the linelist by calculating the delay
between onset dates of infector-infectee pairs. As we did in the **EpiNow2**
section, we will use the `get_pairwise` function from the **epicontacts**
package, which allows us to calculate pairwise differences of linelist
properties between transmission pairs. We first create an epicontacts object
(see [Transmission chains] page for further details):

```{r epidemic_models_epicontacts_projections, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in onset dates between transmission pairs, calculated
using `get_pairwise`, to a gamma distribution. We use the handy `fit_disc_gamma`
from the **epitrix** package for this fitting procedure, as we require a
_discretised_ distribution.

```{r epidemic_models_incubation_estimate_projections, eval=T, warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))

## inspect estimate
serial_interval[c("mu", "sd")]
```

#### Projecting incidence {-}

To project future incidence, we still need to provide historical incidence in
the form of an `incidence` object, as well as a sample of plausible
R<sub>t</sub> values. We will generate these values using the R<sub>t</sub>
estimates generated by **EpiEstim** in the previous section (under "Estimating
R<sub>t</sub>") and stored in the `epiestim_res_emp` object. In the code below,
we extract the mean and standard deviation estimates of R<sub>t</sub> for the
last time window of the outbreak (using the `tail` function to access the last
element in a vector), and simulate 1000 values from a gamma distribution using
`rgamma`. You can also provide your own vector of R<sub>t</sub> values that you
want to use for forward projections.

```{r epidemic_models_projection_setup, eval=T, warning = FALSE}

## create incidence object from dates of onset
inc <- incidence::incidence(linelist$date_onset)

## extract plausible r values from most recent estimate
mean_r <- tail(epiestim_res_emp$R$`Mean(R)`, 1)
sd_r <- tail(epiestim_res_emp$R$`Std(R)`, 1)
shapescale <- gamma_mucv2shapescale(mu = mean_r, cv = sd_r/mean_r)
plausible_r <- rgamma(1000, shape = shapescale$shape, scale = shapescale$scale)

## check distribution
qplot(x = plausible_r, geom = "histogram", xlab = expression(R[t]), ylab = "Counts")

```

We then use the `project()` function to make the actual forecast. We specify how
many days we want to project for via the `n_days` arguments, and specify the
number of simulations using the `n_sim` argument.

```{r epidemic_models_make_projection, eval=T}

## make projection
proj <- project(
  x = inc,
  R = plausible_r,
  si = serial_interval$distribution,
  n_days = 21,
  n_sim = 1000
)

```

We can then handily plot the incidence and projections using the `plot()` and
`add_projections()` functions. We can easily subset the `incidence` object to only
show the most recent cases by using the square bracket operator.

```{r epidemic_models_plot_projection, eval=T, fig.width = 12, fig.height = 8, warning = FALSE}

## plot incidence and projections
plot(inc[inc$dates > as.Date("2015-03-01")]) %>%
  add_projections(proj)

```

You can also easily extract the raw estimates of daily case numbers by
converting the output to a dataframe.

```{r epidemic_models_projection_df, eval=F, warning = FALSE}
## convert to data frame for raw data
proj_df <- as.data.frame(proj)
proj_df
```

```{r epidemic_models_projection_dt, eval=T, echo = F}

## convert to data frame for raw data
proj_df <- as.data.frame(proj)

## data table output
DT::datatable(
  proj_df[1:11],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap'
)

```


<!-- ======================================================= -->
## Resources {  }

* [Here is the paper](https://www.sciencedirect.com/science/article/pii/S1755436519300350) describing
  the methodology implemented in **EpiEstim**.
* [Here is the paper](https://wellcomeopenresearch.org/articles/5-112/v1) describing
  the methodology implemented in **EpiNow2**.
* [Here is a paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008409) describing
  various methodological and practical considerations for estimating R<sub>t</sub>.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/epidemic_models.Rmd-->


# Survey analysis { }  

THIS PAGE IS UNDER CONSTRUCTION

<!-- ======================================================= -->
## Overview {  }

<!-- ======================================================= -->
## Preparation {  }

<!-- ======================================================= -->
## Weighting {  }

<!-- ======================================================= -->
## Random selection {  }

<!-- ======================================================= -->
## Resources {  }




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/survey_analysis.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Survival analysis { }  


<!-- ======================================================= -->
## Overview {}


*Survival analysis* focuses on describing for a given individual or group of individuals, a defined point of event called **_the failure_** (occurrence of a disease, cure from a disease, death, relapse after response to treatment...) that occurs after a period of time called **_failure time_** (or  **_follow-up time_** in cohort/population-based studies) during which individuals are observed. To determine the failure time, it is then necessary to define a time of origin (that can be the inclusion date, the date of diagnosis...). 

The target of inference for survival analysis is then the time between an origin and an event.
In current medical research, it is widely used in clinical studies to assess the effect of a treatment for instance, or in cancer epidemiology to assess a large variety of cancer survival measures. 


It is usually expressed through the **_survival probability_** which is the probability that the event of interest has not occurred by a duration t.


**_Censoring_**: Censoring occurs when at the end of follow-up, some of the individuals have not had the event of interest, and thus their true time to event is unknown. We will mostly focus on right censoring here but for more details on censoring and survival analysis in general, you can see references. 


```{r echo=F, eval=F, out.width = "80%", out.height="80%", fig.align = "center"}
 
#Add a figure from the following chunks for the last version of the page
#do not forget to save the output figure in "images"
# knitr::include_graphics(here::here("images", "survanalysis.png"))

```  

<!-- ======================================================= -->
## Preparation {  }

### Load packages {-}  

To run survival analyses in R, one the most widely used package is the **survival** package. We first install it and then load it as well as the other packages that will be used in this section:

In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, echo=F, message=FALSE, warning=FALSE}

# install/load the different packages needed for this page
pacman::p_load(
  survival,      # survival analysis 
  survminer,     # survival analysis
  rio,           # importing data  
  here,          # relative file pathways  
  SemiCompRisks, # dataset examples and advanced tools for working with Semi-Competing Risks data
  tidyverse,     # data manipulation and visualization
  Epi,           # stat analyses in Epi
  survival,      # survival analysis
  survminer      # survival analysis: advanced KM curves
)


```


This page explores survival analyses using the linelist used in most of the previous pages and on which we apply some changes to have a proper survival data.


### Import dataset {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r echo=F}
# import linelist
linelist_case_data <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r eval=F}
# import linelist
linelist_case_data <- rio::import("linelist_cleaned.xlsx")
```

### Data management and transformation {-}

In short, survival data can be described as having the following three characteristics:

1) the dependent variable or response is the waiting time until the occurrence of a well-defined event,
2) observations are censored, in the sense that for some units the event of interest has not occurred at the time the data are analyzed, and 
3) there are predictors or explanatory variables whose effect on the waiting time we wish to assess or control. 

Thus, we will create different variables needed to respect that structure and run the survival analysis.

We define:

- our event of interest as being "death" (hence our survival probability will be the probability of being alive after a certain time after the time of origin),
- the follow-up time (`futime`) as the time between the time of onset and the time of outcome *in days*,
- censored patients as those who recovered or for whom the final outcome is not known ie the event "death" was not observed (`event=0`).

<span style="color: orange;">**_CAUTION:_** Since in a real cohort study, the information on the time of origin and the end of the follow-up is known given individuals are observed, we will remove observations where the date of onset or the date of outcome is unknown. Also the cases where the date of onset is later than the date of outcome will be removed since they are considered as wrong.</span>

<span style="color: darkgreen;">**_TIP:_** Given that filtering to greater than (>) or less than (<) a date can remove rows with missing values, applying the filter on the wrong dates will also remove the rows with missing dates.</span>

We then create from the var `age_cat` another variable `age_cat_small` that indicates reduces the categories of the age groups to 3.

```{r }
#create a new data called linelist_surv from the linelist_case_data

linelist_surv <-  linelist_case_data %>% 
     
  dplyr::filter(
       # remove observations with wrong or missing dates of onset or date of outcome
       date_outcome > date_onset) %>% 
  
  dplyr::mutate(
       # create the event var which is 1 if the patient died and 0 if he was right censored
       event = ifelse(is.na(outcome) | outcome == "Recover", 0, 1), 
    
       # create the var on the follow-up time in days
       futime = as.double(date_outcome - date_onset), 
    
       # create a new age category variable with only 3 strata levels
       age_cat_small = dplyr::case_when( 
            age_years < 5  ~ "0-4",
            age_years >= 5 & age_years < 20 ~ "5-19",
            age_years >= 20   ~ "20+"),
       
       # previous step created age_cat_small var as character.
       # now convert it to factor and specify the levels.
       # Note that the NA values remain NA's and are not put in a level "unknown" for example,
       # since in the next analyses they have to be removed.
       age_cat_small = factor(age_cat_small,
                              levels = c("0-4", "5-19", "20+")))
```


<span style="color: darkgreen;">**_TIP:_** We can verify the new variables we have created by doing a summary on the `futime` and a cross-tabulation between `event` and `outcome` from which it was created. Besides this verification it is a good habit communicating on the median follow-up time when interpreting survival analysis results.</span>

```{r }

summary(linelist_surv$futime)

# cross tabulate the new event var and the outcome var from which it was created
# to make sure the code did what it was intended to
with(linelist_surv, 
     table(outcome, event, useNA = "ifany")
     )


# cross tabulate the new age_cat_small var and the age_cat var from which it was created,
# to make sure the code did what it was intended to

with(linelist_surv, 
     table(age_cat_small, age_cat, useNA = "ifany")
     ) 


# print the 10 first observations of the linelist_surv data looking at specific variables (including those newly created)

head(linelist_surv[,c("case_id", "age_cat_small", "date_onset","date_outcome","outcome","event","futime")], 10)

```

We can also cross-tabule the variable `age_cat_small` and `gender` to have more details on the distribution of this new variable among the gender groups. For this we use the `stat.table()` function of the **Epi** package.

```{r}

Epi::stat.table( 
  #give variables for the cross tabulation
  list(
    gender, 
    age_cat_small
    ),
  
  #precise the function you want to call (mean,count..)
  list( 
    count(),
    percent(age_cat_small)
    ), 
  
  #add margins
  margins=T, 
  
  #data used
  data = linelist_surv 
  )

```


<!-- ======================================================= -->
## Basics of survival analysis {}


### Building a surv-type object {-}

We will first use `Surv()` to build a standard survival object form the follow-up time and the event variables.The result of such a step is to produce an object of type *survival* that focuses on the time information by precising whether or not the event of interest (death) was observed. This is done using a “+” after the time in the print out of *survobj* that indicates right-censoring.

```{r survobj }

survobj <- with(linelist_surv, 
                
                survival::Surv(futime, event)
                
                )

#print the 50 first elements of the vector to see how it presents
head(survobj,50)


```


### Running initial analyses {-}

We then start our analysis using the `survfit()` function to produce a *survfit object*, which fits the default calculations for **_Kaplan Meier_** (KM) estimates of the overall (marginal) survival curve, which are in fact a step function with jumps at observed event times. The final *survfit object*  contains one or more survival curvesis and is created using the *Surv object* as a response variable in the model formul.

<span style="color: black;">**_NOTE:_** The Kaplan-Meier estimate is a nonparametric maximum likelihood estimate (MLE) of the survival function. . (see resources for more information).</span>

The summary of this *survfit object* will give what is called a *life table* that contains:

* for each of the time of the follow-up (`time`) where an event happened and that are ascending ordered, 
* the number of people who were at risk of developing the event (people who did not have the event yet nor were censored: `n.risk`),
* those who did develop it  (`n.event`), 
* and from this, the probability of not developing the event  (probability of not dying or of surviving past that specific time ).
* Finally the standard error and the confidence interval for that probability are derived.


```{r fit}

#fit the KM estimates using the formula where the previously Surv object "survobj" is the response variable. "~ 1" precises we run the model for the overall survival.

linelistsurv_fit <-  survival::survfit(
  survobj ~ 1
  )

#print its summary for more details
summary(linelistsurv_fit)

```


While using `summary()` we can add the option `times` and  precise the specific times at which we want to see the survival information 

```{r print_spec_times}

#print its summary at specific times
summary(
  linelistsurv_fit,
        times=c(5,10,20,30,60)
        )

```


We can also use the `print()` function. The `print.rmean=TRUE` argument is used to obtain the mean survival time and its standard error (se).

<span style="color: black;">**_NOTE:_** The restricted mean survival time (RMST) is a specific survival measure more and more used in cancer survival analysis and which is often defined as the area under the survival curve given we observe patients up to restricted time T: more details in resources</span>


```{r, mean_survtime}

#print the linelistsurv_fit object and ask for information on the mean survival time and its se. 
print(
  linelistsurv_fit, 
      print.rmean = TRUE
      )

```


<span style="color: darkgreen;">**_TIP:_** We can create the *surv object* directly in the `survfit()` function and save a line of code. This will then give `linelistsurv_quick <-  survfit(Surv(futime, event) ~ 1, data=linelist_surv)`. But as you have seen, in such case we have to precise the data where the variables time and event are taken from.</span>

Besides the `summary()` function, we can also use the `str()` function that gives more details on the structure of the `survfit()` object. Among those details is an important one: *cumhaz* which allows for instance to plot the **_cumulative hazard_**, with the **_hazard_** being the **_instantaneous rate of event occurrence_** (see references).

```{r fit_struct}

print(
  str(linelistsurv_fit)
      )

```

<!-- ======================================================= -->
### Plotting Kaplan-Meir curves  {-}

Once the KM estimates are fitted, we can visualize that probability of being alive through the time using the basic `plot()` function that draws the so-known "Kaplan-Meier curve". In other words the curve below is a conventional illustration of the survival experience in the whole patient group.

We can easily verify the follow-up time min and max on the curve. 

An easy way to interpret it is to say that at time zero, all the participants are still alive: survival probability is then 100%. Then it decreases over time as patients die. The proportion of participants surviving past 60 days of f-u is around 40%.

```{r }

plot(linelistsurv_fit, 
     xlab = "Days of follow-up",    #xaxis label
     ylab="Survival Probability",   #yaxis label
     main= "Overall survival curve" #figure title
     )

```

The confidence interval of the KM estimates of the survival are also plotted by default and can be dismissed by adding the option `conf.int=FALSE` to the `plot()` command.

Since the event of interest is "death", drawing a curve describing the complements of the survival proportions will lead to drawing the cumulative mortality proportions.


```{r }

plot(
     linelistsurv_fit,
     xlab = "Days of follow-up",       
     ylab="Survival Probability",       
     mark.time=TRUE,              #mark times of events to facilitate reading of the curve: a "+" sign is printed on the curve at every event
     conf.int=FALSE,             #do not plot the confidence interval
     main= "Overall survival curve and cumulative mortality"
     )



#draw an additional curve to the previous plot
lines( 
      linelistsurv_fit, 
      lty=3,          #use a different line type to differenciate between the two curves and for legend clarity purposes
      fun = "event", #draw the cumulative events instead of the survival 
      mark.time=FALSE, 
      conf.int=FALSE 
      )

#add a legend to the plot
legend("topright", #position of the legend in the plot
       legend=c("Survival","Cum. Mortality"), #legend text 
       lty = c(1,3), #line types to use in the legend, should follow linetype used to draw the two curves
       cex=.85, #factor that defines size of the legend text
       bty = "n" #no box type to be drawn for the legend
       )

```

<!-- ======================================================= -->
## Comparison of survival curves 

To compare the survival within different groups of our observed participants or patients, we might need to first look at their respective survival curves and then run tests to evaluate the difference between independent groups. This comparison can concern groups based on gender, age, treatment, comorbidity...

### Log rank test {-}

The log rank test is a popular test that compares the entire survival experience between two or more *independent* groups and can be thought of as a test of whether the survival curves are identical (overlapping) or not (null hypothesis of no difference in survival between the groups). The `survdiff()` function of the **survival package** allows running the log-rank test when we specify `rho=0` (which is the default). The test results gives a chi-square statistic along with a p-value since the log rank statistic is approximately distributed as a chi-square test statistic.

We first try to compare the survival curves by gender group. For this, we first try to visualize it (check whether the two survival curves are overlapping). A new *survfit object*  will be created with a slightly different formula. Then the *survdiff object* will be created.

```{r comp_surv, warning=FALSE}

#create the new survfit object based on gender
linelistsurv_fit_sex <-  survfit(
  
              Surv(futime, event) ~ gender, #formula to create the survival curve: ~ gender indicates we no longer plot the overall survival but based on gender
              data = linelist_surv #data to use 
              )


#plot the survival curves by gender: have a look at the order of the strata level in the gender var before defining your colors
col_sex <- c("lightgreen", "darkgreen")

plot(linelistsurv_fit_sex,
     col=col_sex,
     xlab = "Days of follow-up", 
     ylab="Survival Probability"
     )

legend("topright", 
       legend=c("Female","Male"), 
       col =col_sex,
       lty = 1, cex=.9, bty = "n" 
       )

#compute the test of the difference between the survival curves
survival::survdiff(
          Surv(futime, event) ~ gender, 
          data = linelist_surv
         )

```

We see that the survival curve for women and the one for men overlap up to 15 days of follow-up and then women seem to have a slightly better survival. Yet the log-rank test does not gives enough evidence of a statistical difference between the survival for women and the survival for Men at `\alpha= 0.05`.


Some packages allow illustrating survival curves for different groups and testing the difference at once. Using the `ggsurvplot()` function from the *survminer* package, we can add in our curve the print of the risk tables for each group as well the p-value from the log-rank test. 

We find back the p-value that was found in the previous step.

<span style="color: orange;">**_CAUTION:_** **survminer** functions require since the latest versions, specifying again the data used to fit the survival object. Remember doing this to avoid non-specific error messages. </span>

```{r, warning=F, message=F}

survminer::ggsurvplot(
  
    linelistsurv_fit_sex, 
    data= linelist_surv, #precise again the data used to fit the linelistsurv_fit_sex even though it is already precised in that object
    conf.int = F, #do not show confidence interval of KM estimates
    surv.scale = "percent",  #present probabilities in the y axis in %
    break.time.by=10, #present the time axis with an increment of 10 days
    xlab = "Follow-up days", ylab= "Survival Probability",
    pval=T, pval.coord= c(40,.91),  #print p-value of Log-rank test and at the position with these coordinates
    risk.table=T,  #print the risk table 
    legend.title = "Gender",
    legend.labs = c("Female","Male"), font.legend = 10, #legend characteristics
    palette = "Dark2", #existing palette name precised,
    surv.median.line = "hv", #draw a line to the median survival
    ggtheme = theme_light()
)

```


We can then look for a difference in the source of the contamination. In this case, the Log rank test gives enough evidence of a difference in the survival probabilities at `\alpha= 0.005`.
The survival probabilities for patients that got infected in funerals are higher than the survival probabilities for patients that got infected in other places, suggesting a survival benefit.

```{r}

linelistsurv_fit_source <-  survfit(
              Surv(futime, event) ~ source,
              data = linelist_surv
              )

ggsurvplot( 
      linelistsurv_fit_source, data= linelist_surv,
      size=1, linetype = "strata",
      conf.int = T, 
      surv.scale = "percent",  
      break.time.by=10, 
      xlab = "Follow-up days", ylab= "Survival Probability",
      pval=T, pval.coord= c(40,.91),  
      risk.table=T,
      legend.title = "Source of \ninfection", legend.labs = c("Funeral","Other"), 
      font.legend = 10,
      palette = c("#E7B800","#3E606F"),
      surv.median.line = "hv", 
      ggtheme = theme_light()
)

```

<!-- ======================================================= -->
## Cox regression analysis {}

Cox proportional hazards regression is one of the most popular regression techniques for survival analysis. Other models  can also be used since the Cox model requires *important assumptions* that need to be verified for an appropriate use such as the proportional hazards assumption: see references. 

In a Cox proportional hazards regression model, the measure of effect is the **_hazard rate_** (HR), which is the risk of failure (or the risk of death in our example), given that the participant has survived up to a specific time.  Usually, we are interested in comparing *independent* groups with respect to their hazards, and we use a hazard ratio, which is analogous to an odds ratio in the setting of multiple logistic regression analysis. The `cox.ph()` from the **survival** package is used to fit the model.
The function `cox.zph()` from **survival** package may be used to test the proportional hazards assumption for a Cox regression model fit. 



<span style="color: black;">**_NOTE:_** A probability must lie in the range 0 to 1. However, the hazard represents the expected number of events per one unit of time. 

* If the hazard ratio for a predictor is close to 1 then that predictor does not affect survival,
* if the HR is less than 1, then the predictor is protective (i.e., associated with improved survival),
* and if the HR is greater than 1, then the predictor is associated with increased risk (or decreased survival).</span> 

### Fitting a Cox model {-}

We can first fit a model to assess the effect of age and gender on the survival. By just printing the model, we have the information on:

  + the estimated regression coefficients (`coef`) which quantifies the association between the predictors and the outcome,
  + their exponential (for interpretability, `exp(coef)`) which produces the *hazard ratio*,
  + their standard error (`se(coef)`),
  + the z-score: how many standard errors is the estimated coefficient away from  0,
  + and the p-value:  the propability that the estimated coefficient could be 0.
  
  The `summary()` function applied to the cox model object gives more info such as the confidence interval of the estimated HR and the different test scores.

The effect of the first covariate `gender`  is presented in the first row. `genderm` is printed stating that the first strata level ("f") i.e the female group is the reference group for the gender. Thus the interpretation of the test parameter is that of men compared to women. The p-value  indicates there was no enough evidence of an effect of the gender on the expected hazard or of an association between gender and all-cause mortality.

The same lack of evidence is noted regarding age-group.

```{r coxmodel_agesex}

#fitting the cox model
linelistsurv_cox_sexage <-  survival::coxph(
              Surv(futime, event) ~ gender + age_cat_small, 
              data = linelist_surv
              )


#printing the model fitted
linelistsurv_cox_sexage


#summary of the model
summary(linelistsurv_cox_sexage)

```


It was interesting to run the model and look at the results but a first look to verify whether the proportional hazards assumptions is respected could help saving time.

```{r test_assumption}

test_ph_sexage <- survival::cox.zph(linelistsurv_cox_sexage)
test_ph_sexage

```


<span style="color: black;">**_NOTE:_** A second argument called *method* can be specified when computing the cox model. It is the determines how ties are handled. The *default* is "efron", and the other options are "breslow" and "exact".</span>

In another model we add more risk factors such as the source of infection and the number of days between date of onset and admission. This time, we first  verify the proportional hazards assumption before going forward.

In this model, we have included a continuous predictor (`days_onset_hosp`). In this case we interpret the parameter estimates as the increase in the expected log of the relative hazard for each one unit increase in the predictor, holding other predictors constant. We first verify the proportional hazards assumption.  The graphical verification of this assumption may be performed with the function `ggcoxzph()` from the *survminer* package. 

```{r coxmodel_fit_ph,  message=FALSE}

#fit the model
linelistsurv_cox <-  coxph(
                        Surv(futime, event) ~ gender + age_years+ source + days_onset_hosp,
                        data = linelist_surv
                        )


#test the proportional hazard model
linelistsurv_ph_test <- cox.zph(linelistsurv_cox)
linelistsurv_ph_test
survminer::ggcoxzph(linelistsurv_ph_test)

```


The model results indicates there is a negative association between onset to admission duration and all-cause mortality. The expected hazard is 0.9 times lower in a person who who is one day later admitted than another, holding gender constant. Or in a more straightforward explanation, a one unit increase in the duration of onset to admission is associated with a 10.7% (`coef *100`) decrease in the risk of death.

Results show also a positive association between the source of infection and the all-cause mortality. Which is to say there is an increased risk of death (1.21x) for patients that got a source of infection other than funerals.


```{r coxmodel_summary,  message=FALSE}

#print the summary of the model
summary(linelistsurv_cox)

```


<!-- ======================================================= -->

### Forest plots {-}

We can then visualize the results of the cox model using the practical forest plots with the `ggforest()` function of the **survminer package**.

```{r forestp}

ggforest(linelistsurv_cox, data = linelist_surv)

```

<!-- ======================================================= -->
## Time-dependent covariates in survival models {}

Some of the following sections have been adapted with permission from an excellent [introduction to survival analysis in R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html) by [Dr. Emily Zabor](https://www.emilyzabor.com/) 

In the last section we covered using Cox regression to examine associations between covariates of interest and survival outcomes.But these analyses rely on the covariate being measured at baseline, that is, before follow-up time for the event begins.

What happens if you are interested in a covariate that is measured **after** follow-up time begins? Or, what if you have a covariate that can change over time?

For example, maybe you are working with clinical data where you repeated measures of hospital laboratory values that can change over time. This is an example of a **Time Dependent Covariate**. In order to address this you need a special setup, but fortunately the cox model is very flexible and this type of data can also be modeled with tools from the **survival** package. 

### Time-dependent covariate setup {-} 

Analysis of time-dependent covariates in R requires setup of a special dataset. If interested, see the more detailed paper on this by the author of the **survival** package [Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf).

For this, we'll use a new dataset from the `SemiCompRisks` package named `BMT`, which includes data on 137 bone marrow transplant patients. The variables we'll focus on are:  

* `T1`  - time (in days) to death or last follow-up  
* `delta1` - death indicator; 1-Dead, 0-Alive  
* `TA` -  time (in days) to acute graft-versus-host disease  
* `deltaA` -  acute graft-versus-host disease indicator;  
  * 1 - Developed acute graft-versus-host disease  
  * 0 - Never developed acute graft-versus-host disease

We'll load this dataset from the **survival** package using the **base** R command `data()`, which can be used for loading data that is already included in a R package that is loaded. The data frame `BMT` will appear in your R environment.  

```{r}
data(BMT, package = "SemiCompRisks")
```

#### Add unique patient identifier {-}  

There is no unique ID column in the `BMT` data, which is needed to create the type of dataset we want. So we use the function `rowid_to_column()` from the **tidyverse** package **tibble** to create a new id column called `my_id` (adds column at start of data frame with sequential row ids, starting at 1). We name the data frame `bmt`.  

```{r}
bmt <- rowid_to_column(BMT, "my_id")
```

The dataset now looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(bmt, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Expand patient rows {-}  

Next, we'll use the `tmerge()` function with the `event()` and `tdc()` helper functions to create the restructured dataset. Our goal is to restructure the dataset to create a separate row for each patient for each time interval where they have a different value for `deltaA`. In this case, each patient can have at most two rows depending on whether they developed acute graft-versus-host disease during the data collection period. We'll call our new indicator for the development of acute graft-versus-host disease `agvhd`.

- `tmerge()` creates a long dataset with multiple time intervals for the different covariate values for each patient
- `event()` creates the new event indicator to go with the newly-created time intervals
- `tdc()` creates the time-dependent covariate column, `agvhd`, to go with the newly created time intervals

```{r}
td_dat <- 
  tmerge(
    data1 = bmt %>% select(my_id, T1, delta1), 
    data2 = bmt %>% select(my_id, T1, delta1, TA, deltaA), 
    id = my_id, 
    death = event(T1, delta1),
    agvhd = tdc(TA)
    )
```

To see what this does, let's look at the data for the first 5 individual patients.

The variables of interest in the original data looked like this:

```{r}
bmt %>% 
  select(my_id, T1, delta1, TA, deltaA) %>% 
  filter(my_id %in% seq(1, 5))
```

The new dataset for these same patients looks like this:

```{r}
td_dat %>% 
  filter(my_id %in% seq(1, 5))
```

Now some of our patients have two rows in the dataset corresponding to intervals where they have a different value of our new variable, `agvhd`. For example, Patient 1 now has two rows with a `agvhd` value of zero from time 0 to time 67, and a value of 1 from time 67 to time 2081. 

### Cox regression with time-dependent covariates {-} 

Now that we've reshaped our data and added the new time-dependent `aghvd` variable, let's fit a simple single variable cox regression model. We can use the same `coxph()` function as before, we just need to change our `Surv()` function to specify both the start and stop time for each interval using the `time1 = ` and `time2 = ` arguments. 


```{r}
bmt_td_model = coxph(
  Surv(time = tstart, time2 = tstop, event = death) ~ agvhd, 
  data = td_dat
  )

summary(bmt_td_model)
```

Again, we'll visualize our cox model results using the `ggforest()` function from the **survminer package**.:

```{r}

ggforest(bmt_td_model, data = td_dat)

```

As you can see from the forest plot, confidence interval, and p-value, there does not appear to be a strong association between death and acute graft-versus-host disease in the context of our simple model. 

<!-- ======================================================= -->
## Resources {  }

[Survival Analysis Part I: Basic concepts and first analyses](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/)

[Survival Analysis in R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html)

[Survival analysis in infectious disease research: Describing events in time](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2954271/)

[Chapter on advanced survival models Princeton](https://data.princeton.edu/wws509/notes/c7.pdf)

[Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf)

[Survival analysis cheatsheet R](https://publicifsv.sund.ku.dk/~ts/survival/survival-cheat.pdf)

[Survminer cheastsheet](https://paulvanderlaken.files.wordpress.com/2017/08/survminer_cheatsheet.pdf)

[Paper on different survival measures for cancer registry data with Rcode provided as supplementary materials](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6322561/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/survival_analysis.Rmd-->

# GIS basics { }  


<!-- ======================================================= -->
## Overview {  }

Spatial aspects of your data can provide a lot of insights into the situation of the outbreak, and to answer questions such as: 

* Where are the current disease hotspots?
* How have the hotspots have changed over time?
* How is the access to health facilities? Are any improvements needed?

In this section, we will explore basic spatial data visualization methods using **tmap** and **ggplot2** packages.
We will also walk through some of the basic spatial data management and querying methods with the **sf** package.

Here are some example outputs:  

**Choropleth map**  

```{r, fig.align = "center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "gis_choropleth.png"))
```

**Case density heatmap**  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_heatmap.png"))
```

**Health facility catchment areas**

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_hf_catchment.png"))
```



<!-- ======================================================= -->
## Preparation {  }

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,          # to import data
  here,         # to locate files
  tidyverse,    # to clean, handle, and plot the data (includes ggplot2 package)
  sf,           # to manage spatial data using a Simple Feature format
  tmap,         # to produce simple maps, works for both interactive and static maps
  janitor,      # to clean column names
  OpenStreetMap # to add OSM basemap in ggplot map
  ) 
                  
```



### Sample case data {-}

For demonstration purposes, we will work with a random sample of 1000 cases from the simulated Ebola epidemic `linelist` dataframe (computationally, working with fewer cases is easier to display in this handbook). If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page.  

The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import clean case linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))  
```

```{r, eval=F}
# import clean case linelist
linelist <- import("linelist_cleaned.xlsx")  
```

Next we select a random sample of 1000 rows using `sample()` from **base** R.   

```{r}
# generate 1000 random row numbers, from the number of rows in linelist
sample_rows <- sample(nrow(linelist), 1000)

# subset linelist to keep only the sample rows, and all columns
linelist <- linelist[sample_rows,]
```

Now we want to convert this `linelist` which is class dataframe, to an object of class "sf" (spatial features). Given that the linelist has two columns "lon" and "lat" representing the longitude and latitude of each case's residence, this will be easy.  

We use the package **sf** (spatial features) and its function `st_as_sf()` to create the new object we call `linelist_sf`. This new object look essentially the same as the linelist, but the columns `lon` and `lat` have been designated as coordinate columns, and a coordinate reference system (CRS) has been assigned for when the points are displayed.   

```{r}
# Create sf object
linelist_sf <- linelist %>%
     sf::st_as_sf(coords = c("lon", "lat"), crs = 4326)
```




### Admin boundary shapefiles {-}  

**Sierra Leone: Admin boundary shapefiles**  

In advance, we have downloaded all administrative boundaries for Sierra Leone from the Humanitarian Data Exchange (HDX) [website here](https://data.humdata.org/dataset/sierra-leone-all-ad-min-level-boundaries).  

Now we are going to do the following to save the Admin Level 3 shapefile in R:  

1) Import the shapefile  
2) Clean the column names  
3) Filter rows to keep only areas of interest  

To import a shapefile we use the `read_sf()` function from **sf**. It is provided the filepath via `here()`. - in this case the file is within our R project in the "Data" and "shp" subfolders, with filename "sle_adm3.shp" (see pages on [Import and export] and [R projects] for more information).  

```{r}
sle_adm3_raw <- sf::read_sf(here::here("data", "shp", "sle_adm3.shp"))
```


Next we use `clean_names()` from the **janitor** package to standardize the column names of the shapefile. We also use `filter()` to keep only the rows with admin2name of "Western Area Urban" or "Western Area Rural".    

```{r}
# ADM3 level clean
sle_adm3 <- sle_adm3_raw %>%
  janitor::clean_names() %>% # standardize column names
  filter(admin2name %in% c("Western Area Urban", "Western Area Rural")) # filter to keep certain areas
```

Below you can see the how the shapefile looks after import and cleaning. *Scroll to the right* to see how there are columns with admin level 0 (country), admin level 1, admin level 2, and finally admin level 3. Each level has a character name and a pcode unique identifier code. The pcode expands with each increasing admin level e.g. SL (Sierra Leone) -> SL04 (Western) -> SL0410 (Western Area Rural) -> SL040101 (Koya Rural).  

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(sle_adm3, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Population data {-}  

**Sierra Leone: Population by ADM3**  

Again, we import data that we have downloaded from HDX (link [here](https://data.humdata.org/dataset/sierra-leone-population)). This time we use `import()` to load the .csv file. We also pass the imported file to `clean_names()` to standardize the column names.   

```{r}
# Population by ADM3
sle_adm3_pop <- rio::import(here::here("data/population", "sle_admpop_adm3_2020.csv")) %>%
  janitor::clean_names()
```

Here is what the populaton file looks like. Scroll to the right to see how each jurisdiction has columns with `male` population, `female` populaton, `total` population, and the population break-down in columns by age group.  

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_adm3_pop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





### Health Facilities {-}

**Sierra Leone: Health facility data from OpenStreetMap**  

Again we have downloaded the locations of health facilities from HDX [here](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities).  

We import their shapefile with `read_sf()`, again clean the column names, and then filter to keep only the points tagged as either "hospital", "clinic", or "doctors".  


```{r}
# OSM health facility shapefile
sle_hf <- sf::read_sf(here::here("data/shp", "sle_hf.shp")) %>% 
  janitor::clean_names() %>%
  filter(amenity %in% c("hospital", "clinic", "doctors"))
```

Here is the resulting dataframe here - *scroll right* to see the facility name and coordinates.  

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Plotting coordinates {  }

The easiest way to plot X-Y coordinates (longitude/latitude, points) is to draw them as points directly from the `linelist_sf` object which we created in the preparation section.

The package **tmap** offers simple mapping capabilities for both static ("plot" mode) and interactive ("view" mode) with just a few lines of code. The **tmap** syntax is similar to that of *ggplot2**, such that commands are added to each other with `+`. Read more detail in this [vignette](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html). 


1) set the **tmap** mode. In this case we will use "plot" mode, which produces static outputs.  

```{r, warning = F, message=F}
tmap_mode("plot") # choose either "view" or "plot"
```

Below, the points are plotted alone.`tm_shape()` is provided with the `linelist_sf` objects. We then add points via `tm_dots()`, specifying the size and color. Because `linelist_sf` is an sf object, we have already designated the two columns that contain the lat/long coordinates and the coordinate reference system (CRS): 


```{r, warning = F, message=F}
# Just the cases (points)
tm_shape(linelist_sf) + tm_dots(size=0.08, col='blue')
```

Alone, the points do not tell us much. So we should also map the administrative boundaries:  

Again we use `tm_shape()` (see [documentation](https://www.rdocumentation.org/packages/tmap/versions/3.3/topics/tm_shape)) but instead of providing the case points shapefile, we provide the administrative boundary shapefile (polygons).  

With the `bbox = ` argument (bbox stands for "bounding box") we can specify the coordinate boundaries. First we show the map display without `bbox`, and then with it.  

```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Just the administrative boundaries (polygons)
tm_shape(sle_adm3) +               # admin boundaries shapefile
  tm_polygons(col = "#F7F7F7")+    # show polygons in light grey
  tm_borders(col = "#000000",      # show borders with color and line weight
             lwd = 2) +
  tm_text("admin3name")            # column text to display for each polygon


# Same as above, but with zoom from bounding box
tm_shape(sle_adm3,
         bbox = c(-13.3, 8.43,    # corner
                  -13.2, 8.5)) +  # corner
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")

```


And now both points and polygons together:  

```{r, warning=F, message=FALSE}
# All together
tm_shape(sle_adm3, bbox = c(-13.3, 8.43, -13.2, 8.5)) +
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")+
tm_shape(linelist_sf) +
  tm_dots(size=0.08, col='blue') 
```


To read a good comparison of mapping options in R, see this [blog post](https://rstudio-pubs-static.s3.amazonaws.com/324400_69a673183ba449e9af4011b1eeb456b9.html).  




<!-- ======================================================= -->
## Spatial joins {}




### Points in polygon {-}
**Spatial assign administrative units to cases**

The case linelist does not contain any information about the administrative units of the cases. Although it is ideal to collect such information during the initial data collection phase, we can also assign administrative units to individual cases based on their spatial relationships (i.e. point intersects with a polygon).  

The **sf** package offers various methods for spatial joins. See more documentation about the st_join method and spatial join types in this [reference](https://r-spatial.github.io/sf/reference/geos_binary_pred.html).  

Below, we will spatially intersect our case locations (points) with the ADM3 boundaries (polygons):  

1) Begin with the linelist (points)  
2) Spatial join to the boundaries, setting the type of join at "st_intersects"  
3) Use `select()` to keep only certain of the new administrative boundary columns  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3,   join = st_intersects)
```

All the columns from `sle_adms` have been added to the linelist! Each case now has columns detailing it's administrative units. For this example, we only want to keep two of the new columns, so we `select()` the old column names and just the two additional of interest:  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3, join = st_intersects) %>% 
  
  # Keep the old column names and two new admin ones of interest
  select(names(linelist_sf), admin3name, admin3pcod)
```

Below, just for display purposes you can see the first ten cases and that their admin level 3 (ADM3) jurisdictions that have been attached, based on where the point spatially intersected with the polygon shapes.    

```{r, warning=F, message=F}
# Now you will see the ADM3 names attached to each case
linelist_adm %>% select(case_id, admin3name, admin3pcod)
```

Now we can describe our cases by administrative unit - something we were not able to do before the spatial join!  

```{r, warning=F, message=F}
# Make new dataframe containing counts of cases by administrative unit
case_adm3 <- linelist_adm %>%          # begin with linelist with new admin cols
  as_tibble() %>%                      # convert to tibble for better display
  group_by(admin3pcod, admin3name) %>% # group by admin unit, both by name and pcode 
  summarise(cases = n()) %>%           # summarize and count rows
  arrange(desc(cases))                     # arrange in descending order

case_adm3
```

We can also create a bar plot of case counts by administrative unit.  

In this example, we begin the `ggplot()` with the `linelist_adm`, so that we can apply factor functions like `fct_infreq()` which orders the bars by frequency (see page on [Factors] for tips).  

```{r, warning=F, message=F}
ggplot(
  data = linelist_adm,                       # begin with linelist containing admin unit info
  aes(x = fct_rev(fct_infreq(admin3name))))+ # x-axis is admin units, ordered by frequency (reversed)
  geom_bar()+                                # create bars, height is number of rows
  coord_flip()+                              # flip X and Y axes for easier reading of adm units
  theme_classic()+                           # simplify background
  labs(                                      # titles and labels
    x = "Admin level 3",
    y = "Number of cases",
    title = "Number of cases, by adminstative unit",
    caption = "As determined by a spatial join, from 1000 randomly sampled cases from linelist"
  )
```


<!-- ======================================================= -->
### Nearest neighbor {-}

**Finding the nearest health facility / catchment area**  

It might be useful to know where the health facilities are located in relation to the disease hot spots.

We can use the *st_nearest_feature* join method from the `st_join()` function (**sf** package) to visualize the closest health facility to individual cases.  

1) We begin with the shapefile linelist `linelist_sf`  
2) We spatially join with `sle_hf`, which is the locations of health facilities and clinics (points)  

```{r, warning=F, message=F}
# Closest health facility to each case
linelist_sf_hf <- linelist_sf %>%                  # begin with linelist shapefile  
  st_join(sle_hf, join = st_nearest_feature) %>%   # data from nearest clinic joined to case data 
  select(case_id, osm_id, name, amenity)           # keep columns of interest, including id, name, type, and geometry of healthcare facility

```

We can see below (first 50 rows) that the each case now has data on the nearest clinic/hospital  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist_sf_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


We can see that "Den Clinic" is the closest health facility for about ~30% of the cases.

```{r}
# Count cases by health facility
hf_catchment <- linelist_sf_hf %>%    # begin with linelist including nearest clinic data
  as.data.frame() %>%                 # convert from shapefile to dataframe
  group_by(name) %>%                  # group by name of clinic
  summarise(case_n = n()) %>%         # count number of rows per clinic 
  arrange(desc(case_n))               # arrange in descending order

hf_catchment                          # print to console
```

To visualize the results, we can use **tmap** - this time interactive mode for easier viewing  

```{r, warning=F, message=F}
tmap_mode("view")   # set tmap mode to interactive  

# plot the cases and clinic points 
tm_shape(linelist_sf_hf) +            # plot cases
  tm_dots(size=0.08, col='name') +    # cases colored by closest clinic
tm_shape(sle_hf) +                    # plot clinic facilities  
  tm_dots(size=0.3, col='red') +      # red large dots
  tm_text("name") +                   # overlay with name of facility
tm_view(set.view = c(-13.2284, 8.4699, 13), # adjust zoom (center coords, zoom)
        set.zoom.limits = c(13,14))
```


### Buffers {-} 

We can also explore how many cases are located within 2.5km (~30 mins) walking distance from the closest health facility.

*Note: For more accurate distance calculations, it is better to re-project your sf object to the respective local map projection system such as UTM (Earth projected onto a planar surface). In this example, for simplicity we will stick to the World Geodetic System (WGS84) Geograhpic coordinate system (Earth represented in a spherical / round surface, therefore the units are in decimal degrees). We will use a general conversion of: 1 decimal degree = ~111km.*  


See more information about map projections and coordinate systems at this [esri article](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/).  


**First**, create a circular buffer with a radius of ~2.5km around each health facility. This is done with the function `st_buffer()` from **tmap**. Because the units of the map is lat/long decimal degrees, that is how "0.02" is interpreted. If your map coordinate system is in meters, the number must be provided in meters.  

```{r, warning=F, message=F}
sle_hf_2k <- sle_hf %>%
  st_buffer(dist=0.02)       # decimal degrees translating to approximately 2.5km 
```

Below we plot the buffer zones themselves:  

```{r, warning=F, message=F}
tmap_mode("plot")
# buffers
tm_shape(sle_hf_2k) +
  tm_borders(col = "red", lwd = 2)
```


**Second*, we intersect these buffers with the cases (points) using `st_join()` and the join type of *st_intersects*. That is, the data from the buffers are joined to the points that they intersect with. 

```{r, warning=F, message=F}
# Intersect the cases with the buffers
linelist_sf_hf_2k <- linelist_sf_hf %>%
  st_join(sle_hf_2k, join = st_intersects, left = TRUE) %>%
  filter(osm_id.x==osm_id.y | is.na(osm_id.y)) %>%
  select(case_id, osm_id.x, name.x, amenity.x, osm_id.y)
```

Now we can count the results: `r nrow(linelist_sf_hf_2k[is.na(linelist_sf_hf_2k$osm_id.y),])` out of 1000 cases did not intersect with any buffer (that value is missing), and so live more than 30 mins walk from the nearest health facility.

```{r}
linelist_sf_hf_2k %>% 
  filter(is.na(osm_id.y)) %>% # empty column - did not join to any buffer
  nrow()
```

We can visualize the results such that cases that did not intersect with any buffer appear in red.  

```{r, fig.width = 5, fig.height = 3, warning=F, message=F}
tmap_mode("view")

# cases
tm_shape(linelist_sf_hf) +
  tm_dots(size=0.08, col='name') +
# buffers
tm_shape(sle_hf_2k) +
  tm_borders(col = "red", lwd = 2) +

# cases outside buffers
tm_shape(linelist_sf_hf_2k %>%  filter(is.na(osm_id.y))) +
  tm_dots(size=0.1, col='red') +
tm_view(set.view = c(-13.2284,8.4699, 13), set.zoom.limits = c(13,14))
```


### Other spatial joins {-}  

Alternative values for argument `join` include (from the [documentation](https://r-spatial.github.io/sf/reference/st_join.html))

* st_contains_properly  
* st_contains  
* st_covered_by  
* st_covers  
* st_crosses  
* st_disjoint  
* st_equals_exact  
* st_equals  
* st_is_within_distance  
* st_nearest_feature  
* st_overlaps  
* st_touches  
* st_within  





## Choropleth maps {}  


Choropleth maps can be useful to visualize your data by pre-defined area, usually administrative unit or health area. In outbreak response this can help to target resource allocation for specific areas with high incidence rates, for example.

Now that we have the administrative unit names assigned to all cases (see section on spatial joins, above), we can start mapping the case counts by area (choropleth maps).

Since we also have population data by ADM3, we can add this information to the *case_adm3* table created previously.

We begin with the dataframe created in the previous step `case_adm3`, which is a summary table of each administrative unit and its number of cases.  

1) The populaton data `sle_adm3_pop` are joined using a `left_join()` from **dplyr** on the basis of common values across column `admin3pcod` in the `case_adm3` dataframe, and column `adm_pcode` in the `sle_adm3_pop` dataframe. See page on [Joining data]).  
2) `select()` is applied to the new dataframe, to keep only the useful columns - `total` is total population  
3) Cases per 10,000 populaton is calculated as a new column with `mutate()`  


```{r}
# Add population data and calculate cases per 10K population
case_adm3 <- case_adm3 %>% 
     left_join(sle_adm3_pop,                             # add columns from pop dataset
               by = c("admin3pcod" = "adm3_pcode")) %>%  # join based on common values across these two columns
     select(names(case_adm3), total) %>%                 # keep only important columns, including total population
     mutate(case_10kpop = round(cases/total * 10000, 3)) # make new column with case rate per 10000, rounded to 3 decimals

case_adm3                                                # print to console for viewing
```

Join this table with the ADM3 polygons shapefile for mapping

```{r, warning=F, message=F}
case_adm3_sf <- case_adm3 %>%                 # begin with cases & rate by admin unit
  left_join(sle_adm3, by="admin3pcod") %>%    # join to shapefile data by common column
  select(objectid, admin3pcod,                # keep only certain columns of interest
         admin3name = admin3name.x,           # clean name of one column
         admin2name, admin1name,
         cases, total, case_10kpop,
         geometry) %>%                        # keep geometry so polygons can be plotted
  st_as_sf()                                  # convert to shapefile

```


Mapping the results

```{r, message=F, warning=F}
# tmap mode
tmap_mode("plot")               # view static map

# plot polygons
tm_shape(case_adm3_sf) + 
        tm_polygons("cases") +  # color by number of cases column
        tm_text("admin3name")   # name display
```

We can also map the incidence rates  


```{r, warning=F, message=F}
# Cases per 10K population
tmap_mode("plot")             # static viewing mode

# plot
tm_shape(case_adm3_sf) +                # plot polygons
  tm_polygons("case_10kpop",            # color by column containing case rate
              breaks=c(0, 10, 50, 100), # define break points for colors
              palette = "Purples"       # use a purple color palette
              ) +
  tm_text("admin3name")                 # display text

```







<!-- ======================================================= -->
## Basemaps { }

### OpenStreetMap {-} 

Below we describe how to achieve a basemap using OpenStreetMap features. Alternative methods include using **ggmap** which requires free registration with Google ([details](https://www.earthdatascience.org/courses/earth-analytics/lidar-raster-data-r/ggmap-basemap/)).  

First we load the **OpenStreetMap** package, from which we will get our basemap.  

Then, we create the object `map`, which we define using the function `openmap()` from **OpenStreetMap** package ([documentation](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/openmap)). We provide the following:  

* `upperLeft` and `lowerRight` Two coordinate pairs specifying the limits of the basemap tile  
  * In this case we've put in the max and min from the linelist rows, so the map will respond dynamically to the data  
* `zoom = ` (if null it is determined automatically)  
* `type =` which type of basemap - we have listed several possibilities here and the code is currently using the first one (`[1]`) "osm"  
* `mergeTiles = ` we chose TRUE so the basetiles are all merged into one


```{r, message=FALSE, warning=FALSE}
# load package
pacman::p_load(OpenStreetMap)

# Fit basemap by range of lat/long coordinates. Choose tile type
map <- openmap(
  upperLeft = c(max(linelist$lat, na.rm=T), max(linelist$lon, na.rm=T)),   # limits of basemap tile
  lowerRight = c(min(linelist$lat, na.rm=T), min(linelist$lon, na.rm=T)),
  zoom = NULL,
  type = c("osm", "stamen-toner", "stamen-terrain","stamen-watercolor", "esri","esri-topo")[1])
```

If we plot this basemap right now, using `autoplot.OpenStreetMap()` from **OpenStreetMap** package, you see that the units on the axes are not latitude/longitude coordinates. It is using a different coordinate system. To correctly display the case residences (which are stored in lat/long), this must be changed.  

```{r, warning=F, message=F}
autoplot.OpenStreetMap(map)
```
Thus, we want to convert the map to latitude/longitude with the `openproj()` function from **OpenStreetMap** package. We provide the basemap `map` and also provide the Coordinate Reference System (CRS) we want. We do this by providing the "proj.4" character string for the WGS 1984 projection, but you can provide the CRS in other ways as well. (see [this page](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/understand-epsg-wkt-and-other-crs-definition-file-types/) to better understand what a proj.4 string is)  

```{r, warning=F, message=F}
# Projection WGS84
map_latlon <- openproj(map, projection = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
```

Now when we create the plot we see that along the axes are latitude and longitude coordinate. The coordinate system has been converted. Now our cases will plot correctly if overlaid!  

```{r, warning=F, message=F}
# Plot map. Must use "autoplot" in order to work with ggplot
autoplot.OpenStreetMap(map_latlon)
```

See the tutorials [here](http://data-analytics.net/cep/Schedule_files/geospatial.html) and [here](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/autoplot.OpenStreetMap) for more info.  





## Contoured density heatmaps {}

Below we describe how to achieve a contoured density heatmap of cases, over a basemap, beginning with a linelist (one row per case).  

1) Create basemap tile from OpenStreetMap, as described above  
2) Plot the cases from `linelist` using the latitude and longitude columns  
3) Convert the points to a density heatmap with `stat_density_2d()` from **ggplot2**, 


When we have a basemap with lat/long coordinates, we can plot our cases on top using the lat/long coordinates of their residence. 

Building on the function `autoplot.OpenStreetMap()` to create the basemap, **ggplot2** functions will easily add on top, as shown with `geom_point()` below:  

```{r, warning=F, message=F}
# Plot map. Must be autoplotted to work with ggplot
autoplot.OpenStreetMap(map_latlon)+                 # begin with the basemap
  geom_point(                                       # add xy points from linelist lon and lat columns 
    data = linelist,                                
    aes(x = lon, y = lat),
    size = 1, 
    alpha = 0.5,
    show.legend = FALSE) +                          # drop legend entirely
  labs(x = "Longitude",                             # titles & labels
       y = "Latitude",
       title = "Cumulative cases")

```
The map above might be difficult to interpret, especially with the points overlapping. So you can instead plot a 2d density map using the **ggplot2** function `stat_density_2d()`. You are still using the linelist lat/lon coordinates, but a 2D kernel density estimation is performed and the results are displayed with contour lines - like a topographical map. Read the full [documentation here](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).  


```{r, warning=F, message=F}
# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases")

```





<!-- ======================================================= -->
### Time series heatmap {}

The density heatmap above shows *cumulative cases*. We can examine the outbreak over time and space by faceting the heatmap based on the *month of symptom onset*, as derived from the linelist.  

We begin in the `linelist`, creating a new column with the Year and Month of onset. The `format()` function from **base** R changes how a date is displayed. In this case we want "YYYY-MM".  

```{r, warning=F, message=F}
# Extract month of onset
linelist <- linelist %>% 
  mutate(date_onset_ym = format(date_onset, "%Y-%m"))

# Examine the values 
table(linelist$date_onset_ym, useNA = "always")
```

Now, we simply introduce facetting via **ggplot2** to the density heatmap. `facet_wrap()` is applied, using the new column as rows. We set the number of facet columns to 3 for clarity.  


```{r, warning=F, message=F}
# packages
pacman::p_load(OpenStreetMap, tidyverse)

# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases")+
  
  # facet the plot by month-year of onset
  facet_wrap(~ date_onset_ym, ncol = 4)               

```






<!-- ======================================================= -->
## Resources {  }

* R Simple Features and sf package
https://cran.r-project.org/web/packages/sf/vignettes/sf1.html

* R tmap package
https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html

* ggmap: Spatial Visualization with ggplot2
https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf

* [Intro to making maps with R, overview of different packages](https://bookdown.org/nicohahn/making_maps_with_r5/docs/introduction.html)  



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/gis.Rmd-->

# (PART) Data Visualization {-}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/cat_data_viz.Rmd-->


# ggplot tips {}


<!-- ======================================================= -->
## Overview {}

**ggplot2** is the most popular data visualisation package in R, and is generally used instead of **base** `R` for creating figures. **ggplot2** benefits from a wide variety of supplementary packages that further enhance its functionality. Despite this, ggplot syntax is significantly different from **base** `R` plotting, and has a learning curve associated with it. Using **ggplot2** generally requires the user to format their data in a way that is highly **tidyverse** compatible, which ultimately makes using these packages together very effective.

<!-- One of the best resources is the official ggplot cheatsheet, shown here: -->

<!-- ```{r, include=TRUE, fig.align="center", fig.cap=c("ggplot cheatsheet"), echo=FALSE} -->
<!-- knitr::include_graphics("./images/imagename.pdf") -->
<!-- ``` -->

<!-- ``` -->

You can also download this [data visualization with ggplot cheatsheet](https://rstudio.com/resources/cheatsheets/) from the RStudio website.  

If you want inspiration for ways to creatively visualise your data, we suggest reviewing websites like the [R graph gallery](https://www.r-graph-gallery.com/) and [Data-to-viz](https://www.data-to-viz.com/caveats.html). 



<!-- ======================================================= -->
## Preparation {}

### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,      # includes ggplot2 and other
  rio,            # import/export
  here,           # file locator
  stringr,        # working with characters   
  scales,         # transform numbers
  ggrepel,        # smartly-placed labels
  gghighlight,    # highlight one part of plot
  RColorBrewer    # color scales
)
```

### Import data {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, eval=T, echo=F}
linelist_cleaned <- rio::import(here::here("data", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist_cleaned <- rio::import("linelist_cleaned.xlsx")

```

### General cleaning {-}

When preparing data to plot, it is best to make the data adhere to ["tidy" data standards](https://r4ds.had.co.nz/tidy-data.html) as much as possible. How to achieve this is expanded on in the data management pages of this handbook, such as [Cleaning data and core functions]. 

Some simple ways we can prepare our data to make it better for plotting can include making the contents of the data better for display - this does not necessarily mean its better for data manipulation! For example:  

* Replace `NA` values in a character column with the string "Unknown"  
* Clean some columns so that their "data friendly" values with underscores etc are changed to normal text or title case (see [Characters and strings])  

Here are some examples of this in action:

```{r, eval = TRUE}
linelist_cleaned <- linelist_cleaned %>%
  # make display version of columns with more friendly names
  mutate(
    # f to Male, f to Female, NA to Unknown
    gender_disp = case_when(gender == "m" ~ "Male",
                            gender == "f" ~ "Female",
                            is.na(gender) ~ "Unknown"),
    # replace NA with unknown for outcome
    outcome_disp = replace_na(outcome, "Unknown")
  )
```

### Pivoting longer {-}

As a matter of data structure, for **ggplot2** we often also want to pivot our data into *longer* formats, which will allow us to use a set of variables as a single variable. Read more about this is the page on [Pivoting data].  

For example, if we wanted to show the number of cases with specific symptoms, we are limited by the fact that each symptom is a specific column. We can pivot this to a longer format like this:

```{r, eval = T}

linelist_sym <- linelist_cleaned %>%
  pivot_longer(cols = c("fever", "chills", "cough", "aches", "vomit"),
               names_to = "symptom_name",
               values_to = "symptom_is_present") %>%
  mutate(symptom_is_present = replace_na(symptom_is_present, "unknown"))

```

Note that this format is not very useful for other operations, and should just be used for the plot it was made for. However, users should endeavor to use these practices as much as possible for the base dataset, as they are more tidyverse compliant, and will make working with the data easier.





<!-- ======================================================= -->
## Basics of ggplot {}

**"Grammar of graphics" - ggplot2**  

Plotting with **ggplot2** is based on defining base attributes to a plot, and "adding" layers on top. In addition, the user can change various plot attributes like axis settings, colour schemes, and labels with additional objects that are "added" to the plot. While ggplot objects can be highly complex, the basic order of creating a ggplot looks something like this:

1. Define base/default plot attributes and aesthetics with `ggplot()` function
2. Add geometric objects to the plot - i.e. is the plot a bar graph, a line plot, a scatter plot, or a histogram? Or is it a combination of these? These functions all start with `geom_` as a prefix.
3. Change plot aesthetics e.g. changing the axes, labels, colour scheme, background etc.

In code, this might look like this:

```{r, eval = TRUE, warning=F, message=F}

# define base plot attributes and dataset
ggplot(data = linelist_cleaned, mapping = aes(x = age)) +
  # add a geometric object with some parameters
  geom_histogram(binwidth = 10, fill = "red", col = "black") +
  # add labels to the axes
  labs(x = "Age in years", y = "Number of cases")
  

```

With this code, the most important things to note are:

  1. When making a ggplot, all objects are combined with a `+` sign.
  2. Understand the principles behind aesthetic mapping with the `mappping = aes()` argument is essential to using ggplot. This can be done in the `ggplot()` function as well as every geometric object. Mapping with `aes()` is used to define which variables are assigned to each axis (these can be continuous or categorical variables). It is also used to define whether a variable can be used to create different plot aesthetics. This can apply to the:
  
a. line colour (`col = `)  
b. filled colour (`fill = `)  
c. linetype (e.g. dotted, dashed) (`linetype =`)  
d. size of an object (`size = `)  

This list is not exhaustive, but is enough to give a rough overview.
  
  3. Aesthetics of geometric objects can be defined *explicitly* as in the code above - this is different from assigning them to a variable. In cases where this is done, it must be *outside* the `mapping` argument. 
  
```{r, eval = FALSE}
# correct
ggplot(data = linelist_cleaned, mapping = aes(x = age)) +
  geom_histogram(col = "black")

# incorrect
# correct
ggplot(data = linelist_cleaned, mapping = aes(x = age)) +
  geom_histogram(mapping = aes(col = "black"))

```

An example of defining aesthetics with a variable can be seen here:

```{r, eval = TRUE, warning=F, message=F}
# define base plot attributes and dataset
ggplot(data = linelist_cleaned, mapping = aes(x = age, fill = outcome)) +
  # add a geometric object with some parameters (NO FILL GIVEN)
  geom_histogram(binwidth = 10, col = "black") +
  # add labels to the axes
  labs(x = "Age in years", y = "Number of cases")
```


There are a huge number of different geoms that can be used, and they are all used with similar attribute names. While not exhaustive, some of the shapes that can be used are:

  1. Histograms - `geom_histogram()`
  2. Barcharts - `geom_bar()`
  3. Boxplots - `geom_boxplot()`
  4. Dot plots (for scatterplots or with discrete variables) - `geom_point()`
  5. Line graphs - `geom_line()` or `geom_path()`
  6. Trend lines - `geom_smooth()`

You can also add straight lines to your plot with `geom_hline()` (horizontal), `geom_vline()` (vertical) or `geom_abline()` (with a specified y intercept and slope)

There is much more detail we could show here, but we'll finish with an example that ties these concepts together by plotting a correlation between height and weight of all the patients. We can also colour the points by age in years
  
```{r, eval = TRUE, cache.vars = TRUE}


# set up the plot and define key variables
# colour is the outcome
wt_ht_plot <- ggplot(data = linelist_cleaned,
                     aes(y = wt_kg, x = ht_cm, col = age_years)) +
  # define aspects of the geom that are NOT included specific to variables
  # other attributes are inherited
  geom_point(size = 1, alpha = 0.5) +
  # add a trend line
  # use a linear method
  geom_smooth(method = "lm")
wt_ht_plot

```


<!-- ======================================================= -->
## Themes and Labels {}

One of the most important aspects of data visualisation is presenting data in a clear way with nice aesthetics. The plot we made previously looks ok, but we could make the theme a little nicer. `ggplot2` comes with some preset themes that can be used to change the theme of the plot. We can also edit themes of the plot with extreme detail with the `theme()` function. We can also add some nicer labels to the plot with the `labs()` function. There are 5 standard labeling locations:

  1. `x` - the x-axis
  2. `y` - the y-axis
  3. `title` - the main plot title
  4. `subtitle` - directly underneath the plot title in smaller text (by default)
  5. `caption` - bottom of plot, on the right by default
  
For example, we can update the plot we previously plotted with nice labels like this:

```{r, eval = TRUE}

wt_ht_plot <- wt_ht_plot + 
  # set the theme to classic
  theme_classic() +
  # further edit the theme to move the legend position
  # add nicer labels
  labs(y = "Weight (kg)", 
       x = "height (cm)",
       title = "Patient height and weight",
       subtitle = glue::glue("total patients {nrow(linelist_cleaned)}"),
       caption = "produced by me!")
wt_ht_plot

```

The `theme()` function can also be used to edit the defaults of these elements. This function can take an extremely large number of arguments, each of which can be used to edit very specific aspects of the plot. We won't go through all examples, look at how editing aspects of text elements is done. The basic way this is done is:

  1. Calling the specific argument of `theme()` for the element we want to edit (e.g. `plot.title` for the plot title)
  2. Supplying the `element_text()` function to the argument (there are other versions of this e.g. `element_rect()` for editing the plot background aesthetics)
  3. Changing the arguments in `element_text()`
  
For example, we increase the size of the plot title with `size`, make the subtitle italicised with `face`, and right
justify the caption with `hjust`. We'll also change the legend location for good measure!

```{r,eval=T}
wt_ht_plot + 
    theme(legend.position = "bottom",
          # size of title is 30
          plot.title = element_text(size = 30),
          # right justify caption
          plot.caption = element_text(hjust = 0),
          # subtitle is italicised
          plot.subtitle = element_text(face = "italic"))


```

If you ever want to remove an element of a plot, you can also do it through `theme()`! Just pass `element_blank()` to an argument in theme to have it disappear completely!

<!-- ======================================================= -->
## Colour schemes {}

One thing that can initially be difficult to understand with `ggplot2` is control of colour schemes when passing colour or fill as a variable rather than defining them explicitly within a geom. There are a few simple tricks that can be used to achieve this however. Remember that when setting colours, you can use colour names (as long as they are recognised) like `"red"`, or a specific hex colour such as `"#ff0505"`.

One of the most useful tricks is using manual scaling functions to explicity define colours. These are functions with the syntax `scale_xxx_manual()` (e.g. `scale_colour_manual()`). In this function you can explicitly define which colours map to which factor using the `values` argument. You can control the legend title with the `name` argument, and the order of factors with `breaks`. 

If you want predefined palettes, you can use the `scale_xxx_brewer` or `scale_xxx_viridis_y` functions. The brewer functions can draw from colorbrewer.org palettes, and the viridis functions can draw from viridis (colourblind friendly!) palettes. Remember to define if the palette is discrete, continuous, or binned by specifying this at the end of the function (e.g. discrete is `scale_xxx_viridis_d`)

We can see this by using the symptom-specific dataframe we made in the previous section:
```{r,eval = TRUE} 



symp_plot <- ggplot(linelist_sym, aes(x = symptom_name, fill = symptom_is_present)) +
  # show as a portion of all
  geom_bar(position = "fill", col = "black") +
  theme_classic() +
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )

symp_plot

symp_plot +
  scale_fill_manual(
    # explicitly define colours
    values = c("yes" = "black",
               "no" = "white",
               "unknown" = "grey"),
    # order the factors correctly
    breaks = c("yes", "no", "unknown"),
    # legend has no title
    name = ""
  ) 

symp_plot +
  scale_fill_viridis_d(
    breaks = c("yes", "no", "unknown"),
    name = ""
  )


```



<!-- ======================================================= -->
## Change order of discrete variables {}
Changing the order that discrete variables appear in is often difficult to understand for people who are new to `ggplot2` graphs. It's easy to understand how to do this however once you understand how `ggplot2` handles discrete variables under the hood. Generally speaking, if a discrete varaible is used, it is automatically converted to a `factor` type - which orders factors by alphabetical order by default. To handle this, you simply have to reorder the factor levels to reflect the order you would like them to appear in the chart. For more detailed information on how to reorder `factor` objects, see the factor section of the guide. 

We can look at a common example using age groups - by default the 5-9 age group will be placed in the middle of the age groups (given alphabetical order), but we can move it behind the 0-4 age group of the chart by releveling the factors.

```{r, eval = T, warning=F, message=F}

# remove the instances of age_cat5 where data is missing
ggplot(linelist_cleaned %>%
         filter(!is.na(age_cat5)),
       # relevel the factor within the ggplot call (can do externally as well)
       aes(x = forcats::fct_relevel(age_cat5, "5-9", after = 1))) +
  geom_histogram(stat = "count") +
  labs(x = "Age group", y = "Number of hospitalisations",
       title = "Total hospitalisations by age group") +
  theme_minimal()


```


<!-- ======================================================= -->
## Multiple plots {}

Often its useful to show multiple graphs on one page, or one super-figure. There are a few ways to achieve this and a lot of packages that can help to facilitate it. However, while external packages are nice, it is often easier to use faceting as an alternative that is prebuilt into `ggplot2`. Faceting plots is extremely easy to do in terms of code, and produces plots with more predictable aesthetics - you wont have to wrangle legends and ensure that axes are aligned etc.

Faceting is a very specific way to obtain multiple plots - by definition, to facet you have to show the same type of plot in each facet, where every plot is specific to a level of a variable. This is done with one of two functions:

  1. `facet_wrap()` This is used when you want to show a different graph for each level of a *single* variable. One example of this could be showing a different epidemic curve for each hospital in a region. 
  
  2. `facet_grid()` This is used when you want to bring a second variable into the faceting arrangement. Here each element of a grid is shows the intersection between an x or y element of a grid. For example, this could involve showing a different epidemic curve for each hospital in a region, shown horizontally, for each age group, shown vertically.
  
This can quickly become an overwhelming amount of information - its good to ensure you don't have too many levels of each variable that you choose to facet by! Here are some quick examples with the malaria dataset:

```{r, eval = TRUE, warning=F, message=F}
malaria_data <- rio::import(here::here("data", "facility_count_data.rds")) 

# show a wrapped plot with facets by district

ggplot(malaria_data, aes(x = data_date, y = malaria_tot, fill = District)) +
  geom_bar(stat = "identity") +
  labs(
    x = "date of data collection",
    y = "malaria cases",
    title = "Malaria cases by district"
  ) +
  facet_wrap(~District) +
  theme_minimal()

```

We can also use a `facet_grid()` approach with the different age groups - we need to do some data transformations first however, as the age groups all are in their own columns - we want them in a single column. When you pass the two variables to `facet_grid()`, you can use formula notation (e.g. `x ~ y`) or wrap the variables in `vars()`. For reference, this: `facet_grid(x ~ y)` is equivalent to `facet_grid(rows = vars(x), cols = vars(y))` Here's how we can do this:

```{r, eval = T, warning=F, message=F}

malaria_age <- malaria_data %>%
  pivot_longer(
    # choose all the columns that start with malaria rdt (age group specific)
    cols = starts_with("malaria_rdt_"),
    # column names become age group
    names_to = "age_group",
    # values to a single column (num_cases)
    values_to = "num_cases"
  ) %>%
  # clean up age group column - replace "malaria_rdt_" to leave only age group
  # then replace 15 with 15+
  # then refactor the age groups so they are in order
  mutate(age_group = str_replace(age_group, "malaria_rdt_", "") %>%
           ifelse(. == "15", "15+", .) %>%
           forcats::fct_relevel(., "5-14", after = 1))


# make the same plot as before, but show in a grid
ggplot(malaria_age, aes(x = data_date, y = num_cases, fill = age_group)) +
  geom_bar(stat = "identity") +
  labs(
    x = "date of data collection",
    y = "malaria cases",
    title = "Malaria cases by district and age group"
  ) +
  facet_grid(rows = vars(District), cols = vars(age_group)) +
  theme_minimal()



```

While faceting is a convenient approach to plotting, sometimes its not possible to get the results you want from its relatively restrictive approach. Here, you may choose to combine plots by sticking them together into a larger plot. There are three well known packages that are great for this - `cowplot`, `gridExtra`, and `patchwork`. However, these packages largely do the same things, so we'll focus on `cowplot` for this section. 

The `cowplot` package has a fairly wide range of functions, but the easiest use of it can be achieved through the use of `plot_grid()`. This is effectively a way to arrange predefined plots in a grid formation. We can work through another example with the malaria dataset - here we can plot the total cases by district, and also show the epidemic curve over time.


```{r, eval = T, warning=F, message=F}

# bar chart of total cases by district
p1 <- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +
  geom_bar(stat = "identity") +
  labs(
    x = "District",
    y = "Total number of cases",
    title = "Total malaria cases by district"
  ) +
  theme_minimal()

# epidemic curve over time
p2 <- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_bar(stat = "identity") +
  labs(
    x = "Date of data submission",
    y =  "number of cases"
  ) +
  theme_minimal()

cowplot::plot_grid(p1, p2,
                  # 1 column and two rows - stacked on top of each other
                   ncol = 1,
                   nrow = 2,
                   # top plot is 2/3 as tall as second
                   rel_heights = c(2, 3))


```


## Marginal distributions  

To show the distributions on the edges of a `geom_point()` scatterplot, you can use the **ggExtra** package and its function `ggMarginal()`. Save your original ggplot as an object, then pass it to `ggMarginal()` as shown below. Here are the key arguments:  

* You must specify the `type = ` as either "histogram", "density" "boxplot", "violin", or "densigram".  
* By default, marginal plots will appear for both axes. You can set `margins = ` to "x" or "y" if you only want one.  
* Other optional arguments include `fill = ` (bar color), `color = ` (line color), `size = ` (plot size relative to margin size, so larger number makes the marginal plot smaller).  
* You can provide other axis-specific arguments to `xparams = ` and `yparams = `. For example, to have different histogram bin sizes, as shown below.  

You can have the marginal plots reflect groups (columns that have been assigned to `color = ` in your `ggplot()` mapped aesthetics). If this is the case, set the `ggMarginal()` argument `groupColour = ` or `groupFill = ` to `TRUE`, as shown below.  

Read more at [this vignette](https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html), in the [R Graph Gallery](https://www.r-graph-gallery.com/277-marginal-histogram-for-ggplot2.html) or the function R documentation `?ggMarginal`.  

```{r, message=FALSE, warning=FALSE}
# Install/load ggExtra
pacman::p_load(ggExtra)

# Basic scatter plot of weight and age
scatter_plot <- ggplot(data = linelist)+
  geom_point(mapping = aes(y = wt_kg, x = age)) +
  labs(title = "Scatter plot of weight and age")
```

To add marginal histograms:  

```{r, message=FALSE, warning=FALSE}
# with histograms
ggMarginal(
  scatter_plot,                     # add marginal histograms
  type = "histogram",               # specify histograms
  fill = "lightblue",               # bar fill
  xparams = list(binwidth = 10),    # other parameters for x-axis marginal
  yparams = list(binwidth = 5))     # other parameters for y-axis marginal
```

Marginal histograms with grouped/colored values:  

```{r, message=FALSE, warning=FALSE}

# Scatter plot, colored by outcome
# Outcome column is assigned as color in ggplot. groupFill in ggMarginal set to TRUE
scatter_plot_color <- ggplot(data = linelist)+
  geom_point(mapping = aes(y = wt_kg, x = age, color = outcome)) +
  labs(title = "Scatter plot of weight and age")+
  theme(legend.position = "bottom")

ggMarginal(scatter_plot_color, type = "histogram", groupFill = TRUE)
```

Marginal density curve, with demonstration of size and color arguments:  
```{r, message=FALSE, warning=FALSE}
# with density curves
ggMarginal(
  scatter_plot,
  type = "density",
  color = "red",                    # line color
  size = 4)                         # smaller number makes larger marginal plots
```

Marginal boxplots, with demonstration of the margins argument:  

```{r, message=FALSE, warning=FALSE}
# with boxplot 
ggMarginal(
  scatter_plot,
  margins = "x",      # only show x-axis marginal plot
  type = "boxplot")   
```



<!-- ======================================================= -->
## Smart Labeling {}
In `ggplot2`, it is also possible to add text to plots. However, this comes with the notable limitation where text labels often clash with data points in a plot, making them look messy or hard to read. There is no ideal way to deal with this in the base package, but there is a `ggplot2` add-on, known as `ggrepel` that makes dealing with this very simple! 

The `ggrepel` package provides two new functions, `geom_label_repel()` and `geom_text_repel()`, which replace `geom_label()` and `geom_text()`. Simply use these functions instead of the base functions to produce neat labels. Within the function, map the aesthetics `aes()` as always, but include the argument `label = ` to which you provide a column name containing the values you want to display (e.g. patient id, or name, etc.). You can make more complex labels by combining columns and newlines (`\n`) within `str_glue()` as shown below.  

A few tips:  

* Use `min.segment.length = 0` to always draw line segments, or `min.segment.length = Inf` to never draw them  
* Use `size = ` outside of `aes()` to set text size  
* Use `force = ` to change the degree of repulsion between labels and their respective points (default is 1)  
* Include `fill = ` within `aes()` to have label colored by value  
  * A letter "a" may appear in the legend - add `guides(fill = guide_legend(override.aes = aes(color = NA)))+` to remove it  

See this is very in-depth [tutorial](https://ggrepel.slowkow.com/articles/examples.html) for more.  

```{r, eval = T, warning=F, message=F}
pacman::p_load(ggrepel)

linelist %>%                                               # start with linelist
  group_by(hospital) %>%                                   # group by hospital
  summarise(                                               # create new dataset with summary values per hospital
    n_cases = n(),                                           # number of cases per hospital
    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # mean delay per hospital
  ) %>% 
  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # send data frame to ggplot
  geom_point(size = 2)+                                    # add points
  geom_label_repel(                                        # add point labels
    mapping = aes(
      label = stringr::str_glue(
        "{hospital}\n{n_cases} cases, {delay_mean} days")  # how label displays
      ), 
    size = 3,                                              # text size in labels
    min.segment.length = 0)+                               # show all line segments                
  labs(                                                    # add axes labels
    title = "Mean delay to admission, by hospital",
    x = "Number of cases",
    y = "Mean delay (days)")
```

You can label only a subset of the data points - by using standard `ggplot()` syntax to provide different `data = ` for each `geom` layer of the plot. Below, All cases are plotted, but only a few are labeled.    

```{r, warning=F, message=FALSE}

ggplot()+
  # All points in grey
  geom_point(
    data = linelist_cleaned,                                   # all data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    color = "grey",
    alpha = 0.5)+                                              # grey and semi-transparent
  
  # Few points in black
  geom_point(
    data = linelist_cleaned %>% filter(days_onset_hosp > 15),  # filtered data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    alpha = 1)+                                                # default black and not transparent
  
  # point labels for few points
  geom_label_repel(
    data = linelist_cleaned %>% filter(days_onset_hosp > 15),  # filter the data for the labels
    mapping = aes(
      x = ht_cm,
      y = wt_kg,
      fill = outcome,                                          # label color by outcome
      label = stringr::str_glue("Delay: {days_onset_hosp}d")), # label created with str_glue()
    min.segment.length = 0) +                                  # show line segments for all
  
  # remove letter "a" from inside legend boxes
  guides(fill = guide_legend(override.aes = aes(color = NA)))+
  
  # axis labels
  labs(
    title = "Cases with long delay to admission",
    y = "weight (kg)",
    x = "height(cm)")
```





<!-- ======================================================= -->
## Time axes {}

Working with time axes in ggplot can seem daunting, but is made very easy with a few key functions. Remember that when working with time or date that you should ensure that the correct variables are formatted as date or datetime class - see the [Working with dates] page for more information on this, or [Epidemic curves] page (ggplot section) for examples.

The single most useful set of functions for working with dates in `ggplot2` are the scale functions (`scale_x_date()`, `scale_x_datetime()`, and their cognate y-axis functions). These functions let you define how often you have axis labels, and how to format axis labels. To find out how to format dates, see the _working with dates_ section again! You can use the `date_breaks` and `date_labels` arguments to specify how dates should look:

  1. `date_breaks` allows you to specify how often axis breaks occur - you can pass a string here (e.g. `"3 months"`, or "`2 days"`)
  
  2. `date_labels` allows you to define the format dates are shown in. You can pass a date format string to these arguments (e.g. `"%b-%d-%Y"`):


```{r, eval = T, warning=F, message=F}
# make epi curve by date of onset when available
ggplot(linelist_cleaned, aes(x = date_onset)) +
  geom_bar(stat = "count") +
  scale_x_date(
    # 1 break every 1 month
    date_breaks = "1 months",
    # labels should show month then date
    date_labels = "%b %d"
  ) +
  theme_classic()

```



<!-- ======================================================= -->
## Highlighting {}

Highlighting specific elements in a chart is a useful way to draw attention to a specific instance of a variable while also providing information on the dispersion of the full dataset. While this is not easily done in base `ggplot2`, there is an external package that can help to do this known as `gghighlight`. This is easy to use within the ggplot syntax.

The `gghighlight` package uses the `gghighlight()` function to achieve this effect. To use this function, supply a logical statement to the function - this can have quite flexible outcomes, but here we'll show an example of the age distribution of cases in our linelist, highlighting them by outcome.

```{r, eval = T, warning=F, message=F}
# load gghighlight
library(gghighlight)


# replace NA values with unknown in the outcome variable
linelist_cleaned <- linelist_cleaned %>%
  mutate(outcome = replace_na(outcome, "Unknown"))

# produce a histogram of all cases by age
ggplot(linelist_cleaned, 
       aes(x = age_years, fill = outcome)) +
  geom_histogram() + 
  # highlight instances where the patient has died.
  gghighlight::gghighlight(outcome == "Death")


```

This also works well with faceting functions - it allows the user to produce facet plots with the background data highlighted that doesn't apply to the facet!

```{r, eval = T, warning=F, message=F}

# produce a histogram of all cases by age
ggplot(linelist_cleaned, 
       aes(x = age_years, fill = outcome)) +
  geom_histogram() + 
  # highlight instances where the patient has died.
  gghighlight::gghighlight() +
  facet_wrap(~outcome)


```


<!-- ======================================================= -->
## Dual axes {}

A secondary y-axis is often a requested addition to a `ggplot2` graph. While there is a robust debate about the validity of such graphs in the data visualization community, and they are often not recommended, your manager may still want them. Below, we present two methods to achieve them.  

1) Using the **cowplot** package to combine two separate plots  
2) Using a statistical transformation of the data on the primary axis  


### Using **cowplot** {-}  

This approach involves creating two separate plots - one with a y-axis on the left, and the other with y-axis on the right. Both will use a specific `theme_cowplot()` and must have the same x-axis. Then in a third command the two plots are aligned and overlaid on top of each other. The functionalities of **cowplot**, of which this is only one, are described in depth at this [site](https://wilkelab.org/cowplot/articles/aligning_plots.html).  

To demonstrate this technique we will overlay the epidemic curve with a line of the weekly percent of patients who died. We use this example because the alignment of dates on the x-axis is more complex than say, aligning a bar chart with another plot. Some things to note:  

* The epicurve and the line are aggregated into weeks prior to plotting *and* the `date_breaks` and `date_labels` are identical - we do this so that the x-axes of the two plots are the same when they are overlaid.  
* The y-axis is moved to the right-side for plot 2 with the `position = ` argument of `scale_y_continuous()`.  
* Both plots make use of `theme_cowplot()`  

Note there is another example of this technique in the [Epicurves] page - overlaying cumulative incidence on top of the epicurve.  

**Make plot 1**  
This is essentially the epicurve. We use `geom_area()` just to demonstrate its use (area under a line, by default)
```{r, warning=F, message=F}
pacman::p_load(cowplot)            # load/install cowplot

p1 <- linelist %>%                 # save plot as object
     count(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     ggplot()+
          geom_area(aes(x = epiweek, y = n), fill = "grey")+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
     theme_cowplot()+
     labs(
       y = "Weekly cases"
     )

p1                                      # view plot 
```

**Make plot 2**  
Create the second plot showing a line of the weekly percent of cases who died. 
```{r, warning=F, message=F}

p2 <- linelist %>%         # save plot as object
     group_by(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     summarise(
       n = n(),
       pct_death = 100*sum(outcome == "Death", na.rm=T) / n) %>% 
     ggplot(aes(x = epiweek, y = pct_death))+
          geom_line()+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
          scale_y_continuous(
               position = "right")+
          theme_cowplot()+
          labs(
            x = "Epiweek of symptom onset",
            y = "Weekly percent of deaths",
            title = "Weekly case incidence and percent deaths"
          )

p2     # view plot
```

Now we align the plot using the function `align_plots()`, specifying horizontal and vertical alignment ("hv", could also be "h", "v", "none"). We specify alignment of all axes as well (top, bottom, left, and right) with "tblr". The output is of class list (2 elements).    

Then we draw the two plots together using `ggdraw()` (from **cowplot**) and referencing the two parts of the `aligned_plots` object.  

```{r, warning=F, message=F}
aligned_plots <- align_plots(p1, p2, align="hv", axis="tblr")                  # align the two plots and save them as list
aligned_plotted <- ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])  # overlay them and save the visual plot
aligned_plotted                                                                # print the overlayed plots

```

### Statistical transformation {-}  

Unfortunately, secondary axes are not well supported in the `ggplot` syntax. For this reason, you're fairly limited in terms of what can be shown with a secondary axis - the second axis has to be a direct transformation of the secondary axis. 

Differences in axis values will be purely cosmetic - if you want to show two different variables on one graph, with different y-axis scales for each variable, this will not work without some work behind the scenes. To obtain this effect, you will have to transform one of your variables in the data, and apply the same transformation *in reverse* when specifying the axis labels. Based on this, you can either specify the transformation explicitly (e.g. variable a is around 10x as large as variable b) or calculate it in the code (e.g. what is the ratio between the maximum values of each dataset).


The syntax for adding a secondary axis is very straightforward! When calling a `scale_xxx_xxx()` function (e.g. `scale_y_continuous()`), use the `sec.axis` argument to call the `sec_axis()` function. The `trans` argument in this function allows you to specify the label transformation for the axis - provide this in standard tidyverse syntax. 

For example, if we want to show the number of positive RDTs in the malaria dataset for facility 1, showing 0-4 year olds and all cases on chart:


```{r, eval = T, warning=F, message=F}

# take malaria data from facility 1
malaria_facility_1 <- malaria_data %>%
  filter(location_name == "Facility 1")

# calculate the ratio between malaria_rdt_0-4 and malaria_tot 

tf_ratio <- max(malaria_facility_1$malaria_tot, na.rm = T) / max(malaria_facility_1$`malaria_rdt_0-4`, na.rm = T)

# transform the values in the dataset

malaria_facility_1 <- malaria_facility_1 %>%
  mutate(malaria_rdt_0_4_tf = `malaria_rdt_0-4` * tf_ratio)
  

# plot the graph with dual axes

ggplot(malaria_facility_1, aes(x = data_date)) +
  geom_line(aes(y = malaria_tot, col = "Total cases")) +
  geom_line(aes(y = malaria_rdt_0_4_tf, col = "Cases: 0-4 years old")) +
  scale_y_continuous(
    name = "Total cases",
    sec.axis = sec_axis(trans = ~ . / tf_ratio, name = "Cases: 0-4 years old")
  ) +
  labs(x = "date of data collection") +
  theme_minimal() +
  theme(legend.title = element_blank())
  


```



## Miscellaneous  


### Numeric display {-}  

You can disable scientific notation by running this command prior to plotting.  

```{r, eval=F}
options(scipen=999)
```

Or apply `number_format()` from the **scales** package to a specific value or column, as shown below.  

Use functions from the package **scales** to easily adjust how numbers are displayed. These can be applied to columns in your data frame, but are shown on individual numbers for purpose of example.  

```{r}
scales::number(6.2e5)
scales::number(1506800.62,  accuracy = 0.1,)
scales::comma(1506800.62, accuracy = 0.01)
scales::comma(1506800.62, accuracy = 0.01,  big.mark = "." , decimal.mark = ",")
scales::percent(0.1)
scales::dollar(56)
scales::scientific(100000)
```

## Resources

Inspiration
[ggplot graph gallery](https://www.tidyverse.org/blog/2018/07/ggplot2-3-0-0/)

Presentation of data
European Centre for Disease Prevention and Control [Guidelines of presentation of surveillance data](https://ecdc.europa.eu/sites/portal/files/documents/Guidelines%20for%20presentation%20of%20surveillance%20data-final-with-cover-for-we....pdf) 


Facets and labellers
[Using labellers for facet strips](http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#modifying-facet-label-text)
[Labellers](https://ggplot2.tidyverse.org/reference/labellers.html)

Adjusting order with factors
[fct_reorder](https://forcats.tidyverse.org/reference/fct_reorder.html)  
[fct_inorder](https://forcats.tidyverse.org/reference/fct_inorder.html)  
[How to reorder a boxplot](https://cmdlinetips.com/2019/02/how-to-reorder-a-boxplot-in-r/)  
[Reorder a variable in ggplot2](https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2.html)  
[R for Data Science - Factors](https://r4ds.had.co.nz/factors.html)  

Legends  
[Adjust legend order](https://stackoverflow.com/questions/38425908/reverse-stacking-order-without-affecting-legend-order-in-ggplot2-bar-charts)  

Captions
[Caption alignment](https://stackoverflow.com/questions/64701500/left-align-ggplot-caption)  

Labels  
[ggrepel](https://ggrepel.slowkow.com/articles/examples.html)  

Cheatsheets  
[Beautiful plotting with ggplot2](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/)  




TO DO - Under construction


Using option `label_wrap_gen` in facet_wrap to have multiple strip lines
labels and colors of strips

Axis text vertical adjustment
rotation
Labellers

limit range with limit() and coord_cartesian(), ylim(), or scale_x_continuous()
theme_classic()

expand = c(0,0)
coord_flip()
tick marks

ggrepel
animations

remove
remove title
using fill = or color = in labs()
flip order / don't flip order
move location
color?    theme(legend.title = element_text(colour="chocolate", size=16, face="bold"))+ scale_color_discrete(name="This color is\ncalled chocolate!?")
Color of boxes behind points in legend 
     theme(legend.key=element_rect(fill='pink'))   or use fill = NA to remove them. http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/ 
Change size of symbols in legend only guides(colour = guide_legend(override.aes = list(size=4)))


Turn off a layer in the legend
geom_text(data=nmmaps, aes(date, temp, label=round(temp)), size=4)
geom_text(data=nmmaps, aes(date, temp, label=round(temp), size=4), show_guide=FALSE)

Force a legend even if there is no aes(). 
ggplot(nmmaps, aes(x=date, y=o3))+
     geom_line(aes(color="Important line"))+
     geom_point(aes(color="My points"))
Control the shape in the legend with guides - a list with linetype and shape
ggplot(nmmaps, aes(x=date, y=o3))+geom_line(aes(color="Important line"))+
   geom_point(aes(color="Point values"))+
  scale_colour_manual(name='', values=c('Important line'='grey', 'Point values'='red'), guide='legend') +
  guides(colour = guide_legend(override.aes = list(linetype=c(1,0)
                                                      , shape=c(NA, 16))))
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/ggplot_tips.Rmd-->


# Epidemic curves { }  

```{r, out.width=c('75%'), echo=F, message=F}
# import linelist
pacman::p_load(tidyverse, incidence2, lubridate, stringr, here)
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# Create incidence object, data grouped by gender
#################################################

# Classify "gender" column as factor
####################################
# with specific level order and labels, includin for missing values
central_data <- linelist %>% 
  filter(hospital == "Central Hospital") %>% 
  mutate(gender = factor(gender,
                         levels = c(NA, "f", "m"),
                         labels = c("Missing", "Female", "Male"),
                         exclude = NULL))

# Create incidence object, by gender
####################################
gender_outbreak_central <- incidence(
  central_data,
  date_index = date_onset, 
  interval = "week", 
  groups = gender,
  na_as_group = TRUE)   # Missing values assigned their own group

# plot epicurve with modifications
##################################
plot(gender_outbreak_central,
     show_cases = TRUE, fill = gender)+                            # show box around each case
     
     ### ggplot commands added to plot
     # scale modifications
     scale_x_date(expand = c(0,0),
                  date_breaks = "months",
                  date_minor_breaks = "week",
                  date_labels = "%b\n%Y")+
  
    scale_y_continuous(
      expand = c(0,0),
      breaks = seq(0, 35, 5))+
  
      # scale colors
     scale_fill_manual(values = c("grey2", "brown", "turquoise4"))+
  
     # aesthetic themes
     theme_classic()+                               # simplify plot background
  
  
     theme(
       legend.title = element_text(size = 14, face = "bold"),
       axis.title = element_text(face = "bold"),
       plot.caption = element_text(hjust=0, face = "italic"))+   # axis title bold
     
      # plot labels
      labs(fill = "Gender",                         # title of legend
           title = "Weekly case incidence, by gender",
           y = "Weekly case incidence",
           x = "Week of symptom onset",
           caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))      
``` 

An epidemic curve (also known as an "epi curve") is a core epidemiological chart typically used to visualize the temporal pattern of illness onset among a cluster or epidemic of cases.  

Analysis of the epicurve can reveal temporal trends, outliers, the magnitude of the outbreak, the most likely time period of exposure, time intervals between case generations, and can even help identify the mode of transmission of an unidentified disease (e.g. point source, continuous common source, person-to-person propagation). One online lesson on interpretation of epi curves can be found at the website of the [US CDC](https://www.cdc.gov/training/quicklearns/epimode/index.html).    

In this page we demonstrate two approaches to producing epicurves in R:  

* The **incidence2** package, which can produce an epi curve with simple commands  
* The **ggplot2** package, which allows for advanced customizability via more complex commands  

Also addressed are specific use-cases such as:  

* Plotting aggregated count data  
* Faceting or producing small-multiples  
* Applying moving averages  
* Showing which data are "tentative" or subject to reporting delays  
* Overlaying cumulative case incidence using a second axis  

<!-- ======================================================= -->
## Preparation


### Packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r message=F, warning=F}
pacman::p_load(
  rio,          # file import/export
  here,         # relative filepaths 
  lubridate,    # working with dates/epiweeks
  aweek,        # alternative package for working with dates/epiweeks
  incidence2,   # epicurves of linelist data
  i2extras,     # supplement to incidence2
  stringr,      # search and manipulate character strings
  forcats,      # working with factors
  RColorBrewer, # Color palettes from colorbrewer2.org
  tidyverse     # data management + ggplot2 graphics
) 
```


### Import data {-}

Two example datasets are used in this section:  

* Linelist of individual cases from a simulated epidemic  
* Aggregated counts by hospital from the same simulated epidemic  

The datasets are imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.  


```{r, echo=F, message=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```


**Case linelist**

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instruction in the [Download book and data] page.  

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



**Case counts aggregated by hospital**  

For the purposes of the handbook, the dataset of weekly aggregated counts by hospital is created from the `linelist` with the following code. 

```{r, eval=F}
# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```

The first 50 rows are displayed below:  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




### Set parameters {-}

For production of a report, you may want to set editable parameters such as the date for which the data is current (the "data date"). 
You can then reference the object `data_date` in your code when applying filters or in dynamic captions.

```{r set_parameters}
## set the report date for the report
## note: can be set to Sys.Date() for the current date
data_date <- as.Date("2015-05-15")
```



### Verify dates {-}

Verify that each relevant date column is class Date and has an appropriate range of values. You can do this simply using `hist()` for histograms, or `range()` with `na.rm=TRUE`, or with `ggplot()` as below.  

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# check range of onset dates
ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset))
```



<!-- ======================================================= -->
## Epicurves with `incidence2` package { }

Below we demonstrate how to make epicurves using the **incidence2** package. The authors of this package have tried to allow the user to create and modify epicurves without needing to know **ggplot2** syntax. Much of this page is adapted from the package vignettes, which can be found at the **incidence2** [github page](https://github.com/reconhub/incidence2).   


<!-- ======================================================= -->
### Simple example {-}

**2 steps are required to plot an epidemic curve with the *incidence2* package:**  

1) **Create** an *incidence object* (using the function `incidence()`)  
    + Provide the data  
    + Specify the date column to `date_index = `  
    + Specify the `interval = ` into which the cases should be aggregated (daily, weekly, monthly..)  
    + Specify any grouping columns (e.g. gender, hospital, outcome)  
2) **Plot** the incidence object  
    + Specify labels, colors, titles, etc.  


Below, we load the **incidence2** package, create the incidence object from the `linelist` on column `date_onset` and aggregated cases by day. We then print a summary of the incidence object. 

```{r, warning=F, message=F}
# load incidence2 package
pacman::p_load(incidence2)

# create the incidence object, aggregating cases by day
epi_day <- incidence(       # create incidence object
  x = linelist,             # dataset
  date_index = date_onset,  # date column
  interval = "day"          # date grouping interval
  )

# print summary of the incidence object
summary(epi_day)
```

To plot the *incidence* object, use `plot()` on the *name of the incidence object*. In the background, the function `plot.incidence2()` is called, so to read the **incidence2**-specific documentation you would run `?plot.incidence2`.  

```{r}
# plot the incidence object
plot(epi_day)
```


### Change time interval of case aggregation {-}  
The `interval` argument of `incidence()` defines how the observations are grouped into vertical bars. 

**Specify interval**  

**incidence2** provides flexibility and understandable syntax for specifying how you want to aggregate your cases into epicurve bars. Provide a value like the ones below to the `interval = ` argument:   

Argument option | Further explanation 
------------------- | ------------------------------------ |
Number (1, 7, 13, 14, etc.) | Number of days  
"week" | note: Monday start day is default
"2 weeks" | or 3, 4, 5...
"Sunday week" | weeks beginning on Sundays (could also use Thursday, etc.)
"2 Sunday weeks" | or 3, 4, 5...
"MMWRweek" | week starts on Sundays - see US CDC
"month" | 1st of month
"quarter" | 1st of month of quarter
"2 months" | or 3, 4, 5...
"year" | 1st day of calendar year


Below are examples of how different intervals look when applied to the linelist. Note how the default format and frequency of the date *labels* on the x-axis change as the date interval changes.  

```{r incidence, out.width=c('50%', '50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# Create the incidence objects (with different intervals)
##############################
# Weekly (Monday week by default)
epi_wk      <- incidence(linelist, date_onset, interval = "Monday week")

# Sunday week
epi_Sun_wk  <- incidence(linelist, date_onset, interval = "Sunday week")

# Three weeks (Monday weeks by default)
epi_2wk     <- incidence(linelist, date_onset, interval = "2 weeks")

# Monthly
epi_month   <- incidence(linelist, date_onset, interval = "month")

# Quarterly
epi_quarter   <- incidence(linelist, date_onset, interval = "quarter")

# Years
epi_year   <- incidence(linelist, date_onset, interval = "year")


# Plot the incidence objects (+ titles for clarity)
############################
plot(epi_wk)+      labs(title = "Monday weeks")
plot(epi_Sun_wk)+  labs(title = "Sunday weeks")
plot(epi_2wk)+     labs(title = "2 (Monday) weeks")
plot(epi_month)+   labs(title = "Months")
plot(epi_quarter)+ labs(title = "Quarters")
plot(epi_year)+    labs(title = "Years")

```


**Begin at first case**  

If you want the intervals to begin at the first case, you can add the argument `standard = TRUE` to the `incidence()` command. This only works if the interval is either "week", "month", "quarter" or "year".  

**First and late date**  

You can optionally specify the `first_date = ` and `last_date = ` in the `incidence()` command. If given, the data will be trimmed to this range.  



### Groups {-}

Groups are specified in the `incidence()` command, and can be used to color the bars or to facet the data. To specify groups in your data provide the column name(s) to the `groups =` argument in the `incidence()` command (no quotes). If specifying multiple columns, put their names within `c()`.

You can specify that cases with missing values in the grouping columns be listed as a distinct `NA` group by setting `na_as_group = TRUE`. Otherwise, they will be excluded from the plot.   

* To color the bars by a grouping column*, you must again provide the column name to `fill = ` in the `plot()` command.  

* To facet based on a grouping column*, see the section below on facets with **incidence2**.  

In the example below, the cases in the whole outbreak are grouped by their age category. Missing values are included as a group. The epicurve interval is weeks.  


```{r, message=F, warning=F}
# Create incidence object, with data grouped by age category
age_outbreak <- incidence(
  linelist,                # dataset
  date_index = date_onset, # date column
  interval = "week",       # Monday weekly aggregation of cases
  groups = age_cat,        # age_cat is set as a group
  na_as_group = TRUE)      # missing values assigned their own group

# plot the grouped incidence object
plot(
  age_outbreak,             # incidence object with age_cat as group
  fill = age_cat)+          # age_cat is used for bar fill color (must have been set as a groups column above)
labs(fill = "Age Category") # change legend title from default "age_cat" (this is a ggplot2 modification)
```
<span style="color: darkgreen;">**_TIP:_** Change the title of the legend by adding `+` the **ggplot2** command `labs(fill = "your title")` to your **incidence2** plot.</span>  

You can also have the grouped bars display side-by-side by setting `stack = FALSE` in `plot()`, as shown below:  

```{r, warning=F, message=F}
# Make incidence object of monthly counts. 
monthly_gender <- incidence(
 linelist,
 date_index = date_onset,
 interval = "month",
 groups = gender            # set gender as grouping column
)

plot(
  monthly_gender,   # incidence object
  fill = gender,    # display bars colored by gender
  stack = FALSE)    # side-by-side (not stacked)
``` 






### Filtered data {-}

To plot the epicurve of a subset of data:  

1) Filter the linelist data  
2) Provide the filtered data to the `incidence()` command  
3) Plot the incidence object

The example below uses data filtered to show only cases at Central Hospital.  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(central_data, date_index = date_onset, interval = "week")

# plot the incidence object
plot(central_outbreak) + labs(title = "Weekly case incidence at Central Hospital")
```




### Aggregated counts {-}

If your original data are aggregated (counts), provide the name of the column that contains the case counts to the `count = ` argument when creating the incidence object.  

For example, this data frame `count_data` is the linelist aggregated into daily counts by hospital. The first 50 rows look like this:  

```{r message=FALSE, echo=F}
DT::datatable(head(count_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

If you are beginning your analysis with daily count data like the dataset above, your `incidence()` command to convert this to a weekly epicurve by hospital would look like this:  

```{r}
epi_counts <- incidence(
  count_data,                         # dataset with counts aggregated by day
  date_index = date_hospitalisation,  # column with dates
  count = n_cases,                    # column with counts
  interval = "week",                  # aggregate daily counts up to weeks
  groups = hospital                   # group by hospital
  )

# plot the weekly incidence epi curve, with stacked bars by hospital
plot(epi_counts,                      # incidence object
     fill = hospital)                 # color the bars by hospital
```




### Facets/small multiples {-}  

To facet the data by group (i.e. produce "small multiples"):  

1) Specify the faceting column to `groups = ` when you create the incidence object  
2) Use the `facet_plot()` command instead of `plot()`  
3) Specify which grouping columns to use as `fill = ` and which to use as `facets = `  

Below, we set both columns `hospital` and `outcome` as grouping columns in the `incidence()` command. Then, in `facet_plot()` we plot the epicurve, specifying that we want a different epicurve for each hospital and that within each epicurve the bars should be stacked and colored by outcome.  
 

```{r, warning=F, message=F}
epi_wks_hosp_out <- incidence(
  linelist,                      # dataset
  date_index = date_onset,       # date column
  interval = "month",            # monthly bars  
  groups = c(outcome, hospital)  # both outcome and hospital are given as grouping columns
  )

# plot
facet_plot(
  epi_wks_hosp_out,    # incidence object
  facets = hospital,   # facet column
  fill = outcome)      # fill column

```




### Modifications with `plot()` {-} 

An epicurve produced by **incidence2** can be modified via these arguments *within the **incidence2** `plot()` function*.  

**Here are `plot()` arguments relating to the bars:**  

Argument | Description | Examples
------------------|---------------------------------------|-------------------
`fill = `|Bar color. Either a color name or a column name previously specified to `groups = ` in `incidence()` command|`fill = "red"`, or `fill = gender`  
`na_as_group = `|If FALSE, missing values of a grouping column are excluded from plot|`na_as_group = FALSE`
`border = ` |Color around each bar/box, or grouping within a bar|`border = "white"` 
`legend = `|Location of legend|One of "bottom", "top", "left", "right", or "none"  
`alpha = `|Transparency of bars/boxes|1 is fully opaque, 0 is fully transparent
`show_cases = `|Logical; if TRUE, each case shows as a box. Displays best on smaller outbreaks. Can be used with `coord_equal = T` to ensure squares.|`show_cases = TRUE`
`coord_equal = `|For use if `show_cases = TRUE`; if TRUE the x and y axes display with equal ratio|`coord_equal = TRUE`


**Here are `plot()` arguments relating to the date axis:**  

Argument(s)|Description
----------------------|----------------------------------------------------
`n_breaks = `|Number of x-axis label breaks, absent other modifications. These will start counting from the interval of the first case.  
`format = `|Accepts character string of desired format. See `?strptime` or Handbook page on [Working with dates]. For example "%d %b" for "02 Feb".  
`angle = `|Angle of x-axis date labels|`angle = 45`  
`centre_ticks = `|If TRUE, date labels are centered within bars. Only works if interval is singular (one week, month, quarter, or year)  
`group_labels = `|Used if plotting weekly interval. If FALSE labels will appear YYYY-MM-DD, else YYYY-Www (absent other modifications)

<span style="color: darkgreen;">**_TIP:_** For breaks every "nth" interval (e.g. every 4th), use `n_breaks = nrow(i)/n` (where “i” is your incidence object name and “n” is a number). If your data are grouped, you will need to multiply "n" by the number of unique groups.</span>  



**Here are `plot()` arguments relating to labels:**

Argument(s)|Description
----------------------|----------------------------------------------------
`title = `|Title of plot|`title = "Epidemic curve of Acute Jaundice Syndrome (AJS)"`
`xlab = `|Title of x-axis|`xlab = "Date of onset"`  
`ylab = `|Title of y-axis|`ylab = "Daily case"`  
`size = `|Size of x-axis text in pts (use ggplot's theme() to adjust other sizes etc.)  


An example using many of the above arguments:  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = c(outcome))

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  centre_ticks = TRUE,                  # ticks appear in center of interval
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  format = "%a %d %B\n%Y (Week %W)",    # adjust date label format - see dates Handbook page
  n_breaks = nrow(central_outbreak)/15, # date labels every X weeks
  angle = 45                            # angle of date labels
  )
```

To further adjust plot appearance, see the section below on applying `ggplot()` to the **incidence** plot.  






### Modifications with ggplot2 {-}

You can further modify an **incidence2** plot by adding **ggplot2** modifications with a `+` after the close of the incidence `plot()` function, as demonstrated below.  

Below, the **incidence2** plot finishes and then **ggplot2** commands are used to modify the axes, add a caption, and adjust the bold font and text size.  

Note that if you add `scale_x_date()`, most date formatting from `plot()` will be overwritten. See the `ggplot()` epicurves section and the Handbook page [ggplot tips] for more options.  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = c(outcome))

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  centre_ticks = TRUE,                  # ticks appear in center of interval
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  #format = "%a %d %B\n%Y (Week %W)",    # overwritten below
  #n_breaks = nrow(central_outbreak)/15, # overwritten below
  angle = 45                           # angle of date labels
  )+
  
  # Add modifications using ggplot() functions
  ############################################
  scale_x_date(                              # converts to ggplot date scale (changes default label format)
    expand = c(0,0),                         # remove excess space on left and right
    date_labels = "%a %d %B\n%Y (Week %W)",  # set how dates appear
    date_breaks = "6 weeks"                  # set how often dates appear
    )+      
  
  scale_y_continuous(
    breaks = seq(from = 0, to = 30, by = 5),  # specify y-axis increments by 5
    expand = c(0,0))+                         # remove excess space below 0 on y-axis
  
  # add dynamic caption
  labs(
    fill = "Patient outcome",                               # Legend title
    caption = stringr::str_glue(                            # dynamic caption - see page on characters and strings for details
      "n = {central_cases} from Central Hospital
      Case onsets range from {earliest_date} to {latest_date}. {missing_onset} cases are missing date of onset and not shown",
      central_cases = nrow(central_data),
      earliest_date = format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),
      latest_date = format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),      
      missing_onset = nrow(central_data %>% filter(is.na(date_onset)))))+
  
  # adjust bold face, and caption position
  theme(
    axis.title = element_text(size = 12, face = "bold"),    # axis titles larger and bold
    axis.text = element_text(size = 10, face = "bold"),     # axis text size and bold
    plot.caption = element_text(hjust = 0, face = "italic") # move caption to left
  )
  
```




### Change colors  {-}  

#### Specify a palette {-}  

Provide the name of a pre-defined palette to the `col_pal = ` argument in `plot()`. The **incidence2** package comes with 2 pre-defined paletted: "vibrant" and "muted". In "vibrant" the first 6 colors and distinct and in "muted" the first 9 colors are distinct. After these numbers, the colors are interpolations/intermediaries of other colors. These pre-defined palettes can be found at [this website](https://personal.sron.nl/~pault/#sec:qualitative). The palettes exclude grey, which is reserved for missing data (use `na_color = ` to change this default).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# Create incidence object, with data grouped by age category  
age_outbreak <- incidence(
  linelist,
  date_index = date_onset,   # date of onset for x-axis
  interval = "week",         # weekly aggregation of cases
  groups = age_cat)

# plot the epicurve with default palette
plot(age_outbreak, fill = age_cat, title = "'vibrant' default incidence2 palette")

# plot with different color palette
#plot(age_outbreak, fill = age_cat, col_pal = muted, title = "'muted' incidence2 palette")
```

You can also use one of the **base** R palettes (put the name of the palette *without* quotes).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = heat.colors, title = "base R heat.colors palette")

# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = rainbow, title = "base R rainbow palette")
```

You can also add a color palette from the **viridis** package or **RColorBrewer** package. First those packages must be loaded, then add their respective `scale_fill_*()` functions with a `+`, as shown below.

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
pacman::p_load(RColorBrewer, viridis)

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "Viridis palette")+
  scale_fill_viridis_d(
    option = "inferno",     # color scheme, try also "plasma" or the default
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "RColorBrewer palette")+
  scale_fill_brewer(
    palette = "Dark2",      # color palette, try also Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values
```


#### Specify manually {-}  

To specify colors manually, add the **ggplot2** function `scale_fill_manual()` to the `plot()` with a `+` and provide the vector of colors names or HEX codes to the argument `values = `. The number of colors listed must equal the number of groups. Be aware of whether missing values are a group - they can be converted to a character value like "Missing" during your data preparation with the function `fct_explicit_na()` as explained in the page on [Factors].  

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# manual colors
plot(age_outbreak, fill = age_cat, title = "Manually-specified colors")+
  scale_fill_manual(
    values = c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange", "red", "lightblue"),  # colors
    name = "Age Category")      # Name for legend
```

As mentioned in the [ggplot tips] page, you can create your own palettes using `colorRampPalette()` on a vector of colors and specifying the number of colors you want in return. This is a good way to get many colors in a ramp by specifying a few.  

```{r}
my_cols <- c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange")
my_palette <- colorRampPalette(my_cols)(12)  # expand the 6 colors above to 12 colors
my_palette
```
          
          
### Adjust level order {-}  

To adjust the order of group appearance (on plot and in legend), the grouping column must be class Factor. See the page on [Factors] for more information.  

First, let's see a weekly epicurve by hospital with the default ordering:  

```{r, message=F, warning=F}
# ORIGINAL - hospital NOT as factor
###################################

# create weekly incidence object, rows grouped by hospital and week
hospital_outbreak <- incidence(
  linelist,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak, fill = hospital, title = "ORIGINAL - hospital not a factor")
```

Now, to adjust the order so that "Missing" and "Other" are at the top of the epicurve we can do the following:  

* Load the package **forcats**, to work with factors  
* Adjust the dataset - in this case we'll define a new dataset (`plot_data`) in which:  
  * the `gender` column is defined as a factor with `as_factor()`  
  * the order of levels are defined with `fct_relevel()` so that "Other" and "Missing" are first so they appear at the top of the bars  
* The incidence object is created and plotted as before  
* We add **ggplot2** modifications  
  * `scale_fill_manual()` to manually assign colors so that "Missing" is grey and "Other" is beige  
 



```{r, message=F, warning=F}
# MODIFIED - hospital as factor
###############################

# load forcats package for working with factors
pacman::p_load(forcats)

# Convert hospital column to factor and adjust levels
plot_data <- linelist %>% 
  mutate(hospital = as_factor(hospital)) %>%                      # define as factor
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Set "Missing" and "Other" as top levels


# Create weekly incidence object, grouped by hospital and week
hospital_outbreak_mod <- incidence(
  plot_data,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak_mod, fill = hospital)+
  
  # manual specify colors
  scale_fill_manual(values = c("grey", "beige", "darkgreen", "green2", "orange", "red", "pink"))+                      

  # labels added via ggplot
  labs(
      title = "MODIFIED - hospital as factor",   # plot title
      subtitle = "Other & Missing at top of epicurve",
      y = "Weekly case incidence",               # y axis title  
      x = "Week of symptom onset",               # x axis title
      fill = "Hospital")                         # title of legend     
```

<span style="color: darkgreen;">**_TIP:_** If you want to reverse the order of the legend only, add this **ggplot2** command `guides(fill = guide_legend(reverse = TRUE))`.</span>  



### Vertical gridlines {-}  

If you plot with default **incidence2** settings, you may notice that the vertical gridlines appear at each date label and once between each date label. This can result in gridlines intersecting with the top of some bars.  

You can specify the interval for the gridlines by adding **ggplot2**'s `scale_x_date()` command to your **incidence2** plot. Within it, specify the intervals for `date_breaks = ` and `date_minor_breaks = ` (e.g. "weeks" or "3 weeks" or "months"). Note that use of `scale_x_date()` will over-ride any formatting of the date labels in `plot()`, so you will need to specify any string format to `date_labels = ` as below.  

You can also remove all gridlines by adding the **ggplot2** command `theme_classic()`.  

```{r, warning=F, message=F, out.width = c('50%', '50%', '50%'), fig.show='hold'}
# make incidence object
a <- incidence(
  central_data,
  date_index = date_onset,
  interval = "Monday weeks"
)

# Default gridlines
plot(a, title = "Default lines")

# Specified gridline intervals
plot(a, title = "Weekly lines")+
  scale_x_date(
    date_breaks = "4 weeks",      # major vertical lines align on weeks
    date_minor_breaks = "weeks",  # minor vertical lines every week
    date_labels = "%a\n%d\n%b")   # format of date labels

# No gridlines
plot(a, title = "No lines")+
  theme_classic()                 # remove all gridlines
```

Note however, that if using weeks, the `date_breaks` and `date_minor_breaks` arguments only work for *Monday* weeks. If your weeks are by another day of the week you will need to manually provide a vector of dates to the `breaks = ` and `minor_breaks = ` arguments instead. See the **ggplot2** section for examples of this using `seq.Date()`.

### Cumulative incidence {-}  

You can easily produce a plot of cumulative incidence by passing the incidence object to the **incidence2** command `cumulate()` and then to `plot()`. This also works with `facet_plot()`.  

```{r}
# make weekly incidence object
wkly_inci <- incidence(
  linelist,
  date_index = date_onset,
  interval = "week"
)

# plot cumulative incidence
wkly_inci %>% 
  cumulate() %>% 
  plot()
```


See the section farther down on this page for alternative method to plot cumulative incidence with **ggplot2** - for example to overlay a cumulative incidence line over an epicurve.  

### Rolling average  {-}

You can add a rolling average to an **incidence2** plot easily with `add_rolling_average()` from the **i2extras** package. Pass your incidence2 object to this function, and then to `plot()`. Set `before = ` as the number of previous days you want included in the rolling average (default is 2). If your data are grouped, the rolling average will be calculated per group. 

```{r, warning=F, message=F}
rolling_avg <- incidence(                    # make incidence object
  linelist,
  date_index = date_onset,
  interval = "week",
  groups = gender) %>% 
  
  i2extras::add_rolling_average(before = 6)  # add rolling averages (in this case, by gender)

# plot
plot(rolling_avg, n_breaks = 3, format = "%d %b\n%Y") # faceted automatically because rolling average on groups
```

To learn how to apply rolling averages more generally on data, see the Handbook page on [Rolling averages].  


<!-- ======================================================= -->
## Epicurves with ggplot2 { }

Using `ggplot()` to build your epicurve allows for more flexibility, but requires more effort and understanding of how `ggplot()` works. It is also easier to accidentally make mistakes.  

Unlike using the **incidence2** package, you must *manually* control the aggregation of the cases by time (into weeks, months, etc) *and* the intervals of the labels on the date axis. This must be carefully managed.  

One advantage of using **ggplot2** is that you can create a histogram without any lines between the bars. This is a more traditional epidemic curve.  

These examples use a subset of the `linelist` dataset - only the cases from Central Hospital.  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r}
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")
```

```{r, eval=F, echo=F}
detach("package:tidyverse", unload=TRUE)
library(tidyverse)
```


To produce an epicurve with `ggplot()` there are three main elements:  

* A histogram, with linelist cases aggregated into "bins" distinguished by specific "break" points  
* Scales for the axes and their labels  
* Themes for the plot appearance, including titles, labels, captions, etc.



### Specify case bins {-}  

Here we show how to specify how cases will be aggregated into histogram bins ("bars"). It is important to recognize that the aggregation of cases into histogram bins is **not** necessarily the same intervals as the dates that will appear on the x-axis. 

Below is perhaps the most simple code to produce daily and weekly epicurves.  

In the over-arching `ggplot()` command the dataset is provided to `data = `. Onto this foundation, the geometry of a histogram is added with a `+`. Within the `geom_histogram()`, we map the aesthetics such that the column `date_onset` is mapped to the x-axis. Also within the `geom_histogram()` but *not* within `aes()` we set the `binwidth =` of the histogram bins, in days. If this **ggplot2** syntax is confusing, review the page on [ggplot tips].  

<span style="color: orange;">**_CAUTION:_** Plotting weekly cases by using `binwidth = 7` starts the first 7-day bin at the first case, which could be any day of the week! To create specific weeks, see section below .</span>


``` {r ggplot_simple,  out.width = c('50%', '50%'), fig.show='hold', warning= F, message = F}
# daily 
ggplot(data = central_data) +          # set data
  geom_histogram(                      # add histogram
    mapping = aes(x = date_onset),     # map date column to x-axis
    binwidth = 1)+                     # cases binned by 1 day 
  labs(title = "Central Hospital - Daily")                # title

# weekly
ggplot(data = central_data) +          # set data 
  geom_histogram(                      # add histogram
      mapping = aes(x = date_onset),   # map date column to x-axis
      binwidth = 7)+                   # cases binned every 7 days, starting from first case (!) 
  labs(title = "Central Hospital - 7-day bins, starting at first case") # title
```

Let us note that the first case in this Central Hospital dataset had symptom onset on:  

```{r}
format(min(central_data$date_onset, na.rm=T), "%A %d %b, %Y")
```

**To manually specify the histogram bin breaks, do *not* use the `binwidth = ` argument, and instead supply a vector of dates to `breaks = `.**  

Create the vector of dates with the **base** R function `seq.Date()`. This function expects arguments `to = `, `from = `, and `by = `. For example, the command below returns monthly dates starting at Jan 15 and ending by June 28.

```{r}
monthly_breaks <- seq.Date(from = as.Date("2014-02-01"), to = as.Date("2015-07-15"), by = "months")

monthly_breaks   # print
```

This vector can be provided to `geom_histogram()` as `breaks = `:  

```{r, warning=F, message=F}
# monthly 
ggplot(data = central_data) +  
  geom_histogram(
    mapping = aes(x = date_onset),
    breaks = monthly_breaks)+         # provide the pre-defined vector of breaks                    
  labs(title = "Monthly case bins")   # title
```

A simple weekly date sequence can be returned by setting `by = "week"`. For example: 

```{r}
weekly_breaks <- seq.Date(from = as.Date("2014-02-01"), to = as.Date("2015-07-15"), by = "week")
```

 
An alternative to supplying specific start and end dates is to write *dynamic* code so that weekly bins begin *the Monday before the first case*. **We will use these date vectors throughout the examples below.**  
     
```{r}
# Sequence of weekly Monday dates for CENTRAL HOSPITAL
weekly_breaks_central <- seq.Date(
  from = as.Date(floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1)), # monday before first case
  to   = as.Date(ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1)), # monday after last case
  by   = "week")
```  

Let's unpack the rather daunting code above:  

* The "from" value (earliest date of the sequence) is created as follows: the minimum date value (`min()` with `na.rm=TRUE`) in the column `date_onset` is fed to `floor_date()` from the **lubridate** package. `floor_date()` set to "week" returns the start date of that cases's "week", given that the start day of each week is a Monday (`week_start = 1`).  
* Likewise, the "to" value (end date of the sequence) is created using the inverse function `ceiling_date()` to return the Monday *after* the last case.  
* The "by" argument of `seq.Date()` can be set to any number of days, weeks, or months.   
* Use `week_start = 7` for Sunday weeks  

As we will use these date vectors throughout this page, we also define one for the whole outbreak (the above is for Central Hospital only).  

```{r}
# Sequence for the entire outbreak
weekly_breaks_all <- seq.Date(
  from = as.Date(floor_date(min(linelist$date_onset, na.rm=T),   "week", week_start = 1)), # monday before first case
  to   = as.Date(ceiling_date(max(linelist$date_onset, na.rm=T), "week", week_start = 1)), # monday after last case
  by   = "week")
```

These `seq.Date()` outputs can be used to create histogram bin breaks, but also the breaks for the date labels, which may be independent from the bins. Read more about the date labels in later sections.  

<span style="color: darkgreen;">**_TIP:_** For a more simple `ggplot()` command, save the bin breaks and date label breaks as named vectors in advance, and simply provide their names to `breaks = `.</span>  







### Weekly epicurve example {-}  

**Below is detailed example code to produce a weekly epicurves for Monday weeks, with aligned bars, date labels, and vertical gridlines.** This section is for the user who needs code quickly. To understand each aspect (themes, date labels, etc.) in-depth, continue to the subsequent sections. Of note:  

* The *histogram bin breaks* are defined with `seq.Date()` as explained above to begin the Monday before the earliest case and to end the Monday after the last case  
* The interval of *date labels* is specified by `date_breaks =` within `scale_x_date()`  
* The interval of the minor vertical gridlines between the date labels is specified to `date_minor_breaks = `  
* `expand = c(0,0)` in the x and y scales removes excess space on each side of the axes, which also ensures the date labels begin from the first bar.  

```{r, warning=F, message=F}
# TOTAL MONDAY WEEK ALIGNMENT
#############################
# Define sequence of weekly breaks
weekly_breaks_central <- seq.Date(
      from = as.Date(floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1)), # Monday before first case
      to   = as.Date(ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1)), # Monday after last case
      by   = "week")    # bins are 7-days 


ggplot(data = central_data) + 
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    
    # mapping aesthetics
    mapping = aes(x = date_onset),  # date column mapped to x-axis
    
    # histogram bin breaks
    breaks = weekly_breaks_central, # histogram bin breaks defined previously
    
    # bars
    color = "darkblue",     # color of lines around bars
    fill = "lightblue"      # color of fill within bars
  )+ 
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),           # remove excess x-axis space before and after case bars
    date_breaks       = "4 weeks",        # date labels and major vertical gridlines appear every 3 Monday weeks
    date_minor_breaks = "week",           # minor vertical lines appear every Monday week
    date_labels       = "%a\n%d %b\n%Y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+             # remove excess y-axis space below 0 (align histogram flush with x-axis)
  
  # aesthetic themes
  theme_minimal()+                # simplify plot background
  
  theme(
    plot.caption = element_text(hjust = 0,        # caption on left side
                                face = "italic"), # caption in italics
    axis.title = element_text(face = "bold"))+    # axis titles in bold
  
  # labels including dynamic caption
  labs(
    title    = "Weekly incidence of cases (Monday weeks)",
    subtitle = "Note alignment of bars, vertical gridlines, and axis labels on Monday weeks",
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### Sunday weeks {-}  

To achieve the above plot for Sunday weeks a few modifications are needed, because the `date_breaks = "weeks"` only work for Monday weeks.  

* The break points of the *histogram bins* must be set to Sundays (`week_start = 7`)  
* Within `scale_x_date()`, the similar date breaks should be provided to `breaks =` and `minor_breaks = ` to ensure the date labels and vertical gridlines align on Sundays.  

For example, the `scale_x_date()` command for Sunday weeks could look like this:  

```{r, eval=F}
scale_x_date(
    expand = c(0,0),
    
    # specify interval of date labels and major vertical gridlines
    breaks = seq.Date(
      from = as.Date(floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7)), # Sunday before first case
      to   = as.Date(ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7)), # Sunday after last case
      by   = "4 weeks"),
    
    # specify interval of minor vertical gridline 
    minor_breaks = seq.Date(
      from = as.Date(floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7)), # Sunday before first case
      to   = as.Date(ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7)), # Sunday after last case
      by   = "week"),
   
    # date label format
    date_labels = "%a\n%d %b\n%Y")+         # day, above month abbrev., above 2-digit year

```



### Group/color by value {-}

The histogram bars can be colored by group and "stacked". To designate the grouping column, make the following changes. See the [ggplot tips] page for details.  

* Within the histogram aesthetic mapping `aes()`, map the column name to the `group = ` and `fill = ` arguments  
* Remove any `fill = ` argument *outside* of `aes()`, as it will override the one inside  
* Arguments *inside* `aes()` will apply *by group*, whereas any *outside* will apply to all bars (e.g. you may still want `color = ` outside, so each bar has the same border)  

Here is what the `aes()` command would look like to group and color the bars by gender:  

```{r, eval=F}
aes(x = date_onset, group = gender, fill = gender)
```

Here it is applied:  

```{r, warning=F, message=F}
ggplot(data = linelist) +     # begin with linelist (many hospitals)
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = hospital,       # set data to be grouped by hospital
      fill = hospital),       # bar fill (inside color) by hospital
    
    # bin breaks are Monday weeks
    breaks = weekly_breaks_all,   # sequence of weekly Monday bin breaks for whole outbreak, defined in previous code       
    
    # Color around bars
    color = "black")
```


### Adjust colors {-}  

* To *manually* set the fill for each group, use `scale_fill_manual()` (note: `scale_color_manual()` is different!).
  * Use the `values = ` argument to apply a vector of colors.  
  * Use `na.value = ` to specify a color for `NA` values.  
  * To change the text of legend labels you *can* use the `labels = ` argument in `scale_fill_manual()`, but it is dangerously easy to accidentally give colors incorrect legend text! Instead, it is recommended to change legend text by converting the grouping column to class Factor and adjusting its labels as described in the [Factors] page and briefly below.  
* To adjust the colors via a pre-defined color scale, see the page on [ggplot tips].  

```{r, warning=F, message=F}
ggplot(data = linelist)+           # begin with linelist (many hospitals)
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,          # cases grouped by hospital
        fill = hospital),          # bar fill by hospital
    
    # bin breaks
    breaks = weekly_breaks_all,        # sequence of weekly Monday bin breaks, defined in previous code
    
    # Color around bars
    color = "black")+              # border color of each bar
  
  # manual specification of colors
  scale_fill_manual(
    values = c("black", "orange", "grey", "beige", "blue", "brown")) # specify fill colors ("values") - attention to order!
```




### Adjust level order {-}  

The order in which grouped bars are stacked is best adjusted by classifying the grouping column as class Factor. You can then designate the factor level order (and their display labels). See the page on [Factors] or [ggplot tips] for details.  

Before making the plot, convert the grouping column to class Factor using `as_factor()` from the **forcats** package. Then you can make other adjustments to the levels, as detailed in the page on [Factors].  

```{r}
# load forcats package for working with factors
pacman::p_load(forcats)

# Define new dataset with hospital as factor
plot_data <- linelist %>% 
  mutate(hospital = as_factor(hospital)) %>%                      # define hospital as factor
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Set "Missing" and "Other" as top levels to appear on epicurve top

levels(plot_data$hospital) # print levels in order
```

In the below plot, the only differences from previous is that column `hospital` has been consolidated as above, and we use `guides()` to reverse the legend order, so that "Missing" is on the bottom of the legend.  

```{r, warning=F, message=F}
ggplot(plot_data) +                     # Use NEW dataset with hospital as re-ordered factor
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,               # cases grouped by hospital
        fill = hospital),               # bar fill (color) by hospital
    
    breaks = weekly_breaks_all,         # sequence of weekly Monday bin breaks for whole outbreak, defined at top of ggplot section
    
    color = "black")+                   # border color around each bar
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space before and after case bars
    date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
    date_minor_breaks = "week",         # vertical lines appear every Monday week
    date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+                   # remove excess y-axis space below 0
  
  # manual specification of colors, ! attention to order
  scale_fill_manual(
    values = c("grey", "beige", "black", "orange", "blue", "brown"))+ 
  
  # aesthetic themes
  theme_minimal()+                      # simplify plot background
  
  theme(
    plot.caption = element_text(face = "italic", # caption on left side in italics
                                hjust = 0), 
    axis.title = element_text(face = "bold"))+   # axis titles in bold
  
  # labels
  labs(
    title    = "Weekly incidence of cases by hospital",
    subtitle = "Hospital as re-ordered factor",
    x        = "Week of symptom onset",
    y        = "Weekly cases",
    fill     = "Hospital")                        # title of legend
```

<span style="color: darkgreen;">**_TIP:_** To reverse the order of the legend only, add this **ggplot2** command: `guides(fill = guide_legend(reverse = TRUE))`.</span>  





### Adjust legend {-}

Read more about legends in the [ggplot tips] page. Here are a few highlights:  

* `labs(fill = "Legend title")` to edit the legend title (this is because the grouping column is set to `fill = ` in `aes()`)  
* `theme(legend.title = element_blank())` to have no title  
* `theme(legend.position = "top")` (or "bottom", "left", "right", "none")
* `theme(legend.direction = "horizontal")` horizontal legend 
* `guides(fill = guide_legend(reverse = TRUE))` to reverse order of the legend  







### Bars side-by-side {-}  

Side-by-side display of group bars (as opposed to stacked) is specified within `geom_histogram()` with `position = "dodge"` (place this outside of `aes()`).  

If there are more than two value groups, these can become difficult to read. Consider instead using a faceted plot (small multiples). To improve readability in this example, missing gender values could be removed.  

```{r, warning=F, message=F}
ggplot(central_data)+             # begin with Central Hospital cases
    geom_histogram(
        mapping = aes(
          x = date_onset,
          group = gender,         # cases grouped by gender
          fill = gender),         # bars filled by gender
        
        # histogram bin breaks
        breaks = weekly_breaks_central,   # sequence of weekly dates for Central outbreak - defined at top of ggplot section
        
        color = "black",          # bar edge color
        
        position = "dodge")+      # SIDE-BY-SIDE bars
                      
  
  # The labels on the x-axis
  scale_x_date(expand            = c(0,0),         # remove excess x-axis space below and after case bars
               date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
               date_minor_breaks = "week",         # vertical lines appear every Monday week
               date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+             # removes excess y-axis space between bottom of bars and the labels
  
  #scale of colors and legend labels
  scale_fill_manual(values = c("brown", "orange"),  # specify fill colors ("values") - attention to order!
                    na.value = "grey" )+     

  # aesthetic themes
  theme_minimal()+                                               # a set of themes to simplify plot
  theme(plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
        axis.title = element_text(face = "bold"))+               # axis titles in bold
  
  # labels
  labs(title    = "Weekly incidence of cases, by gender",
       subtitle = "Subtitle",
       fill     = "Gender",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported")
```




### Axis limits {-}  

You can set maximum and minimum date values using `limits = c()` within `scale_x_date()`. For example:  

```{r eval=F}
scale_x_date(limits = c(as.Date("2014-04-01"), NA)) # sets a minimum date but leaves the maximum open.  
```

Likewise, if you want to the x-axis to extend to a specific date (e.g. current date), even if no new cases have been reported, you can use:  

```{r eval=F}
scale_x_date(limits = c(NA, Sys.Date()) # ensures date axis will extend until current date  
```

<span style="color: orange;">**_CAUTION:_** Caution using limits! They remove all data outside the limits, which can impact y-axis max/min, modeling, and other statistics. Strongly consider instead using limits by adding `coord_cartesian( xlim= c(), ylim=c() )` to your plot, which acts as a "zoom" without removing data. </span>

<span style="color: red;">**_DANGER:_** Be cautious setting the y-axis scale breaks or limits (e.g. 0 to 30 by 5: `seq(0, 30, 5)`). Such static numbers can cut-off your plot too short if the data changes to exceed the limit!.</span>



### Date-axis labels/gridlines {-} 

<span style="color: darkgreen;">**_TIP:_** Remember that date-axis **labels** are independent from the aggregation of the data into bars, but visually it can be important to align bins, date labels, and vertical grid lines.</span>

To **modify the date labels and grid lines**, use `scale_x_date()` in one of these ways:  

* **If your histogram bins are days, Monday weeks, months, or years**:  
  * Use `date_breaks = ` to specify the interval of labels and major gridlines (e.g. "day", "week", "3 weeks", "month", or "year")
  * Use `date_minor_breaks = ` to specify interval of minor vertical gridlines (between date labels)  
  * Add `expand = c(0,0)` to begin the labels at the first bar  
  * Use `date_labels = ` to specify format of date labels - see the Dates page for tips (use `\n` for a new line)  
* **If your histogram bins are Sunday weeks**:  
  * Use `breaks = ` and `minor_breaks = ` by providing a sequence of date breaks for each
  * You can still use `date_labels = ` and `expand = ` for formatting as described above  

Some notes:  

* See the opening ggplot section for instructions on how to create a sequence of dates using `seq.Date()`.  
* See [this page](https://rdrr.io/r/base/strptime.html) or the [Working with dates] page for tips on creating date labels.  




#### Demonstrations {-}

Below is a demonstration of plots where the bins and the plot labels/grid lines are aligned and not aligned:  

```{r fig.show='hold', class.source = 'fold-hide', warning=F, message=F}
# 7-day bins + Monday labels
#############################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,                 # 7-day bins with start at first case
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),               # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",       # Monday every 3 weeks
    date_minor_breaks = "week",    # Monday weeks
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  scale_y_continuous(
    expand = c(0,0))+              # remove excess space under x-axis, make flush with labels
  
  labs(
    title = "MISALIGNED",
    subtitle = "!CAUTION: 7-day bars start Thursdays at first case\nDate labels and gridlines on Mondays\nNote how ticks don't align with bars")



# 7-day bins + Months
#####################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),                  # remove excess x-axis space below and after case bars
    date_breaks = "months",           # 1st of month
    date_minor_breaks = "week",       # Monday weeks
    date_labels = "%a\n%d %b\n%Y")+    # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush with labels
  
  labs(
    title = "MISALIGNED",
    subtitle = "!CAUTION: 7-day bars start Thursdays with first case\nMajor gridlines and date labels at 1st of each month\nMinor gridlines weekly on Mondays\nNote uneven spacing of some gridlines and ticks unaligned with bars")


# TOTAL MONDAY ALIGNMENT: specify manual bin breaks to be mondays
#################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,    # defined earlier in this page
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "4 weeks",           # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%a\n%d %b\n%Y")+      # label format
  
  labs(
    title = "ALIGNED Mondays",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels and gridlines on Mondays as well")


# TOTAL MONDAY ALIGNMENT WITH MONTHS LABELS:
############################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,            # defined earlier in this page
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "months",            # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%b\n%Y")+          # label format
  
  theme(panel.grid.major = element_blank())+  # Remove major gridlines (fall on 1st of month)
          
  labs(
    title = "ALIGNED Mondays with MONTHLY labels",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels on 1st of Month\nMonthly major gridlines removed")


# TOTAL SUNDAY ALIGNMENT: specify manual bin breaks AND labels to be Sundays
############################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Sunday before first case
    breaks = seq.Date(from = as.Date(floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7)),
                      to   = as.Date(ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7)),
                      by   = "7 days"),
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),
    # date label breaks and major gridlines set to every 3 weeks beginning Sunday before first case
    breaks = seq.Date(from = as.Date(floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7)),
                      to   = as.Date(ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7)),
                      by   = "3 weeks"),
    
    # minor gridlines set to weekly beginning Sunday before first case
    minor_breaks = seq.Date(from = as.Date(floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7)),
                            to   = as.Date(ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7)),
                            by   = "7 days"),
    
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  labs(title = "ALIGNED Sundays",
       subtitle = "7-day bins manually set to begin Sunday before first case (27 Apr)\nDate labels and gridlines manually set to Sundays as well")

```





### Aggregated data  

Often instead of a linelist, you begin with aggregated counts from facilities, districts, etc. You can make an epicurve with `ggplot()` but the code will be slightly different. This section will utilize the `count_data` dataset that was imported earlier, in the data preparation section. This dataset is the `linelist` aggregated to day-hospital counts. The first 50 rows are displayed below.  

```{r message=FALSE, warning=F, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


#### Plotting daily counts {-}  

We can plot a daily epicurve from these *daily counts*. Here are the differences to the code:  

* Within the aesthetic mapping `aes()`, specify `y = ` as the counts column (in this case, the column name is `n_cases`)
* Add the argument `stat = "identity"` within `geom_histogram()`, which specifies that bar height should be the `y = ` value, not the number of rows as is the default  


```{r, message=FALSE, warning=F}
ggplot(data = count_data)+
     geom_histogram(
       mapping = aes(x = date_hospitalisation, y = n_cases),
       stat = "identity")+
     labs(x = "Date of report", 
          y = "Number of cases",
          Title = "Daily case incidence, from daily count data")
```

#### Plotting weekly counts {-}

If your data are already case counts by week, they might look like this dataset (called `count_data_weekly`):  

```{r, warning=F, message=F, echo=F}
# Create weekly dataset with epiweek column
count_data_weekly <- count_data %>%
  mutate(epiweek = lubridate::floor_date(date_hospitalisation, "week")) %>% 
  group_by(hospital, epiweek, .drop=F) %>% 
  summarize(n_cases_weekly = sum(n_cases, na.rm=T))   
```

The first 50 rows of `count_data_weekly` are displayed below. You can see that the counts have been aggregated into weeks. Each week is displayed by the first day of the week (Monday by default).  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(count_data_weekly, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now plot so that `x = ` the epiweek column. Remember to add `y = ` the counts column to the aesthetic mapping, and add `stat = "identity"` as explained above.  

```{r, warning=F, message=F}
ggplot(data = count_data_weekly)+
  
  geom_histogram(
    mapping = aes(
      x = epiweek,           # x-axis is epiweek (as class Date)
      y = n_cases_weekly,    # y-axis height in the weekly case counts
      group = hospital,      # we are grouping the bars and coloring by hospital
      fill = hospital),
    stat = "identity")+      # this is also required when plotting count data
     
  # labels for x-axis
  scale_x_date(
    date_breaks = "2 months",      # labels every 2 months 
    date_minor_breaks = "1 month", # gridlines every month
    date_labels = '%b\n%Y')+       #labeled by month with year below
     
  # Choose color palette (uses RColorBrewer package)
  scale_fill_brewer(palette = "Pastel2")+ 
  
  theme_minimal()+
  
  labs(
    x = "Week of onset", 
    y = "Weekly case incidence",
    fill = "Hospital",
    title = "Weekly case incidence, from aggregated count data by hospital")
```




### Moving averages

See the page on [Moving averages] for a detailed description and several options. Below is one option for calculating moving averages with the package **slider**. In this approach, *the moving average is calculated in the dataset prior to plotting*:  

1) Aggregate the data into counts as necessary (daily, weekly, etc.) (see [Grouping data] page)  
2) Create a new column to hold the moving average, created with `slide_index()` from **slider** package  
3) Plot the moving average as a `geom_line()` on top of (after) the epicurve histogram  

See the helpful online [vignette for the **slider** package](https://cran.r-project.org/web/packages/slider/vignettes/slider.html)  


```{r, warning=F, message=F}
# load package
pacman::p_load(slider)  # slider used to calculate rolling averages

# make dataset of daily counts and 7-day moving average
#######################################################
ll_counts_7day <- linelist %>%    # begin with linelist
  
  ## count cases by date
  count(date_onset, name = "new_cases") %>%   # name new column with counts as "new_cases"
  filter(!is.na(date_onset)) %>%              # remove cases with missing date_onset
  
  ## calculate the average number of cases in 7-day window
  mutate(
    avg_7day = slider::slide_index(    # create new column
      new_cases,                       # calculate based on value in new_cases column
      .i = date_onset,                 # index is date_onset col, so non-present dates are included in window 
      .f = ~mean(.x, na.rm = TRUE),    # function is mean() with missing values removed
      .before = 6,                     # window is the day and 6-days before
      .complete = FALSE),              # must be FALSE for unlist() to work in next step
    avg_7day = unlist(avg_7day))       # convert class list to class numeric


# plot
######
ggplot(data = ll_counts_7day) +  # begin with new dataset defined above 
    geom_histogram(              # create epicurve histogram
      mapping = aes(
        x = date_onset,          # date column as x-axis
        y = new_cases),          # height is number of daily new cases
        stat = "identity",       # height is y value
        fill="#92a8d1",          # cool color for bars
        colour = "#92a8d1",      # same color for bar border
        )+ 
    geom_line(                   # make line for rolling average
      mapping = aes(
        x = date_onset,          # date column for x-axis
        y = avg_7day,            # y-value set to rolling average column
        lty = "7-day \nrolling avg"), # name of line in legend
      color="red",               # color of line
      size = 1) +                # width of line
    scale_x_date(                # date scale
      date_breaks = "1 month",
      date_labels = '%d/%m',
      expand = c(0,0)) +
    scale_y_continuous(          # y-axis scale
      expand = c(0,0),
      limits = c(0, NA)) +       
    labs(
      x="",
      y ="Number of confirmed cases",
      fill = "Legend")+ 
    theme_minimal()+
    theme(legend.title = element_blank())  # removes title of legend
```




### Faceting/small-multiples

As with other ggplots, you can create facetted plots ("small multiples"). As explained in the [ggplot tips] page of this handbook, you can use either `facet_wrap()` or `facet_grid()`. Here we demonstrate with `facet_wrap()`. For epicurves, `facet_wrap()` is typically easier as it is likely that you only need to facet on one column.  

The general syntax is `facet_wrap(rows ~ cols)`, where to the left of the tilde (~) is the name of a column to be spread across the "rows" of the facetted plot, and to the right of the tilde is the name of a column to be spread across the "columns" of the facetted plot. Most simply, just use one column name, to the right of the tilde: `facet_wrap(~age_cat)`.  


**Free axes**  
You will need to decide whether the scales of the axes for each facet are "fixed" to the same dimensions (default), or "free" (meaning they will change based on the data within the facet). Do this with the `scales = ` argument within `facet_wrap()` by specifying "free_x" or "free_y", or "free".  


**Number of cols and rows of facets**  
This can be specified with `ncol = ` and `nrow = ` within `facet_wrap()`. 


**Order of panels**  
To change the order of appearance, change the underlying order of the levels of the factor column used to create the facets.  


**Aesthetics**  
Font size and face, strip color, etc. can be modified through `theme()` with arguments like:  

* `strip.text = element_text()` (size, colour, face, angle...)
* `strip.background = element_rect()` (e.g. element_rect(fill="grey"))  
* `strip.position = ` (position of the strip "bottom", "top", "left", or "right")  


**Strip labels**  
Labels of the facet plots can be modified through the "labels" of the column as a factor, or by the use of a "labeller".  

Make a labeller like this, using the function `as_labeller()` from **ggplot2**. Then provide the labeller to the `labeller = ` argument of `facet_wrap()` as shown below.  

```{r, class.source = 'fold-show'}
my_labels <- as_labeller(c(
     "0-4"   = "Ages 0-4",
     "5-9"   = "Ages 5-9",
     "10-14" = "Ages 10-14",
     "15-19" = "Ages 15-19",
     "20-29" = "Ages 20-29",
     "30-49" = "Ages 30-49",
     "50-69" = "Ages 50-69",
     "70+"   = "Over age 70"))
```

**An example facetted plot** - facetted by column `age_cat`.


```{r, warning=F, message=F}
# make plot
###########
ggplot(central_data) + 
  
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),    # arguments inside aes() apply by group
      
    color = "black",      # arguments outside aes() apply to all data
        
    # histogram breaks
    breaks = weekly_breaks_central)+  # pre-defined date vector (see earlier in this page)
                      
  # The labels on the x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+                       # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "grey"))+         # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,
    ncol = 4,
    strip.position = "top",
    labeller = my_labels)+             
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```

See this [link](https://ggplot2.tidyverse.org/reference/labellers.html) for more information on labellers.  




#### Total epidemic in facet background {-}

To show the total epidemic in the background of each facet, add the function `gghighlight()` with empty parentheses to the ggplot. This is from the package **gghighlight**. Note that the y-axis maximum in all facets is now based on the peak of the entire epidemic.  

```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # epicurves by group
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),  # arguments inside aes() apply by group
    
    color = "black",    # arguments outside aes() apply to all data
    
    # histogram breaks
    breaks = weekly_breaks_central)+     # pre-defined date vector (see top of ggplot section)                
  
  # add grey epidemic in background to each facet
  gghighlight::gghighlight()+
  
  # labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space below 0
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "white"))+        # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,                          # each plot is one value of age_cat
    ncol = 4,                          # number of columns
    strip.position = "top",            # position of the facet title/strip
    labeller = my_labels)+             # labeller defines above
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### One facet with data {-}  

If you want to have one facet box that contains all the data, duplicate the entire dataset and treat the duplicates as one faceting value. A "helper" function `CreateAllFacet()` below can assist with this (thanks to this [blog post](https://stackoverflow.com/questions/18933575/easily-add-an-all-facet-to-facet-wrap-in-ggplot2)). When it is run, the number of rows doubles and there will be a new column called `facet` in which the duplicated rows will have the value "all", and the original rows have the their original value of the faceting colum. Now you just have to facet on the `facet` column.   

Here is the helper function. Run it so that it is available to you.  

```{r}
# Define helper function
CreateAllFacet <- function(df, col){
     df$facet <- df[[col]]
     temp <- df
     temp$facet <- "all"
     merged <-rbind(temp, df)
     
     # ensure the facet value is a factor
     merged[[col]] <- as.factor(merged[[col]])
     
     return(merged)
}
```

Now apply the helper function to the dataset, on column `age_cat`:  

```{r}
# Create dataset that is duplicated and with new column "facet" to show "all" age categories as another facet level
central_data2 <- CreateAllFacet(central_data, col = "age_cat") %>%
  
  # set factor levels
  mutate(facet = factor(facet,
                        levels = c("all", "0-4", "5-9",
                             "10-14", "15-19", "20-29",
                             "30-49", "50-69", "70+")))

# check levels
table(central_data2$facet, useNA = "always")
```

Notable changes to the `ggplot()` command are:  

* The data used is now central_data2 (double the rows, with new column "facet")
* Labeller will need to be updated, if used  
* Optional: to achieve vertically stacked facets: the facet column is moved to rows side of equation and on right is replaced by "." (`facet_wrap(facet~.)`), and `ncol = 1`. You may also need to adjust the width and height of the saved png plot image (see `ggsave()` in [ggplot tips]).  

```{r, fig.height=12, fig.width=5, warning=F, message=F}
ggplot(central_data2) + 
  
  # actual epicurves by group
  geom_histogram(
        mapping = aes(
          x = date_onset,
          group = age_cat,
          fill = age_cat),  # arguments inside aes() apply by group
        color = "black",    # arguments outside aes() apply to all data
        
        # histogram breaks
        breaks = weekly_breaks_central)+    # pre-defined date vector (see top of ggplot section)
                     
  # Labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom")+               
  
  # create facets
  facet_wrap(facet~. ,                            # each plot is one value of facet
             ncol = 1)+            

  # labels
  labs(title    = "Weekly incidence of cases, by age category",
       subtitle = "Subtitle",
       fill     = "Age category",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported",
       caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```








## Tentative data  


The most recent data shown in epicurves should often be marked as tentative, or subject to reporting delays. This can be done in by adding a vertical line and/or rectangle over a specified number of days. Here are two options:  

1) Use `annotate()`:  
    + For a line use `annotate(geom = "segment")`. Provide `x`, `xend`, `y`, and `yend`. Adjust size, linetype (`lty`), and color.  
    + For a rectangle use `annotate(geom = "rect")`. Provide xmin/xmax/ymin/ymax. Adjust color and alpha.  
2) Group the data by tentative status and color those bars differently  

<span style="color: orange;">**_CAUTION:_** You might try `geom_rect()` to draw a rectangle, but adjusting the transparency does not work in a linelist context. This function overlays one rectangle for each observation/row!. Use either a very low alpha (e.g. 0.01), or another approach. </span>

### Using `annotate()` {-}

* Within `annotate(geom = "rect")`, the `xmin` and `xmax` arguments must be given inputs of class Date.  
* Note that because these data are aggregated into weekly bars, and the last bar extends to the Monday after the last data point, the shaded region may appear to cover 4 weeks  
* Here is an `annotate()` [online example](https://ggplot2.tidyverse.org/reference/annotate.html)


```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # histogram
  geom_histogram(
    mapping = aes(x = date_onset),
    
    breaks = weekly_breaks_central,   # pre-defined date vector - see top of ggplot section
    
    color = "darkblue",
    
    fill = "lightblue") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "1 month",           # 1st of month
    date_minor_breaks = "1 month",     # 1st of month
    date_labels = "%b\n'%y")+          # label format
  
  # labels and theme
  labs(
    title = "Using annotate()\nRectangle and line showing that data from last 21-days are tentative",
    x = "Week of symptom onset",
    y = "Weekly case indicence")+ 
  theme_minimal()+
  
  # add semi-transparent red rectangle to tentative data
  annotate(
    "rect",
    xmin  = as.Date(max(central_data$date_onset, na.rm = T) - 21), # note must be wrapped in as.Date()
    xmax  = as.Date(Inf),                                          # note must be wrapped in as.Date()
    ymin  = 0,
    ymax  = Inf,
    alpha = 0.2,          # alpha easy and intuitive to adjust using annotate()
    fill  = "red")+
  
  # add black vertical line on top of other layers
  annotate(
    "segment",
    x     = max(central_data$date_onset, na.rm = T) - 21, # 21 days before last data
    xend  = max(central_data$date_onset, na.rm = T) - 21, 
    y     = 0,         # line begins at y = 0
    yend  = Inf,       # line to top of plot
    size  = 2,         # line size
    color = "black",
    lty   = "solid")+   # linetype e.g. "solid", "dashed"

  # add text in rectangle
  annotate(
    "text",
    x = max(central_data$date_onset, na.rm = T) - 15,
    y = 15,
    label = "Subject to reporting delays",
    angle = 90)
```


The same black vertical line can be achieved with the code below, but using `geom_vline()` you lose the ability to control the height:  

```{r, eval=F}
geom_vline(xintercept = max(central_data$date_onset, na.rm = T) - 21,
           size = 2,
           color = "black")
```



### Bars color {-}  

An alternative approach could be to adjust the color or display of the tentative bars of data themselves. You could create a new column in the data preparation stage and use it to group the data, such that the `aes(fill = )` of tentative data can be a different color or alpha than the other bars. 

```{r, message=F, warning=F}
# add column
############
plot_data <- central_data %>% 
  mutate(tentative = case_when(
    date_onset >= max(date_onset, na.rm=T) - 7 ~ "Tentative", # tenative if in last 7 days
    TRUE                                       ~ "Reliable")) # all else reliable

# plot
######
ggplot(plot_data, aes(x = date_onset, fill = tentative)) + 
  
  # histogram
  geom_histogram(
    breaks = weekly_breaks_central,   # pre-defined data vector, see top of ggplot page
    color = "black") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_fill_manual(values = c("lightblue", "grey"))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",           # Monday every 3 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%d\n%b\n'%y")+      # label format
  
  # labels and theme
  labs(title = "Show days that are tentative reporting",
    subtitle = "")+ 
  theme_minimal()+
  theme(legend.title = element_blank())                 # remove title of legend
  
```


## Multi-level date labels  

If you want multi-level date labels (e.g. month and year) *without duplicating the lower label levels*, consider one of the approaches below:  

Remember - you can can use tools like `\n` *within* the `date_labels` or `labels` arguments to put parts of each label on a new line below. However, the code below helps you take years or months (for example) on a lower line *and only once*. A few notes on the code below:  

* Case counts are aggregated into weeks for aesthetic reasons. See Epicurves page (aggregated data tab) for details.  
* A `geom_area()` line is used instead of a histogram, as the faceting approach below does not work well with histograms.  


**Aggregate to weekly counts**

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}

# Create dataset of case counts by week
#######################################
central_weekly <- linelist %>%
  filter(hospital == "Central Hospital") %>%   # filter linelist
  mutate(week = lubridate::floor_date(date_onset, unit = "weeks")) %>%  
  count(week) %>%                              # summarize weekly case counts
  filter(!is.na(week)) %>%                     # remove cases with missing onset_date
  complete(week = seq.Date(
    from = min(week),   # fill-in all weeks with no cases reported
    to   = max(week),
    by   = "week"))
```

**Make plots**  

```{r, warning=F, message=F}
# plot with box border on year
##############################
ggplot(central_weekly) +
  geom_area(aes(x = week, y = n),    # make line, specify x and y
            stat = "identity") +             # because line height is count number
  scale_x_date(date_labels="%b",             # date label format show month 
               date_breaks="month",          # date labels on 1st of each month
               expand=c(0,0)) +              # remove excess space
  facet_grid(~lubridate::year(week), # facet on year (of Date class column)
             space="free_x",                
             scales="free_x",                # x-axes adapt to data range (not "fixed")
             switch="x") +                   # facet labels (year) on bottom
  theme_bw() +
  theme(strip.placement = "outside",         # facet labels placement
        strip.background = element_rect(fill = NA, # facet labels no fill grey border
                                        colour = "grey50"),
        panel.spacing = unit(0, "cm"))+      # no space between facet panels
  labs(title = "Nested year labels, grey label border")


# plot with no box border on year
#################################
ggplot(central_weekly,
       aes(x = week, y = n)) +              # establish x and y for entire plot
  geom_line(stat = "identity",              # make line, line height is count number
            color = "#69b3a2") +            # line color
  geom_point(size=1, color="#69b3a2") +     # make points at the weekly data points
  geom_area(fill = "#69b3a2",               # fill area below line
            alpha = 0.4)+                   # fill transparency
  scale_x_date(date_labels="%b",            # date label format show month 
               date_breaks="month",         # date labels on 1st of each month
               expand=c(0,0)) +             # remove excess space
  facet_grid(~lubridate::year(week),   # facet on year (of Date class column)
             space="free_x",                
             scales="free_x",               # x-axes adapt to data range (not "fixed")
             switch="x") +                  # facet labels (year) on bottom
  theme_bw() +
  theme(strip.placement = "outside",                     # facet label placement
          strip.background = element_blank(),            # no facet lable background
          panel.grid.minor.x = element_blank(),          
          panel.border = element_rect(colour="grey40"),  # grey border to facet PANEL
          panel.spacing=unit(0,"cm"))+                   # No space between facet panels
  labs(title = "Nested year labels - points, shaded, no label border")
```

The above techniques were adapted from [this](https://stackoverflow.com/questions/44616530/axis-labels-on-two-lines-with-nested-x-variables-year-below-months) and [this](https://stackoverflow.com/questions/20571306/multi-row-x-axis-labels-in-ggplot-line-chart) post on stackoverflow.com.  






<!-- ======================================================= -->
## Dual-axis { }  

Although there are fierce discussions about the validity of dual axes within the data visualization community, many epi supervisors still want to see an epicurve or similar chart with a percent overlaid with a second axis.

One example, of the **cowplot** method is shown below:  

* Two distinct plots are made, and then combined with **cowplot** package.  
* The plots must have the exact same x-axis (set limits) or else the data and labels will not align  
* Each uses `theme_cowplot()` and one has the y-axis moved to the right side of the plot  

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
#######################################
plot_cases <- linelist %>% 
  
  # plot cases per week
  ggplot()+
  
  # create histogram  
  geom_histogram(
    
    mapping = aes(x = date_onset),
    
    # bin breaks every week beginning monday before first case, going to monday after last case
    breaks = weekly_breaks_all)+  # pre-defined vector of weekly dates (see top of ggplot section)
        
  # specify beginning and end of date axis to align with other plot
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  # labels
  labs(
      y = "Daily cases",
      x = "Date of symptom onset"
    )+
  theme_cowplot()


# make second plot of percent died per week
###########################################
plot_deaths <- linelist %>%                        # begin with linelist
  group_by(week = floor_date(date_onset, "week")) %>%  # create week column
  
  # summarise to get weekly percent of cases who died
  summarise(n_cases = n(),
            died = sum(outcome == "Death", na.rm=T),
            pct_died = 100*died/n_cases) %>% 
  
  # begin plot
  ggplot()+
  
  # line of weekly percent who died
  geom_line(                                # create line of percent died
    mapping = aes(x = week, y = pct_died),  # specify y-height as pct_died column
    stat = "identity",                      # set line height to the value in pct_death column, not the number of rows (which is default)
    size = 2,
    color = "black")+
  
  # Same date-axis limits as the other plot - perfect alignment
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  
  # y-axis adjustments
  scale_y_continuous(                # adjust y-axis
    breaks = seq(0,100, 10),         # set break intervals of percent axis
    limits = c(0, 100),              # set extent of percent axis
    position = "right")+             # move percent axis to the right
  
  # Y-axis label, no x-axis label
  labs(x = "",
       y = "Percent deceased")+      # percent axis label
  
  theme_cowplot()                   # add this to make the two plots merge together nicely
```

Now use **cowplot** to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of `theme_cowplot()`.  

```{r, warning=F, message=F}
aligned_plots <- align_plots(plot_cases, plot_deaths, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```




## Cumulative Incidence {}

Note: If using **incidence2**, see the section on how you can produce cumulative incidence with a simple function. This page will address how to calculate cumulative incidence and plot it with ggplot().  

If beginning with a case linelist, create a new column containing the cumulative number of cases per day in an outbreak using `cumsum()` from **base** R:    

```{r}
cumulative_case_counts <- linelist %>% 
  count(date_onset) %>%                # count of rows per day (returned in column "n")   
  mutate(                         
    cumulative_cases = cumsum(n)       # new column of the cumulative number of rows at each date
    )
```

The first 10 rows are shown below:  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(cumulative_case_counts, 10), rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```



This cumulative column can then be plotted against `date_onset`, using `geom_line()`:

```{r, warning=F, message=F}
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")

plot_cumulative
```


It can also be overlaid onto the epicurve, with dual-axis using the **cowplot** method described above and in the [ggplot tips] page:

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
plot_cases <- ggplot()+
  geom_histogram(          
    data = linelist,
    aes(x = date_onset),
    binwidth = 1)+
  labs(
    y = "Daily cases",
    x = "Date of symptom onset"
  )+
  theme_cowplot()

# make second plot of cumulative cases line
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")+
  scale_y_continuous(
    position = "right")+
  labs(x = "",
       y = "Cumulative cases")+
  theme_cowplot()+
  theme(
    axis.line.x = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank())
```

Now use **cowplot** to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of `theme_cowplot()`.  

```{r, warning=F, message=F}
aligned_plots <- align_plots(plot_cases, plot_cumulative, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```


<!-- ======================================================= -->
## Resources { }








```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/epicurves.Rmd-->


# Plot continuous data { }  

This page will discuss appropriate plotting of continuous data, such as age, clinical measurements, and distance. **ggplot2**, part of the **tidyverse** family of packages, is a fantastic and versatile package for visualising continuous data. As usual, R also has built-in **base** functions, which can be helpful for quick looks at the data.

Visualisations covered here include:

* Plots for one continuous variable: 
  * **Histograms**, the classic graph to present the distribution of a continuous variable. 
  * **Box plots** (also called box and whisker), in which the box represents the 25th, 50th, and 75th percentile of a continuous variable, and the line outside of this represent tail ends of distribution of the the continuous variable, and dots represent outliers.
  * **Violin plots**, which are similar to histograms in that they show the distribution of a continuous variable based on the symettrical width of the 'violin'. 
  * **Jitter plots**, which visualise the distribution of a continuous variable by showing all values as dots, rather than collectively as one larger shape. Each dot is 'jittered' so that they can all (mostly) be seen, even where two have the same value. 
  * **Sina plots**, are a cross between jitter and violin plots, where the individual points can be seen but in the symmetrical shape of the distribution (note this brings in the `ggforce` package).
* **Scatter plots** for two continuous variables.

```{r echo=F, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
pacman::p_load(ggplot2,
               dplyr)

linelist <- rio::import(here::here("data", "linelist_cleaned.rds")) %>% #Load the data
  mutate(age = as.numeric(age),
         wt_kg = as.numeric(wt_kg)) # Converting age and weight to numeric value if needed

# with the boxplot() function
graphics::boxplot(age ~ outcome,
                  data = linelist,
                  col = c("gold", "darkgreen"),
                  main = "A) BOX PLOT made using R's built in graphics boxplot")


# with ggplot2  - boxplot
ggplot(data = linelist,
       mapping = aes(x = age))+
  geom_histogram(binwidth=2)+
  labs(title = "B) HISTOGRAM made using ggplot2")


# with ggplot2 - violin plot
ggplot(data = linelist,
       mapping = aes(y = age, x = outcome))+
  geom_violin(fill = "darkgreen")+
  labs(title = "C) VIOLIN PLOT made using ggplot2")

# with ggplot2 - scatter plot
ggplot(data = linelist,
       mapping = aes(y = age, x = wt_kg))+
  geom_point()+
  labs(title = "D) SCATTER PLOT made using ggplot2")



```



<!-- ======================================================= -->
## Preparation {  }


### Load packages {-}

Preparation includes loading the relevant packages, here `ggplot2` and `dplyr`, and ensuring your data is the correct class and format. 

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

Note: You could also only load **tidyverse**, which includes **ggplot2** and **dplyr** among other packages (e.g. **stringr**, **tidyr**, and **forcats** for instance). 

```{r}
pacman::p_load(ggplot2, dplyr)
```

### Import data {-}  


### Import data {-}


For the examples in this section, we will use the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below. We will focus on the continuous variables `age`, `wt_kg` (weight in kilos), `ct_blood` (CT values), and `days_onset_hosp` (difference between onset date and hospitalisation).  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Column class {-}  

Below we use `mutate()` to confirm the class of the columns that are important to our analysis.  

```{r}
linelist <- linelist %>%  
  mutate(age = as.numeric(age),       # Ensure vars are class numeric
         ct_blood = as.numeric(ct_blood),
         days_onset_hosp = as.numeric(days_onset_hosp),
         wt_kg = as.numeric(wt_kg))  
```

You should have conducted various data checks before this point, including checking the missingness of the data. 

<!-- ======================================================= -->
## Plotting with ggplot2 {  }

### Code syntax {-}

Ggplot2 has extensive functionality, and the same code syntax can be used for many different plot types.

A basic breakdown of the ggplot code is as follows:


```
ggplot(data = linelist)+  
  geom_XXXX(aes(x = col1, y = col2),
       fill = "color") 
```

* ```ggplot()``` starts off the function. You can specify the data and aesthetics (see next point) within the ggplot bracket, unless you are combining different data sources or plot types into one
* ```aes()``` stands for 'aesthetics', and is where the columns used for the visualisation are specified. For instance ```aes(x = col1, y = col2)``` to specify the data used for the x and y values (where y is the continuous variable in these examples).
* ```fill``` specifies the colour of the boxplot areas. One could also write ```color``` to specify outline or point colour. 
* ```geom_XXX``` specifies what type of plot. Options include:
  * `geom_boxplot()` for a boxplot
  * `geom_histogram` for a histogram
  * `geom_violin()` for a violin plot
  * `geom_jitter()` for a jitter plot
  * `geom_point()` for a scatter plot
  * `geom_sina()` for a jitter plot where the width of the jitter is controlled by the density distribution of the data within each class

Note that the `aes()` bracket can be within the `ggplot()` bracket or within the specific geom_XXX bracket. If you are layering different ggplots with diferent aesthetics, you will need to specify them within each geom_XXX.

For more see section on [ggplot tips](#ggplottips). We also walk through further customisation below.

### Plotting one continuous variable

**Box plots**

Below is code for creating **box plots**, to show the distribution of CT values of Ebola patients in an entire dataset and by sub group. Note that for the subgroup breakdowns, the 'NA' values are also removed using dplyr, otherwise ggplot plots the age distribution for 'NA' as a separate boxplot. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Simple boxplot of one numeric variable
ggplot(data = linelist, aes(y = ct_blood))+  # only y variable given (no x variable)
  geom_boxplot()+
  labs(title = "A) Simple ggplot2 boxplot")

# B) Box plot by group
ggplot(data = linelist %>% filter(!is.na(outcome)), 
       aes(y = ct_blood,                            # Continous variable
           x = outcome)) +                          # Grouping variable
  geom_boxplot(fill = "gold")+                      # Create the boxplot and specify colour
  labs(title = "B) ggplot2 boxplot by gender")      
```

**Histograms**

Below is code for generating **histograms**, to show the distribution of CT values of Ebola patients. Within the `aes()` bracket, you specify which variable you want to see the distribution of. You can supply either the x or the y, which will change the direction of the plot. The y or the x respectively will then show the count, represented by columns referred to as 'bins'. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Regular histogram
ggplot(data = linelist, aes(x = ct_blood))+  # provide x variable
  geom_histogram()+
  labs(title = "A) Simple ggplot2 histogram")

# B) Histogram with values across y axis
ggplot(data = linelist, aes(y = ct_blood))+  # provide y variable 
  geom_histogram()+
  labs(title = "B) Simple ggplot2 histogram with axes swapped")
```

In the examples above, R has guessed the most appropriate way to present the data, and issues a message to tell you how many bins (columns) it went with, and to prompt you to customise it yourself:

```
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
```

It used 30 bins, and they look spaced out because some of them have 0 values. This relates to the way the values have been rounded.

To change this, you can specify `binwidth` (e.g. the range of values that the bin is counting) or `bins` (the number of  bins) within the `geom_histogram` argument. These are then evenly grouped, between the minimum and maximum values of the histogram. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Histogram with specified bin number
ggplot(data = linelist, aes(x = ct_blood))+   # Provide x variable
  geom_histogram(bins=10,                     # Add bin number
                 color = "white")+            # Add white outline so bars can easily be distinguished
  labs(title = "A) Ggplot histogram with 10 bins")

# B) Histogram with specified bin width
ggplot(data = linelist, aes(x = ct_blood))+   # Provide y variable 
  geom_histogram(binwidth = 1,                # Each bar includes a CT value range of 1
                 color = "white")+            # Add white outline so bars can easily be distinguished
  labs(title = "B) Ggplot histogram with bindwidth of 1")
```

Rather than counts, you can change the stats within the `aes()` bracket to specify proportions - see (plot A) below. You can also layer different histograms with different settings (plot B).

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Histogram with proportion
ggplot(data = linelist, aes(x = ct_blood,           # provide x variable
                            y = stat(density)))+    # Calculate proportion
  geom_histogram(bins=10,                           # Add bin number
                 color = "white")+ # Add white outline so bars can easily be distinguished
  labs(title = "A) Ggplot histogram showing proportion")

# B) Layered histograms with different bin widths
ggplot(data = linelist, aes(x = ct_blood))+         # provide x variable 
  geom_histogram(binwidth = 2) +                    # Underlying layer has binwidth of 2
  geom_histogram(binwidth = 1,                      # Top layer has binwidth of 1
                 alpha = 0.4,                       # Set top layer to be slightly see through
                 fill = "blue")+ 
  labs(title = "B) Layered ggplot histograms")
```

**Violin, jitter, and sina plots**

Below is code for creating **violin plots** (`geom_violin`) and **jitter plots** (`geom_jitter`) to show age distributions. One can specify that the 'fill' or 'color'is also determined by the data, thereby inserting these options within the `aes` bracket. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

# A) Violin plot by group
ggplot(data = linelist %>% filter(!is.na(outcome)), 
       aes(y = age,                                # Continuous variable
           x = outcome,                            # Grouping variable
           fill = outcome))+                       # fill variable (color of boxes)
  geom_violin()+                                   # create the violin plot
  labs(title = "A) ggplot2 violin plot by gender")    


# B) Jitter plot by group
ggplot(data = linelist %>% filter(!is.na(outcome)), 
       aes(y = age,                               # Continuous variable
           x = outcome,                           # Grouping variable
           color = outcome))+ # Color variable
  geom_jitter()+                                  # Create the violin plot
  labs(title = "B) ggplot2 jitter plot by gender")     


```

One can combine the two using the `geom_sina` option, which is actually part of the `ggforce` package. This can be easier to visually interpret. A) on the left shows basic layering of both a `geom_violin` and `geom_sina`. B) shows slightly more effort put into the appearance of the ggplot (see in-line comments). 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

pacman::p_load(ggforce)

# A) Sina plot by group
ggplot(data = linelist %>% filter(!is.na(outcome)), 
       aes(y = age,             # numeric variable
           x = outcome)) +      # group variable
  geom_violin()+                # create the violin plot
  geom_sina()+
  labs(title = "A) ggplot() violin and sina plot by gender")      


# A) Sina plot by group
ggplot(data = linelist %>% filter(!is.na(outcome)), 
       aes(y = age,             # numeric variable
           x = outcome)) +      # group variable
  geom_violin(aes(fill = outcome), # fill variable (color of violin background)
              color = "white",  # Plot has white outline rather than default black 
              alpha = 0.2)+     # Alpha value where 0 transparent to 1 opaque
  geom_sina(size=1,             # Change the size of the jitter
            aes(color = outcome))+ # color variable (color of dots)
  scale_fill_manual(values = c("Death" = "#bf5300", 
                        "Recover" = "#11118c")) + # Define colours for death/recover 
                                                  # (but note they will come out a bit transparent)
  scale_color_manual(values = c("Death" = "#bf5300", 
                         "Recover" = "#11118c")) + # Define colours for death/recover
  theme_minimal() +                                # Remove the gray background
  theme(legend.position = "none") +                # Remove unnecessary legend
  labs(title = "B) ggplot() violin and sina plot by gender with formatting")      



```

### One continuous variable within facets {-}

**Faceting basics** 

To examine further subgroups, one can 'facet' the graph. This means the plot will be recreated within specified subgroups. One can use:

* `facet_wrap()` - this will recreate the sub-graphs and present them alphabetically (typically, unless stated otherwise). You can invoke certain options to determine the look of the facets, e.g. `nrow=1` or `ncol=1` to control the number of rows or columns that the faceted plots are arranged within. See plot A below. 
* `facet_grid()` - this is suited to seeing subgroups for particular combinations of discrete variables. See plot B  below. `nrow` and `ncol` are not relevant, as the subgroups are presented in a grid, with the subgroups always in the x or y axis (see notes in code below)

You can stipulate up to two faceting variables, with a '~' between them. If only one faceting variable, a '.' is used as a placeholder for a non-used second faceting variable - see code examples.

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Histogram of hospitalisation dates faceted by hospital
ggplot(data = linelist %>% 
         filter(hospital != "Missing"),               # filter removes unknown hospital
       aes(x = date_hospitalisation ))+
  geom_histogram(binwidth=7) +                        # Bindwidth = 7 days
  labs(title = "A) Ggplot 2 histogram of hospitalisation dates by hospital")+
  facet_wrap(hospital~.,                              # Facet by just hospital
            ncol = 2)                                 # Facet in two columns

# B) Boxplot of age faceted in a grid with two variables, gender and outcome
ggplot(data = linelist %>% 
         filter(!is.na(gender) & !is.na(outcome)),    # filter retains non-missing gender/outcome
       aes(y = age))+
  geom_boxplot()+
  labs(title = "A) A Ggplot2 boxplot by gender and outcome")+
  facet_grid(outcome~gender)                          # Outcome is the row, gender is the column

```

**Further faceting options**

The scales used when facetting are consistent across subgroups, which is helpful for comparisons, but not always appropriate or optimal. 

When using `facet_wrap` or `facet_grid`, we can add `scales = "free_y"` (plot A) so that the heights of the faceted histograms are standardised and the shapes are easier to compare. This is particularly useful if the actual counts are small for one of the subcategories and trends are otherwise hard to see. Instead of `free_y` we can also write `free_x` to do the same for the x axis or `free` for both axes. Note that in `facet_grid`, the y scales will be the same for facets in the same row, and the x scales will be the same for facets in the same column.

When using `facet_grid` only, we can add `space = "free_y"` or `space = "free_x"` so that the actual height or width of the facet is weighted to the values of the figure within. This only works if `scales = "free"` (y or x) already applies. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Facet hospitalsation date by hospital, free y axis
ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital
       aes(x = date_hospitalisation ))+
  geom_histogram(binwidth=7) + # Bindwidth = 7 days
  labs(title = "A) Histogram with free y axis scales")+
  facet_grid(hospital~., # Facet with hospital as the row 
             scales = "free_y") # Free the y scale of each facet

# B) Facet hospitalisation date by hospital, free y axis and vertical spacing
ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital
       aes(x = date_hospitalisation ))+
  geom_histogram(binwidth=7) + # Bindwidth = 7 days
  labs(title = "B) Histogram with free y axis scales and spacing")+
  facet_grid(hospital~., # Facet with hospital as the row 
             scales = "free_y", # Free the y scale of each facet
             space = "free_y") # Free the vertical spacing of each facet to optimise space

```

### Two continuous variables {-}  

Following similar syntax, `geom_point()` will allow one to plot two continuous variables against eachother in a **scatter plot**. This is useful for showing actual values rather than their distributions.

A basic scatter plot of age vs weight is shown in (A). In (B) we again use `facet_grid()` to show the relationship between two continuous variables in the linelist. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Basic scatter plot of weight and age
ggplot(data = linelist, 
       aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "A) Scatter plot of weight and age")

# Scatter plot of weight and age by gender and Ebola outcome
ggplot(data = linelist %>% filter(!is.na(gender) & !is.na(outcome)), # filter retains non-missing gender/outcome
       aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "B) Scatter plot of weight and age faceted by gender and outcome")+
  facet_grid(gender~outcome) 

```


### Three continuous variables {-}  



<!-- ======================================================= -->
## Plotting with base graphics {  }

Using base graphics can sometimes be quicker than ggplot, and is helpful for that initial first look.

### One continuous variable {-}

**Box plots and histograms**

The in-built graphics package comes with the `boxplot()` and `hist()` functions, allowing straight-forward visualisation of a continuous variable. 

```{r, fig.show='hold', out.width=c('50%', '50%')}

# Boxplot
boxplot(linelist$wt_kg,
                  main = "A) Base boxplot") 


# Histogram
hist(linelist$wt_kg,
                  main = "B) Base histogram") 

```

**Further customisation**

Subgroups can also be shown, by subgroup or crossed groups. Note how with plot B below,  ```outcome``` and ```gender``` are written as ```outcome*gender``` such that the boxplots are for the four combinations of the two columns. They do not get facetted across different rows and columns like in ggplot2.

We specify `linelist` as the dataset so we do not need to write `age` as `linelist$age`
```{r, fig.show='hold', out.width=c('50%', '50%')}

# Box plot by subgroup
boxplot(age ~ outcome,
                  data = linelist, 
                  main = "A) Base boxplot by subgroup")

# Box plot by crossed subgroups
boxplot(age ~ outcome*gender,
                  data = linelist, 
                  main = "B) Base boxplot) by crossed groups")

```

Some further options with `boxplot()` shown below are:  

* Boxplot width proportional to sample size (A)
* Violin plots, with notched representing the median and x around it (B)
* Horizontal (C)  


```{r, out.width=c('33%', '33%', '33%'), fig.show='hold'}

# Varying width by sample size 
boxplot(linelist$age ~ linelist$outcome,
                  varwidth = TRUE, # width varying by sample size
                  main="A) Proportional boxplot() widths")

                  
# Notched (violin plot), and varying width
boxplot(age ~ outcome,
        data=linelist,
        notch=TRUE,      # notch at median
        main="B) Notched boxplot()",
        col=(c("gold","darkgreen")),
        xlab="Suppliment and Dose")

# Horizontal
boxplot(age ~ outcome,
        data=linelist,
        horizontal=TRUE,  # flip to horizontal
        col=(c("gold","darkgreen")),
        main="C) Horizontal boxplot()",
        xlab="Suppliment and Dose")
```

### Two continuous variables {-}

Using base R, we can quickly visualise the relationship between two continuous variables with the `plot` function.


```{r}
plot(linelist$age, linelist$wt_kg)
```


<!-- ======================================================= -->
## Resources {  }

There is a huge amount of help online, especially with ggplot. See:

* http://r-statistics.co/ggplot2-cheatsheet.html
* https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/plot_continuous.Rmd-->

# Plot categorical data  { }  

For appropriate plotting of categorical data, e.g. the distribution of sex, symptoms, ethnic group, etc. 


<!-- ======================================================= -->
## Overview {  }

In this section we cover use of R's built-in functions or functions from the `ggplot2` package to visualise categorical/categorical data. The additional functionality of ggplot2 compared to R means we recommend it for presentation-ready visualisations. 

We cover visualising distributions of categorical values, as counts and proportions.


```{r echo=F, fig.height=3, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('100%')}

# 
pacman::p_load(tidyverse)

linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

linelist <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, 
                                c("St. Mark's Maternity Hospital (SMMH)", 
                                  "Port Hospital", 
                                  "Central Hospital",
                                  "Military Hospital",
                                  "Other",
                                  "Missing")))


ggplot(linelist %>% filter(!is.na(outcome))) + 
  geom_bar(aes(x=hospital, fill = outcome)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90)) +
  coord_flip() +
  scale_x_discrete(limits=rev) + 
  scale_fill_manual(values = c("Death"= "#3B1c8C",
                               "Recover" = "#21908D" )) +
  labs(fill = "Outcome", y = "Count",x = "Hospital of admission") +
  labs(subtitle = "Number of Ebola cases per hospital, by outcome")

```


<!-- ======================================================= -->
## Preparation {  }

Preparation includes loading the relevant packages, namely `ggplot2` for examples covered here. We also load the data.

### Load packages

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
# Load packages we will be using repeatedly
pacman::p_load(ggplot2, # Package for visualisation
       dplyr,           # Package for data management
       forcats)         # Package for factors
```


### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Process columns for analysis

For the examples in this section, we use the simulated Ebola linelist, focusing on the categorical variables `hospital`, and `outcome`. These need to be the correct class and format. 

Let's take a look at the `hospital` column. 

```{r}
# View class of hospital column - we can see it is a character
class(linelist$hospital)

# Look at values held within hospital column
table(linelist$hospital)

```

We can see the values within are characters, as they are hospital names, and by default they are ordered alphabetically. There are 'other' and 'missing' values, which we would prefer to be the last subcategories when presenting breakdowns. So we change this column into a factor and re-order it. This is covered in more detail in the 'factors' data management section.


```{r}
# Change hospital to factor variable
linelist <- linelist %>% 
  mutate(hospital = factor(hospital))

# Define the levels of factor with forcats - so other and missing are last
linelist <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, 
                                c("St. Mark's Maternity Hospital (SMMH)", 
                                  "Port Hospital", 
                                  "Central Hospital",
                                  "Military Hospital",
                                  "Other",
                                  "Missing")))

```

### Ensure correct data structure

For displaying frequencies and distributions of categorical variables, you have the option of creating plots based on: 

* The linelist data, with one row per observation, or 
* A summary table based on the linelist, with one row per category. An example is below to show the use of `dplyr` to create a table of case counts per hospital. 

Tables can be created using the 'table' method for built-in graphics. The `useNA = "ifany"` arguments ensures that missing values are included, as table otherwise automatically excludes them. 

```{r}
#Table method
  outcome_nbar <- table(linelist$outcome, 
                        useNA = "ifany")

  outcome_nbar # View full table
```

Or using other data management packages such as dplyr. In this example we add on a percentage column.

```{r}
#Dplyr method
  outcome_n <- linelist %>% 
    group_by(outcome) %>% 
    count %>% 
    ungroup() %>% # Ungroup so proportion is out of total
    mutate(proportion = n/sum(n)*100) # Caculate percentage
  
  
   outcome_n #View full table
```

### Filter to relevant data

You may consider dropping rows not needed for this analysis. For instance, for the next few examples we want to understand trends amongst persons with a known outcome, so we drop rows with missing `outcome` column values.

```{r}
#Drop missing from full linelist
linelist <- linelist %>% 
  filter(!is.na(outcome))

#Drop missing from dplyr table
outcome_n <- outcome_n %>% 
  filter(!is.na(outcome))

```

<!-- ======================================================= -->
## Plotting with ggplot2 {  }
<h2> Plotting with ggplot2 </h2>

### Code syntax

Ggplot has extensive functionality, and the same code syntax can be used for many different plot types.

Similar to the _plotting continuous data_ section, basic breakdown of the ggplot code is as follows:


```
ggplot(data = linelist)+  
  geom_XXXX(aes(x = col1, y = col2),
       fill = "color") 
```

* ```ggplot()``` starts off the function. You can specify the data and aesthetics (see next point) within the ggplot bracket, unless you are combining different data sources or plot types into one
* ```aes()``` stands for 'aesthetics', and is where the columns used for the visualisation are specified. For instance ```aes(x = col1, y = col2)``` to specify the data used for the x and y values.
* ```fill``` specifies the colour of bars, or of the subgroups if specified within the `aes` breacket.
* ```geom_XXX``` specifies what type of plot. Options include:
  * `geom_bar()` for a bar chart based on a linelist
  * `geom_col()` for a bar chart based on a table with values (see preparation section)

Note that the `aes()` bracket can be within the `ggplot()` bracket or within the specific geom_XXX bracket. If you are layering different ggplots with diferent aesthetics, you will need to specify them within each geom_XXX. 

For more see section on [ggplot tips](#ggplottips). 

### Bar charts using raw data

Below is code using `geom_bar` for creating some simple bar charts to show frequencies of Ebola patient outcomes: A) For all cases, and B) By hospital.

In the `aes` bracket, only `x` needs to be specified - or `y` if you want the bars presented horizontally. Ggplot knows that the unspecified y (or x) will be the number of observations that fall into those categories. 

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Outcomes in all cases
ggplot(linelist) + 
  geom_bar(aes(x=outcome)) +
  labs(title = "A) Number of recovered and dead Ebola cases")


# B) Outcomes in all cases by hosptial
ggplot(linelist) + 
  geom_bar(aes(x=outcome, fill = hospital)) +
  theme(axis.text.x = element_text(angle = 90)) + # Add preference to rotate the x axis text
  labs(title = "B) Number of recovered and dead Ebola cases, by hospital")

```


### Bar charts using processed data

Below is code using `geom_col` for creating  simple bar charts to show the distribution of Ebola patient outcomes. With geom_col, both x and y need to be specified. Here x is the categorical variable along the x axis, and y is the generated proportions column `proportion`. 

```{r, fig.height = 3, fig.width=4.5}
# Outcomes in all cases
ggplot(outcome_n) + 
  geom_col(aes(x=outcome, y = proportion)) +
  labs(subtitle = "Number of recovered and dead Ebola cases")

```

To show breakdowns by hospital, an additional table needs to be created for frequencies of the combined categories `outcome` and `hospital`. 

```{r, fig.height = 4, fig.width=6}
outcome_n2 <- linelist %>% 
  group_by(hospital, outcome) %>% 
  count() %>% 
  group_by(hospital) %>% # Group so proportions are out of hospital total
  mutate(proportion = n/sum(n)*100)

head(outcome_n2) #Preview data
```

We then create the ggplot with some added formatting:

  * **Axis flip**: Swapped the axis around with `coord_flip()` so that we can read the hospital names.
  * **Columns side-by-side**: Added a `position = "dodge"` argument so that the bars for death and recover are presented side by side rather than stacked. Note stacked bars are the default.
  * **Column width**: Specified 'width', so the columns are half as thin as the full possible width.
  * **Column order**: Reversed the order of the categories on the y axis so that 'Other' and 'Missing' are at the bottom, with `scale_x_discrete(limits=rev)`. Note that we used that rather than `scale_y_discrete` because hospital is stated in the `x` argument of `aes()`, even if visually it is on the y axis. We do this because Ggplot seems to present categories backwards unless we tell it not to.  
  * **Other details**: Labels/titles and colours added within `labs` and `scale_fill_color` respectively.
  
```{r, fig.height = 4, fig.width=8}

# Outcomes in all cases by hospital
ggplot(outcome_n2) +  
  geom_col(aes(x=hospital, 
               y = proportion, 
               fill = outcome),
           width = 0.5,          # Make bars a bit thinner (out of 1)
           position = "dodge") + # Bars are shown side by side, not stacked
  scale_x_discrete(limits=rev) + # Reverse the order of the categories
  theme_minimal() +              # Minimal theme 
  coord_flip() +
  labs(subtitle = "Number of recovered and dead Ebola cases, by hospital",
       fill = "Outcome",        # Legend title
       x = "Count",             # X axis title
       y = "Hospital of admission")  + # Y axis title
  scale_fill_manual(values = c("Death"= "#3B1c8C",
                               "Recover" = "#21908D" )) 

```



Note that the proportions are binary, so we may prefer to drop 'recover' and just show the proportion who died. This is just for illustration purposes though. 

### Facetting 

We can also use faceting to create futher mini-graphs, which is detailed with examples in the continuous data visualisation section. Specifically, one can use:

* `facet_wrap()` - this will recreate the sub-graphs and present them alphabetically (typically, unless stated otherwise). You can invoke certain options to determine the look of the facets, e.g. `nrow=1` or `ncol=1` to control the number of rows or columns that the faceted plots are arranged within. 
* `facet_grid()` - this is suited to seeing subgroups for particular combinations of categorical variables. 


<!-- ======================================================= -->
## Plotting with base graphics {  }
<h2> In-built graphics package </h2>

**Bar charts**

To create bar plots in R, we create a frequency table using the `table` function. This creates an object of a table class, that R can recognise for plotting. We can create a simple frequency graph showing Ebola case outcomes (A), or add in colours to present outcomes by gender (B).

Note that NA values are excluded from these plots by default.

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Outcomes in all cases
outcome_nbar <- table(linelist$outcome)
barplot(outcome_nbar, main= "A) Outcomes")

# B) Outcomes in all cases by gender of case
outcome_nbar2 <- table(linelist$outcome, linelist$gender) # The first column is for groupings within a bar, the second is for the separate bars
barplot(outcome_nbar2, legend.text=TRUE, main = "B) Outcomes by gender") # Specify inclusion of legend

```




<!-- ======================================================= -->
## Resources {  }
<h2> Resources </h2>

There is a huge amount of help online, especially with ggplot. see:

* http://r-statistics.co/ggplot2-cheatsheet.html
* https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/plot_discrete.Rmd-->


# HTML tables { }  

This section demonstrates how to create publication-ready tables, which can be inserted directly into shareable documents, including R Markdown outputs.  

See the [Descriptive tables] page for instruction on how to make tabulations, cross-tabulations, and descriptive summary tables.  

<!-- ======================================================= -->
## Overview {  }

We build on previous sections on basic statistics and creating summary tables (e.g. using **dplyr** and **gtsummary** and show how to create publication-read tables. The primary package we use is **flextable**, which is compatible with multiple R Markdown formats, including html and word documents. 

Example: 

**Table of Ebola patients with outcome information: Number, proportion, and CT values of cases who recovered and died** (with Military hospital highlighted)

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                      # number with known outcome
    Pct_Death = N_Death / N_Known * 100,               # percent cases who died
    Pct_Recover = N_Recover/N_Known * 100) %>%         # percent who recovered
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>% 
  bold(i = 1, bold = TRUE, part = "header")

table
```


<!-- ======================================================= -->
## Preparation {  }

### Load packages {-} 

Load, and install if necessary, **flextable**, which we will use to convert the above table into a fully formatted and presentable table. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

```

### Import data {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Prepare table {-}  

*Before* beginning to use **flextable** you will need to *create* your table contents. Use packages discussed in other sections such as **janitor** or **dplyr** to create a table with the content of interest, with the correct columns and rows. 
  
Below is an example - we create a simple summary table of patient outcomes. We are interested in knowing the number and proportion of patients that recover or died, as well as their median CT values, by hospital of admission. 

```{r message=FALSE, warning=FALSE}
table <- linelist %>% 
  # filter
  ########
  filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                      # number with known outcome
    Pct_Death = N_Death / N_Known * 100,               # percent cases who died
    Pct_Recover = N_Recover/N_Known * 100) %>%         # percent who recovered
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known)                                    # Arrange rows from lowest to highest (Total row at bottom)

table  # print

```




<!-- ======================================================= -->
## Basic flextable {  }

**Creating a flextable**

To create and manage **flextable** objects, we pass the table object through the `flextable()` function and progressively pipe the object through more **flextable** formatting functions. The general syntax of each line of flextable code is as follows:

* `function(table, i = X, j = X, part = "X")`, where:
  * `table = ` is the name of the table object, although does not need to be stated if the table is piped into the function.
  * The 'function' can be one of many different functions, such as `width()` to determine column widths, `bg()` to set background colours, `align()` to set whether text is centre/right/left aligned, and so on. 
  * `part = ` refers to which part of the table the function is being applied to. E.g. "header", "body" or "all". 
  * `i = ` specifies the *row* to apply the function to, where 'X' is the row number. If multiple rows, e.g. the first to third rows, one can specify: `i = c(1:3)`. Note if 'body' is selected, the first row starts from underneath the header section.
  * `j = ` specifies the *column* to apply the function to, where 'x' is the column number or name. If multiple columns, e.g. the fifth and sixth, one can specify: `j = c(5,6)`. 
  
You can find the complete list of **flextable** formatting function [here](https://davidgohel.github.io/flextable/reference/index.html) or review the documentation with `?flextable`.  


```{r}

my_table <- flextable(table) 
my_table

```


**Formatting cell content**

We can ensure that the proportion columns display only one decimal place using the function `colformat_num()`. Note this could also have been done at data management stage with the `round()` function. 

```{r}
my_table <- colformat_num(my_table, j = c(4,7), digits = 1)
my_table
```

**Formatting column width**

We can use the `autofit()` function, which nicely stretches out the table so that each cell only has one row of text. The function `qflextable()` is a convenient shorthand for `flextable()` and `autofit()`.  

```{r}

my_table %>% autofit()

```

However, this might not always be appropriate, especially if there are very long values within cells, meaning the table might not fit on the page. 

Instead, we can specify widths with the `width()` function. It can take some playing around to know what width value to put. In the example below, we specify different widths for column 1, column 2, and columns 4 to 8. 

```{r}

my_table <- my_table %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1)

my_table
  
```

**Column headers**

We want to clearer headers for easier interpretation of table contents.

First we can add an extra header layer for clarity. We do this with the `add_header_row()` function with `top = ` set to TRUE, so that columns covering the same subgroups can be grouped together. We provide the new name of each column to `values = `, leaving empty values "" for column we know we will merge together later.  

We also rename the now-second header. Finally we use `merge_at()` to merge the column headers in the top header row. 

```{r}
my_table <- my_table %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header")     # Horizontally merge columns 6 to 8 in new header row

my_table  # print

```

**Formatting borders and background **

You can adjust the borders, internal lines, etc. with various **flextable** functions. It is often easier to start by removing all existing borders with `border_remove()`.  

Then, you can apply default border themes by passing the table to `theme_box()`, `theme_booktabs()`, or `theme_alafoli()`.  

You can add vertical and horizontal lines with a variety of functions. `hline()` and `vline()` add lines to a specified row or column, respectively. Within each, you must specify the `part = ` as either "all", "body", or "header". For vertical lines, specify to `j = ` the column, and for horizontal line to `i = `. Other functions like `vline_right()`, `vline_left()`, `hline_top()`, and `hline_bottom()` add lines to the outsides only.  

In all of these functions, the actual line style itself must be specified to `border = ` and must be the output of the `fp_border()` function from the **officer** package. This function helps you define the width and color or the line. You can define this outside of the table, as shown below.  

```{r}
# define style for border line
border_style = officer::fp_border(color="black", width=1)

# add border lines to table
my_table <- my_table %>% 

  # Remove all existing borders
  border_remove() %>%  
  
  # add horizontal lines via a pre-determined theme setting
  theme_booktabs() %>% 
  
  # add vertical lines to separate Recovered and Died sections
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style)       # at column 5

my_table
```

**Font and alignment**

We centre-align all columns aside from the left-most column with the hospital names, using the `align()` function from **flextable**.

```{r}
my_table <- my_table %>% 
   flextable::align(align = "center", j = c(2:8), part = "all") 
my_table
```

Additionally, we can increase the header font size and change then to bold. We can also change the total row to bold.  

```{r}

my_table <-  my_table %>%  
  fontsize(i = 1, size = 12, part = "header") %>%   # adjust font size of header
  bold(i = 1, bold = TRUE, part = "header") %>%     # adjust bold face of header
  bold(i = 6, bold = TRUE, part = "body")           # adjust bold face of total row (row 6)

my_table

```

**Merged cells**  

Just as we merge cells horizontally in the header row, we can also merge cells vertically using `merge_at()` and specifying the rows (`i`) and column (`j`).  

```{r}
my_table <- my_table %>% 
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header")

```

**Background**

To distinguish the content of the table from the headers, we may want to add additional formatting. e.g. changing the background colour. In this example we change the table body to gray.

```{r}
my_table <- my_table %>% 
    bg(part = "body", bg = "gray95")  

my_table 
```


<!-- ======================================================= -->
## Conditional flextable formatting {  }

We can  highlight all values in a column that meet a certain rule, e.g. where more than 55% of cases died. 

```{r}

my_table %>% 
  bg(j=7, i= ~ Pct_Death >=55, part = "body", bg = "red") 

```



Or, we can higlight the entire row meeting a certain criterion, such as a hospital of interest. This is particularly helpful when looping through e.g. reports per geographical area, to highlight in tables where the current iteration compares to the other geographies. To do this we just remove the column (`j`) specification.


```{r}

my_table %>% 
  bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") 

```

<!-- ======================================================= -->
## Saving your table {  }

There are different ways the table can be integrated into your output. 

**Save single table**

You can export the tables to Word, PowerPoint or HTML or as an image (PNG) files. To do this, one of the following functions is used:

* save_as_docx
* save_as_pptx
* save_as_image
* save_as_html

For instance: 
```{r message=FALSE, warning=FALSE}
save_as_docx("my table" = my_table, path = "file.docx")
# Edit the 'my table' as needed for the title of table. If not specified the whole file will be blank. 

save_as_image(my_table, path = "file.png")
```

Note the packages `webshot` or `webshot2` are required to save a flextable as an image.Images may come out with transparent backgrounds.

If you want to view a 'live' versions of the flextable output in the intended document format, for instance so you can see if it fits in the page or so you can copy it into another document, you can use the print method with the argument preview set to “pptx” or “docx”. The document will pop up.

```
print(my_table, preview = "docx") # Word document example
print(my_table, preview = "pptx") # Powerpoint example
```

**Save table to R markdown document**

This table can be integrated into your an automated document, an R markdown output, if the table object is called within the R markdown chunk. This means the table can be updated as part of a report where the data might change, so the numbers can be refreshed.

See detail in the R markdown section of this handbook. 

<!-- ======================================================= -->
## Resources {  }

The full **flextable** book is here: https://ardata-fr.github.io/flextable-book/
The Github site is [here](https://davidgohel.github.io/flextable/)  
A manual of all the **flextable** functions can be found [here](https://davidgohel.github.io/flextable/reference/index.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/tables.Rmd-->


# Age pyramids and Likert-scales {}  

Age pyramids can be useful to show patterns by age group. They can show gender, or the distribution of other characteristics. These tabs demonstrate how to produce age pyramids using:  

* Fast & easy:   Using the **apyramid** package  
* More flexible: Using `ggplot()`  
* Having baseline demographics displayed in the background of the pyramid  
* Using pyramid-style plots to show other types of data (e.g responses to **Likert-style** survey questions)  


<!-- ======================================================= -->
## Overview {}

```{r, out.width = c('50%', '50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pop_pyramid_baseline.png"))

knitr::include_graphics(here::here("images", "likert.png"))
```


Age/gender demographic pyramids in R are generally made with `ggplot()` by creating two barplots (one for each gender), converting one's values to negative values, and flipping the x and y axes to display the barplots vertically. 

Here we offer a quick approach through the **apyramid** package:  

* More customizable code using the raw `ggplot()` commands  
* How to combine case demographic data and compare with that of a baseline population (as shown above)  
* Application of these methods to show other types of data (e.g. responses to Likert-style survey questions)  



<!-- ======================================================= -->
## Preparation {}

For this tab we use the `linelist` dataset that is cleaned in the Cleaning tab.  

To make a traditional age/sex demographic pyramid, the data must first be cleaned in the following ways:  

* The gender column must be cleaned.  
* Age should be in an age category column, and should be an of class Factor (with correctly ordered levels)

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(rio,       # to import data
               here,      # to locate files
               tidyverse, # to clean, handle, and plot the data (includes ggplot2 package)
               apyramid,  # a package dedicated to creating age pyramids
               stringr)   # working with strings for titles, captions, etc.
```

### Import data {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow along step-by-step, see instruction in the [Download book and data] page.  

```{r, echo=F}
linelist <- rio::import(here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.csv")
```

### Check column classes {-}  

Ensure that the column you want to use for age is class Numeric.  

```{r}
class(linelist$age_years)
```


<!-- ======================================================= -->
## **apyramid** package {}

The package **apyramid** allows you to quickly make an age pyramid. For more nuanced situations, see the tab on using `ggplot()` to make age pyramids. You can read more about the **apyramid** package in its Help page by entering `?age_pyramid` in your R console. 

### Linelist data  


Using the cleaned linelist dataset, we can create an age pyramid with just one simple command. If you need help cleaning your data, see the handbook page on Cleaning data and core functions (LINK). In this command:  

* The *data* argument is set as the `linelist` dataframe  
* The *age_group* argument is set to the name (in quotes) of the numeric category variable (in this case `age_cat5`)  
* The *split_by* argument (bar colors) should be a binary column (in this case "gender")  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender")
```


The same result can be shown as percents of all cases, instead of counts, by setting `proportional = TRUE`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      proportional = TRUE)
```


When using **agepyramid** package, if the `split_by` column is binary (e.g. male/female, or yes/no), then the result will appear as a pyramid. However if there are more than two values in the `split_by` column (not including `NA`), the pyramid will appears as a faceted barplot with empty bars in the background indicating the range of the un-faceted data set for the age group. Values of split_by will appear as labels at top of each facet. For example below if the `split_by` variable is "hospital".  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "hospital",
                      na.rm = FALSE)        # show a bar for patients missing age, (note: this changes the pyramid into a faceted barplot)
```

**Missing values**  
Rows missing values for the `split_by` or `age_group` columns, if coded as `NA`, will not trigger the faceting shown above. By default these rows will not be shown. However you can specify that they appear, in an adjacent barplot and as a separate age group at the top, by specifying `na.rm = FALSE`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      na.rm = FALSE)         # show patients missing age or gender
```

**Proportions, colors, & aesthetics**  

By default, the bars display counts (not %), a dashed mid-line for each group is shown, and the colors are green/purple. Each of these parameters can all be adjusted, as shown below:  

You can also add additional `ggplot()` commands to the plot using the standard `ggplot()` "+" syntax, such as aesthetic themes and label adjustments: 

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      proportional = TRUE,                  # show percents, not counts
                      show_midpoint = FALSE,                # remove bar mid-point line
                      #pal = c("orange", "purple")          # can specify alt. colors here (but not labels, see below)
                      )+                 
  
  # additional ggplot commands
  theme_minimal()+                                          # simplify the background
  scale_fill_manual(values = c("orange", "purple"),         # to specify colors AND labels
                     labels = c("Male", "Female"))+
  labs(y = "Percent of all cases",                          # note that x and y labels are switched (see ggplot tab)
       x = "Age categories",                          
       fill = "Gender", 
       caption = "My data source and caption here",
       title = "Title of my plot",
       subtitle = "Subtitle with \n a second line...")+
  theme(
    legend.position = "bottom",                             # move legend to bottom
    axis.text = element_text(size = 10, face = "bold"),     # fonts/sizes, see ggplot tips page
    axis.title = element_text(size = 12, face = "bold"))
```



### Aggregated data  

The examples above assume your data are in a linelist-like format, with one row per observation. If your data are already aggregated into counts by age category, you can still use the **apyramid** package, as shown below.  

This code aggregates the linelist data into counts by age category and gender, in a "wide" format. Learn more about [Grouping data] and [Pivoting data] in their respective pages:  

```{r, warning=F, message=F}
demo_agg <- linelist %>% 
  count(age_cat5, gender, name = "cases") %>% 
  pivot_wider(id_cols = age_cat5, names_from = gender, values_from = cases) %>% 
  rename(`missing_gender` = `NA`)
```

...which makes the dataset looks like this: with columns for age category, and male counts, female counts, and missing counts.  

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

To set-up these data for the age pyramid, we will pivot the data to be "long" with the `pivot_longer()` function from **dplyr**. This is because `ggplot()` generally prefers "long" data, and **apyramid** is using `ggplot()`.  

```{r, warning=F, message=F}
# pivot the aggregated data into long format
demo_agg_long <- demo_agg %>% 
  pivot_longer(c(f, m, missing_gender),            # cols to elongate
               names_to = "gender",                # name for new col of categories
               values_to = "counts") %>%           # name for new col of counts
  mutate(gender = na_if(gender, "missing_gender")) # convert "missing_gender" to NA
``` 

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg_long, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Then use the `split_by` and `count` arguments of `age_pyramid()` to specify the respective columns:  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = demo_agg_long,
                      age_group = "age_cat5",
                      split_by = "gender",
                      count = "counts")      # give the column name for the aggregated counts
```

Note in the above, that the factor order of "m" and "f" is different (pyramid reversed). To adjust the order you must re-define gender in the aggredated data as a Factor and order the levels as desired.  




<!-- ======================================================= -->
## `ggplot()` {}

Using `ggplot()` to build your age pyramid allows for more flexibility, but requires more effort and understanding of how `ggplot()` works. It is also easier to accidentally make mistakes.  

**apyramid** uses `ggplot()` in the background (and accepts `ggplot()` commands added), but this page shows how to adjust or recreate a pyramid only using `ggplot()`, if you wish.  



<!-- ======================================================= -->
### Constructing the plot {} 

First, understand that to make such a pyramid using `ggplot()` the approach is to:

* Within the `ggplot()`, create **two** graphs by age category. Create one for each of the two grouping values (in this case gender). See filters applied to the `data` arguments in each `geom_histogram()` commands below.  

* If using `geom_histogram()`, the graphs operate off the numeric column (e.g. `age_years`), whereas if using `geom_barplot()` the graphs operate from an ordered Factor (e.g. `age_cat5`). 

* One graph will have positive count values, while the other will have its counts converted to negative values - this allows both graphs to be seen and compared against each other in the same plot.  

* The command `coord_flip()` switches the X and Y axes, resulting in the graphs turning vertical and creating the pyramid.

* Lastly, the counts-axis labels must be specified so they appear as "positive" counts on both sides of the pyramid (despite the underlying values on one side being negative). 

A **simple** version of this, using `geom_histogram()`, is below:

```{r, warning=F, message=F}
  # begin ggplot
  ggplot(data = linelist, aes(x = age, fill = gender)) +
  
  # female histogram
  geom_histogram(data = filter(linelist, gender == "f"),
                 breaks = seq(0,85,5),
                 colour = "white") +
  
  # male histogram (values converted to negative)
  geom_histogram(data = filter(linelist, gender == "m"),
                 breaks = seq(0,85,5),
                 aes(y=..count..*(-1)),
                 colour = "white") +
  
  # flip the X and Y axes
  coord_flip() +
  
  # adjust counts-axis scale
  scale_y_continuous(limits = c(-600, 900),
                     breaks = seq(-600,900,100),
                     labels = abs(seq(-600, 900, 100)))
```

<span style="color: red;">**_DANGER:_** If the **limits** of your counts axis are set too low, and a counts bar exceeds them, the bar will disappear entirely or be artificially shortened! Watch for this if analyzing data which is routinely updated. Prevent it by having your count-axis limits auto-adjust to your data, as below.</span>  

There are many things you can change/add to this simple version, including:  

* Auto adjust counts-axis count scale to your data (avoid errors discussed in warning below)  
* Manually specify colors and legend labels  

Convert counts to percents:  

```{r, warning=F, message=F}
# create dataset with proportion of total
pyramid_data <- linelist %>%
  count(age_cat5, gender, name = "counts") %>% 
  ungroup() %>%                                   # ungroup so percent calculations are not by group
  mutate(percent = round(100*(counts / sum(counts, na.rm=T)),1), 
         percent = case_when(
            gender == "f" ~ percent,
            gender == "m" ~ -percent,
            TRUE          ~ NA_real_))
```

Importantly here we save the max and min (which is the max in the opposite direction) percents so we know how tall the scale should be. These will be used in the `ggplot()` command below.    
```{r}
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)

max_per
min_per
```

Finally we make the `ggplot()` on the percent data. We specify `scale_y_continuous()` to extend the pre-defined lengths in each direction (positive and "negative"). We use `floor()` and `ceiling()` to round decimals the appropriate direction (down or up) for the side of the axis.  

```{r, warning=F, message=F}
# begin ggplot
  ggplot()+  # default x-axis is age in years;

  # case data graph
  geom_bar(data = pyramid_data,
           stat = "identity",
           aes(x = age_cat5,
               y = percent,
               fill = gender),        # 
           colour = "white")+         # white around each bar
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  

  # adjust the axes scales (remember they are flipped now!)
  #scale_x_continuous(breaks = seq(0,100,5), labels = seq(0,100,5)) +
  scale_y_continuous(limits = c(min_per, max_per),
                     breaks = seq(floor(min_per), ceiling(max_per), 2),
                     labels = paste0(abs(seq(floor(min_per), ceiling(max_per), 2)), "%"))+

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",
               "m" = "darkgreen"),
    labels = c("Female", "Male"),
  ) +
  
  # label values (remember X and Y flipped now)
  labs(
    x = "Age group",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Data are from linelist \nn = {nrow(linelist)} (age or sex missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases) \nData as of: {format(Sys.Date(), '%d %b %Y')}")) +
  
  # optional aesthetic themes
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0.5), 
    plot.caption = element_text(hjust=0, size=11, face = "italic")) + 
  
  ggtitle(paste0("Age and gender of cases"))
    

```



<!-- ======================================================= -->
### Compare to baseline  

With the flexibility of `ggplot()`, you can have a second layer of bars in the background that represent the true population pyramid. This can provide a nice visualization to compare the observed counts with the baseline.  

Import and view the population data
```{r echo=F}
# import the population demographics data
pop <- rio::import(here::here("data", "country_demographics.csv"))
```

```{r eval=F}
# import the population demographics data
pop <- rio::import("country_demographics.csv")
```

```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop, rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```
First some data management steps:  

Here we record the order of age categories that we want to appear. Due to some quirks the way the `ggplot()` is implemented, it is easiest to store these as a character vector and use them later in the plotting function.  

```{r}
# record correct age cat levels
age_levels <- c("0-4","5-9", "10-14", "15-19", "20-24",
                "25-29","30-34", "35-39", "40-44", "45-49",
                "50-54", "55-59", "60-64", "65-69", "70-74",
                "75-79", "80-84", "85+")
```

Combine the population and case data through the **dplyr** function `bind_rows()`:  

* First, ensure they have the *exact same* column names, age categories values, and gender values  
* Make them have the same data structure: columns of age category, gender, counts, and percent of total  
* Bind them together, one on-top of the other (`bind_rows()`)  



```{r, warning=F, message=F}
# create/transform populaton data, with percent of total
########################################################
pop_data <- pivot_longer(pop, c(m, f), names_to = "gender", values_to = "counts") %>% # pivot gender columns longer
  mutate(data = "population",                                                         # add column designating data source
         percent  = round(100*(counts / sum(counts, na.rm=T)),1),                     # calculate % of total
         percent  = case_when(                                                        # if male, convert % to negative
                            gender == "f" ~ percent,
                            gender == "m" ~ -percent,
                            TRUE          ~ NA_real_))
```

Review the changed population dataset
```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop_data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now implement the same for the case linelist.  Slightly different because it begins with case-rows, not counts.  

```{r, warning=F, message=F}
# create case data by age/gender, with percent of total
#######################################################
case_data <- linelist %>%
  group_by(age_cat5, gender) %>%  # aggregate linelist cases into age-gender groups
  summarize(counts = n()) %>%     # calculate counts per age-gender group
  ungroup() %>% 
  mutate(data = "cases",                                          # add column designating data source
         percent = round(100*(counts / sum(counts, na.rm=T)),1),  # calculate % of total for age-gender groups
         percent = case_when(                                     # convert % to negative if male
            gender == "f" ~ percent,
            gender == "m" ~ -percent,
            TRUE          ~ NA_real_))
```

Review the changed case dataset  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(case_data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now the two datasets are combined, one on top of the other (same column names)
```{r, warning=F, message=F}
# combine case and population data (same column names, age_cat values, and gender values)
pyramid_data <- bind_rows(case_data, pop_data)
```

Store the maximum and minimum percent values, used in the plotting funtion to define the extent of the plot (and not cut off any bars!)  

```{r}
# Define extent of percent axis, used for plot limits
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)
```

Now the plot is made with `ggplot()`: 

* One bar graph of population data (wider, more transparent bars)
* One bar graph of case data (small, more solid bars)  


```{r, warning=F, message=F}

# begin ggplot
##############
ggplot()+  # default x-axis is age in years;

  # population data graph
  geom_bar(data = filter(pyramid_data, data == "population"),
           stat = "identity",
           aes(x = age_cat5,
               y = percent,
               fill = gender),        
           colour = "black",                               # black color around bars
           alpha = 0.2,                                    # more transparent
           width = 1)+                                     # full width
  
  # case data graph
  geom_bar(data = filter(pyramid_data, data == "cases"), 
           stat = "identity",                              # use % as given in data, not counting rows
           aes(x = age_cat5,                               # age categories as original X axis
               y = percent,                                # % as original Y-axis
               fill = gender),                             # fill of bars by gender
           colour = "black",                               # black color around bars
           alpha = 1,                                      # not transparent 
           width = 0.3)+                                   # half width
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  
  # adjust axes order, scale, and labels (remember X and Y axes are flipped now)
  # manually ensure that age-axis is ordered correctly
  scale_x_discrete(limits = age_levels)+ 
  
  # set percent-axis 
  scale_y_continuous(limits = c(min_per, max_per),                                          # min and max defined above
                     breaks = seq(floor(min_per), ceiling(max_per), by = 2),                # from min% to max% by 2 
                     labels = paste0(                                                       # for the labels, paste together... 
                       abs(seq(floor(min_per), ceiling(max_per), by = 2)),                  # ...rounded absolute values of breaks... 
                       "%"))+                                                               # ... with "%"
                                                                                            # floor(), ceiling() round down and up 

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",         # assign colors to values in the data
               "m" = "darkgreen"),
    labels = c("f" = "Female",
               "m"= "Male"),      # change labels that appear in legend, note order
  ) +

  # plot labels, titles, caption    
  labs(
    title = "Case age and gender distribution,\nas compared to baseline population",
    subtitle = "",
    x = "Age category",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Cases shown on top of country demographic baseline\nCase data are from linelist, n = {nrow(linelist)}\nAge or gender missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases\nCase data as of: {format(max(linelist$date_onset, na.rm=T), '%d %b %Y')}")) +
  
  # optional aesthetic themes
  theme(
    legend.position = "bottom",                             # move legend to bottom
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0), 
    plot.caption = element_text(hjust=0, size=11, face = "italic"))

```


<!-- ======================================================= -->
## Likert scale {}

The techniques used to make a population pyramid with `ggplot()` can also be used to make plots of Likert-scale survey data.  

```{r, eval=F, echo=F}
data_raw <- import("P:/Shared/equateur_mve_2020/lessons learned/Ebola After-Action Survey - HQ epi team (form responses).csv")


likert_data <- data_raw %>% 
  select(2, 4:11) %>% 
  rename(status = 1,
         Q1 = 2,
         Q2 = 3,
            Q3 = 4,
            Q4 = 5,
            Q5 = 6,
            Q6 = 7,
            Q7 = 8,
            Q8 = 9) %>% 
  mutate(status = case_when(
           stringr::str_detect(status, "Mar") ~ "Senior",
           stringr::str_detect(status, "Jan") ~ "Intermediate",
           stringr::str_detect(status, "Feb") ~ "Junior",
           TRUE ~ "Senior")) %>% 
  mutate(Q4 = recode(Q4, "Not applicable" = "Very Poor"))

table(likert_data$status)

rio::export(likert_data, here::here("data", "likert_data.csv"))
```

Import the data

```{r echo=F}
# import the likert survey response data
likert_data <- rio::import(here::here("data", "likert_data.csv"))
```

```{r, eval=F}
# import the likert survey response data
likert_data <- rio::import("likert_data.csv")
```

Start with data that looks like this, with a categorical classification of each respondent (`status`) and their answers to 8 questions on a 4-point Likert-type scale ("Very poor", "Poor", "Good", "Very good").  

```{r, echo=F, message=FALSE}
# display the linelist data as a table
DT::datatable(likert_data, rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```

First, some data management steps:  

* Pivot the data longer  
* Create new column `direction` depending on whether response was generally "positive" or "negative"  
* Set the Factor level order for the `status` column and the `Response` column  
* Store the max count value so limits of plot are appropriate  


```{r, warning=F, message=F}
melted <- pivot_longer(likert_data, Q1:Q8, names_to = "Question", values_to = "Response") %>% 
     mutate(direction = case_when(
               Response %in% c("Poor","Very Poor") ~ "Negative",
               Response %in% c("Good", "Very Good") ~ "Positive",
               TRUE ~ "Unknown"),
            status = factor(status, levels = rev(c(
                 "Senior", "Intermediate", "Junior"))),
            Response = factor(Response, levels = c("Very Good", "Good",
                                             "Very Poor", "Poor"))) # must reverse Very Poor and Poor for ordering to work

melted_max <- melted %>% 
   group_by(status, Question) %>% 
   summarize(n = n())
```

Just like in the `ggplot()` age pyramid, we save the max value to dynamically calibrate our axis scale.  

```{r}
melted_max <- max(melted_max$n, na.rm=T)
melted_max
```


Now make the plot:

```{r, warning=F, message=F}
# make plot
ggplot()+
     # bar graph of the "negative" responses 
     geom_bar(data = filter(melted,
                            direction == "Negative"), 
              aes(x = status,
                        y=..count..*(-1),    # counts inverted to negative
                        fill = Response),
                    color = "black",
                    closed = "left", 
                    position = "stack")+
     
     # bar graph of the "positive responses
     geom_bar(data = filter(melted, direction == "Positive"),
              aes(x = status, fill = Response),
              colour = "black",
              closed = "left",
              position = "stack")+
     
     # flip the X and Y axes
     coord_flip()+
  
     # Black vertical line at 0
     geom_hline(yintercept = 0, color = "black", size=1)+
     
    # convert labels to all positive numbers
    scale_y_continuous(limits = c(-ceiling(melted_max/10)*11, ceiling(melted_max/10)*10),   # seq from neg to pos by 10, edges rounded outward to nearest 5
                       breaks = seq(-ceiling(melted_max/10)*10, ceiling(melted_max/10)*10, 10),
                       labels = abs(unique(c(seq(-ceiling(melted_max/10)*10, 0, 10),
                                            seq(0, ceiling(melted_max/10)*10, 10))))) +
     
    # color scales manually assigned 
    scale_fill_manual(values = c("Very Good"  = "green4", # assigns colors
                                  "Good"      = "green3",
                                  "Poor"      = "yellow",
                                  "Very Poor" = "red3"),
                       breaks = c("Very Good", "Good", "Poor", "Very Poor"))+ # orders the legend
     
    
     
    # facet the entire plot so each question is a sub-plot
    facet_wrap(~Question, ncol = 3)+
     
    # labels, titles, caption
    labs(x = "Respondent status",
          y = "Number of responses",
          fill = "")+
     ggtitle(str_glue("Likert-style responses\nn = {nrow(likert_data)}"))+

     # aesthetic settings
     theme_minimal()+
     theme(axis.text = element_text(size = 12),
           axis.title = element_text(size = 14, face = "bold"),
           strip.text = element_text(size = 14, face = "bold"),  # facet sub-titles
           plot.title = element_text(size = 20, face = "bold"),
           panel.background = element_rect(fill = NA, color = "black")) # black box around each facet
```


<!-- ======================================================= -->
## Resources {}




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/age_pyramid.Rmd-->


# Diagrams and charts { }  

<!-- ======================================================= -->
## Overview { }

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "flow_chart.png"))
knitr::include_graphics(here::here("images", "sankey_diagram.png"))
```


This page covers:  

* Flow diagrams using **DiagrammeR**  
* Alluvial/Sankey diagrams  
* Event timelines  
* GANTT charts  
* Dendrogram organizational trees (e.g. of folder contents)  
* DAGs (Directed Acyclic Graphs)  



<!-- ======================================================= -->
## Preparation { }

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  DiagrammeR,     # for flow diagrams
  networkD3       # For alluvial/Sankey diagrams
  )
```

### Import data {-}  

Most of the content in this page does not require a dataset. However, in the Sankey diagram section, we will use the case linelist from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instruction in the [Download book and data] page. It is previewed below.  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Flow diagrams { }

One can use the R package **DiagrammeR** to create charts/flow charts. They can be static, or they can adjust somewhat dynamically based on changes in a dataset.  

**Tools**  

The function `grViz()` is used to create a "Graphviz" diagram. This function accepts a *character string input containing instructions* for making the diagram. Within that string, the instructions are written in a different language, called [DOT](https://graphviz.org/doc/info/lang.html) - it is quite easy to learn the basics.  

**Basic structure**  

1) Open the instructions `grViz("`  
2) Specify directionality and name of the graph, and open brackets, e.g. `digraph my_flow_chart {`
3) Graph statement (layout, rank direction)  
4) Nodes statements (create nodes)
5) Edges statements (gives links between nodes)  
6) Close the instructions `}")`  

### Simple examples  

Below are two simple examples  

A very minimal example:  

```{r out.width='50%'}
# A minimal plot
DiagrammeR::grViz("digraph {
  
graph[layout = dot, rankdir = LR]

a
b
c

a -> b -> c
}")
```

An example with applied public health context:  

```{r out.width='50%'}
grViz("                           # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,
         overlap = true,
         fontsize = 10]
  
  # nodes
  #######
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]               # width of circles
  
  Primary                         # names of nodes
  Secondary
  Tertiary

  # edges
  #######
  Primary   -> Secondary [label = ' case transfer']
  Secondary -> Tertiary [label = ' case transfer']
}
")
```

### Syntax  

**Basic syntax**  

Node names, or edge statements, can be separated with spaces, semicolons, or newlines.  

**Rank direction**  

A plot can be re-oriented to move left-to-right by adjusting the `rankdir` argument within the graph statement. The default is TB (top-to-bottom), but it can be LR (left-to-right), RL, or BT.  

**Node names**  

Node names can be single words, as in the simple example above. To use multi-word names or special characters (e.g. parentheses, dashes), put the node name within single quotes (' '). It may be easier to have a short node name, and assign a *label*, as shown below within brackets [ ]. If you want to have a newline within the node's name, you must do it via a label - use `\n` in the node label within single quotes, as shown below.  

**Subgroups**  
Within edge statements, subgroups can be created on either side of the edge with curly brackets ({ }). The edge then applies to all nodes in the bracket - it is a shorthand.  


**Layouts**  

* dot (set `rankdir` to either TB, LR, RL, BT, )
* neato  
* twopi  
* circo  


**Nodes - editable attributes**  

* `label` (text, in single quotes if multi-word)  
* `fillcolor` (many possible colors)  
* `fontcolor`  
* `alpha` (transparency 0-1)  
* `shape` (ellipse, oval, diamond, egg, plaintext, point, square, triangle)  
* `style`  
* `sides`  
* `peripheries`  
* `fixedsize` (h x w)  
* `height`  
* `width`  
* `distortion`  
* `penwidth` (width of shape border)  
* `x` (displacement left/right)  
* `y` (displacement up/down)  
* `fontname`  
* `fontsize`  
* `icon`  


**Edges - editable attributes**  

* `arrowsize`  
* `arrowhead` (normal, box, crow, curve, diamond, dot, inv, none, tee, vee)  
* `arrowtail`  
* `dir` (direction, )  
* `style` (dashed, ...)  
* `color`  
* `alpha`  
* `headport` (text in front of arrowhead)  
* `tailport` (text in behind arrowtail)  
* `fontname`  
* `fontsize`  
* `fontcolor`  
* `penwidth` (width of arrow)  
* `minlen` (minimum length)

**Color names**: hexadecimal values or 'X11' color names, see [here for X11 details](http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html) 


### Complex examples  

The example below expands on the surveillance_diagram, adding complex node names, grouped edges, colors and styling


```
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```


```{r out.width='50%', echo=F}
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```

**Sub-graph clusters**  

To group nodes into boxed clusters, put them within the same named subgraph (`subgraph name {}`). To have each subgraph identified within a bounding box, begin the name of the subgraph with "cluster", as shown with the 4 boxes below.  

```
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance']; 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)']; 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)'];
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media'; Radio} -> EBS
  
  RECOs -> CBS
}
")

```


```{r out.width='120%', echo=F}
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance']; 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)']; 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)'];
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media'; Radio} -> EBS
  
  RECOs -> CBS
}
")

```


**Node shapes**  

The example below, borrowed from [this tutorial](http://rich-iannone.github.io/DiagrammeR/), shows applied node shapes and a shorthand for serial edge connections  

```{r out.width='75%'}
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle, style = filled, fillcolor = Linen]

data1 [label = 'Dataset 1', shape = folder, fillcolor = Beige]
data2 [label = 'Dataset 2', shape = folder, fillcolor = Beige]
process [label =  'Process \n Data']
statistical [label = 'Statistical \n Analysis']
results [label= 'Results']

# edge definitions with the node IDs
{data1 data2}  -> process -> statistical -> results
}")
```


### Outputs  

How to handle and save outputs  

* Outputs will appear in RStudio's Viewer pane, by default in the lower-right alongside Files, Plots, Packages, and Help.  
* To export you can "Save as image" or "Copy to clipboard" from the Viewer. The graphic will adjust to the specified size.  




### Parameterized figures  

"Parameterized figures: A great benefit of designing figures within R is that we are able to connect the figures directly with our analysis by reading R values directly into our flowcharts. For example, suppose you have created a filtering process which removes values after each stage of a process, you can have a figure show the number of values left in the dataset after each stage of your process. To do this we, you can use the @@X symbol directly within the figure, then refer to this in the footer of the plot using [X]:, where X is the a unique numeric index. Here is a basic example:"  
https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/

```{r, eval=F}
# Define some sample data
data <- list(a=1000, b=800, c=600, d=400)


DiagrammeR::grViz("
digraph graph2 {

graph [layout = dot]

# node definitions with substituted label text
node [shape = rectangle, width = 4, fillcolor = Biege]
a [label = '@@1']
b [label = '@@2']
c [label = '@@3']
d [label = '@@4']

a -> b -> c -> d

}

[1]:  paste0('Raw Data (n = ', data$a, ')')
[2]: paste0('Remove Errors (n = ', data$b, ')')
[3]: paste0('Identify Potential Customers (n = ', data$c, ')')
[4]: paste0('Select Top Priorities (n = ', data$d, ')')
")

```


Much of the above is adapted from the tutorial [at this site](https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/)  

Other more in-depth tutorial: http://rich-iannone.github.io/DiagrammeR/



### CONSORT diagram  

https://scriptsandstatistics.wordpress.com/2017/12/22/how-to-draw-a-consort-flow-diagram-using-r-and-graphviz/

Note above is out of date via DiagrammeR




<!-- ======================================================= -->
## Alluvial/Sankey Diagrams { }

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(networkD3)
```

### Plotting from dataset  

Plotting the connections in a dataset. Below we demonstrate using this package on the case `linelist`.  

Here is an [online tutorial](https://www.r-graph-gallery.com/321-introduction-to-interactive-sankey-diagram-2.html).    


Counts of age category and hospital, re-labled as target and source, respectively.  

```{r}
# counts by hospital and age category
links <- linelist %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = hospital,
         target = age_cat)
```


```{r message=FALSE, echo=F}
DT::datatable(head(links, 30), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Now formalize the nodes list, and adjust the ID columns to be numbers instead of labels:  


```{r}
# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

# match to numbers, not names
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1
```

Now plot the Sankey diagram:  

```{r}

# plot
######
p <- sankeyNetwork(Links = links,
                   Nodes = nodes,
                   Source = "IDsource",
                   Target = "IDtarget",
                   Value = "n",
                   NodeID = "name",
                   units = "TWh",
                   fontSize = 12,
                   nodeWidth = 30)
p
```



Here is an example where the patient Outome is included as well. Note in the data management step how we bind rows of counts of hospital -> outcome, using the same column names.   

```{r}
# counts by hospital and age category
links <- linelist %>% 
  select(hospital, age_cat) %>%
  mutate(age_cat = stringr::str_glue("Age {age_cat}")) %>% 
  count(hospital, age_cat) %>% 
  rename(source = age_cat,
         target = hospital) %>% 
  bind_rows(
    linelist %>% 
      select(hospital, outcome) %>% 
      count(hospital, outcome) %>% 
      rename(source = hospital,
             target = outcome)
  )

# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

# match to numbers, not names
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1

# plot
######
p <- sankeyNetwork(Links = links,
                   Nodes = nodes,
                   Source = "IDsource",
                   Target = "IDtarget",
                   Value = "n",
                   NodeID = "name",
                   units = "TWh",
                   fontSize = 12,
                   nodeWidth = 30)
p

```


https://www.displayr.com/sankey-diagrams-r/

Timeline Sankey - LTFU from cohort... application/rejections... etc.


<!-- ======================================================= -->
## Event timelines { }

To make a timeline showing specific events, you can use the `vistime` package.

See this [vignette](https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning)

```{r}
# load package
pacman::p_load(vistime,  # make the timeline
               plotly    # for interactive visualization
               )
```

```{r, echo=F}
# reference: https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning

data <- read.csv(text="event, group, start, end, color
                       Event 1, Group A,2020-01-22,2020-01-22, #90caf9
                       Event 1, Group B,2020-01-23,2020-01-23, #90caf9
                       Event 1, Group C,2020-01-23,2020-01-23, #1565c0
                       Event 1, Group D,2020-01-25,2020-01-25, #f44336
                       Event 1, Group E,2020-01-25,2020-01-25, #90caf9
                       Event 1, Group F,2020-01-26,2020-01-26, #8d6e63
                       Event 1, Group G,2020-01-27,2020-01-27, #1565c0
                       Event 1, Group H,2020-01-27,2020-01-27, #90caf9
                       Event 1, Group I,2020-01-27,2020-01-27,#90a4ae
                       Event 2, Group A,2020-01-28,2020-01-28,#fc8d62
                       Event 2, Group C,2020-01-28,2020-01-28, #6a3d9a
                       Event 2, Group J,2020-01-28,2020-01-28, #90caf9
                       Event 2, Group J,2020-01-28,2020-01-28, #fc8d62
                       Event 2, Group J,2020-01-28,2020-01-28, #1565c0
")
```

Here is the events dataset we begin with:  

```{r message=FALSE, echo=F}
DT::datatable(data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



```{r}
p <- vistime(data)    # apply vistime

library(plotly)

# step 1: transform into a list
pp <- plotly_build(p)

# step 2: Marker size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "markers") pp$x$data[[i]]$marker$size <- 10
}

# step 3: text size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textfont$size <- 10
}


# step 4: text position
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textposition <- "right"
}

#print
pp

```



<!-- ======================================================= -->
## DAGs { }

You can build a DAG manually using the **DiagammeR** package and DOT language, as described in another tab. Alternatively, there are packages like **ggdag** and **daggity**

https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html

https://www.r-bloggers.com/2019/08/causal-inference-with-dags-in-r/#:~:text=In%20a%20DAG%20all%20the,for%20drawing%20and%20analyzing%20DAGs.




<!-- ======================================================= -->
## Resources { }

Links to other online tutorials or resources.




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/diagrams.Rmd-->


# Combinations analysis { }  

<!-- ======================================================= -->
## Overview {  }


This analysis plots the frequency of different **combinations** of values/responses. In this example, we plot the frequency of symptom combinations.  

This analysis is often called:  

* **Multiple response analysis**  
* **Sets analysis**  
* **Combinations analysis**  

The first method shown uses the package **ggupset**, an the second using the package **UpSetR**. 


An example plot is below. Five symptoms are shown. Below each vertical bar is a line and dots indicating the combination of symptoms reflected by the bar above. To the right, horizontal bars reflect the frequency of each individual symptom.


```{r echo=F, out.width= "100%", warning=F, message=F}
pacman::p_load(tidyverse,
               UpSetR,
               ggupset)

# Adds new symptom variables to the linelist, with random "yes" or "no" values 
linelist_sym <- linelist %>% 
  mutate(fever  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.80, 0.20)),
         chills = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.20, 0.80)),
         cough  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.9, 0.15)),
         aches  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.10, 0.90)),
         shortness_of_breath = sample(c("yes", "no"), nrow(linelist), replace = T))

linelist_sym_2 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into the symptom name itself
  mutate(fever = case_when(fever == "yes" ~ 1,          # if old value is "yes", new value is "fever"
                           TRUE           ~ 0),   # if old value is anything other than "yes", the new value is NA
         
         chills = case_when(chills == "yes" ~ 1,
                           TRUE           ~ 0),
         
         cough = case_when(cough == "yes" ~ 1,
                           TRUE           ~ 0),
         
         aches = case_when(aches == "yes" ~ 1,
                           TRUE           ~ 0),
         
         shortness_of_breath = case_when(shortness_of_breath == "yes" ~ 1,
                           TRUE           ~ 0))

# Make the plot
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, shortness_of_breath),
  sets = c("fever", "chills", "cough", "aches", "shortness_of_breath"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```
  



<!-- ======================================================= -->
## Preparation {  }

### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, warning=F, message=F}
pacman::p_load(
  tidyverse,     # data management and visualization
  UpSetR,        # special package for combination plots
  ggupset)       # special package for combination plots
```

<!-- ======================================================= -->
### Import data {-}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instruction in the [Download book and data] page.  

This linelist includes five "yes/no" variables on reported symptoms. We will need to transform these variables a bit to use the **ggupset** package to make our plot.  

View the data (scroll to the right to see the symptoms variables). The first 50 rows are shown.  

```{r, echo=F, warning=F, message=F}
DT::datatable(head(linelist_sym,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

<!-- ======================================================= -->
### Re-format values {-}  

To align with the format expected by **ggupset** we convert the "yes" and "no the the actual symptom name, using `case_when()` from **dplyr**. If "no", we set the value as blank. 

```{r, warning=F, message=F}
# create column with the symptoms named, separated by semicolons
linelist_sym_1 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into the symptom name itself
  mutate(fever = case_when(fever == "yes" ~ "fever",          # if old value is "yes", new value is "fever"
                           TRUE           ~ NA_character_),   # if old value is anything other than "yes", the new value is NA
         
         chills = case_when(chills == "yes" ~ "chills",
                           TRUE           ~ NA_character_),
         
         cough = case_when(cough == "yes" ~ "cough",
                           TRUE           ~ NA_character_),
         
         aches = case_when(aches == "yes" ~ "aches",
                           TRUE           ~ NA_character_),
         
         shortness_of_breath = case_when(shortness_of_breath == "yes" ~ "shortness_of_breath",
                           TRUE           ~ NA_character_))
```

Now we make two final variables:  
1. Pasting together all the symptoms of the patient (character variable)  
2. Convert the above to class *list*, so it can be accepted by **ggupset** to make the plot  

```{r, warning=F, message=F}
linelist_sym_1 <- linelist_sym_1 %>% 
  mutate(
         # combine the variables into one, using paste() with a semicolon separating any values
         all_symptoms = paste(fever, chills, cough, aches, shortness_of_breath, sep = "; "),
         
         # make a copy of all_symptoms variable, but of class "list" (which is required to use ggupset() in next step)
         all_symptoms_list = as.list(strsplit(all_symptoms, "; "))
         )
```

View the new data. Note the two columns at the end - the pasted combined values, and the list

```{r, echo=F, , warning=F, message=F}
DT::datatable(head(linelist_sym,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


<!-- ======================================================= -->
## **ggupset** {  }

Load the package

```{r}
pacman::p_load(ggupset)
```


Create the plot. We begin with a `ggplot()` and `geom_bar()`, but then we add the special `scale_x_upset()` from the package.  

```{r, warning=F, message=F}
ggplot(
  data = linelist_sym_1,
  aes(x = all_symptoms_list)) +
geom_bar() +
scale_x_upset(
  reverse = FALSE,
  n_intersections = 10,
  sets = c("fever", "chills", "cough", "aches", "shortness_of_breath"))+
  labs(title = "Signs & symptoms",
       subtitle = "10 most frequent combinations of signs and symptoms",
       caption = "Caption here.",
       x = "Symptom combination",
       y = "Frequency in dataset")

```
  
More information on **ggupset** can be found [online](https://rdrr.io/cran/ggupset/man/scale_x_upset.html) or offline in the package documentation in your RStudio Help tab `?ggupset`.  


<!-- ======================================================= -->
## `UpSetR` {  }

The **UpSetR** package allows more customization of the plot, but it can be more difficult to execute:


**Load package**  

```{r}
pacman::p_load(UpSetR)
```

**Data cleaning**  

We must convert the `linelist` symptoms values to 1/0. 

```{r}
# Make using upSetR

linelist_sym_2 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into the symptom name itself
  mutate(fever = case_when(fever == "yes" ~ 1,          # if old value is "yes", new value is "fever"
                           TRUE           ~ 0),   # if old value is anything other than "yes", the new value is NA
         
         chills = case_when(chills == "yes" ~ 1,
                           TRUE           ~ 0),
         
         cough = case_when(cough == "yes" ~ 1,
                           TRUE           ~ 0),
         
         aches = case_when(aches == "yes" ~ 1,
                           TRUE           ~ 0),
         
         shortness_of_breath = case_when(shortness_of_breath == "yes" ~ 1,
                           TRUE           ~ 0))
```

Now make the plot using the custom function `upset()` - using only the symptoms columns. You must designate which "sets" to compare (the names of the symptom columns). Alternatively, use `nsets = ` and `order.by = "freq"` to only show the top X combinations.  

```{r, warning=F, message=F}

# Make the plot
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, shortness_of_breath),
  sets = c("fever", "chills", "cough", "aches", "shortness_of_breath"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```


<!-- ======================================================= -->
## Resources {  }

https://github.com/hms-dbmi/UpSetR  *read this*
https://gehlenborglab.shinyapps.io/upsetr/ *Shiny App version - you can upload your own data*
https://cran.r-project.org/web/packages/UpSetR/UpSetR.pdf *documentation - difficult to interpret*

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/combination_analysis.Rmd-->


# Heat tiles { }  


Heat tiles ("heatmaps") can be useful visualizations when trying to display 3 variables (x-axis, y-axis, and fill). Below we demonstrate two examples:  

* A visual matrix of transmission events by age ("who infected whom")  
* Tracking reporting metrics across many facilities/jurisdictions over time  


```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "transmission_matrix.png"))

knitr::include_graphics(here::here("images", "heat_tile.png"))

```





<!-- ======================================================= -->
## Preparation { }

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,       # data manipulation and visualization
  rio,             # importing data 
  lubridate        # working with dates
  )
```

**Datasets**  

This page utilizes the case linelist of a simulated outbreak for the transmission matrix section, and a separate dataset of daily malaria case counts by facility for the metrics tracking section. They are loaded and cleaned in their individual sections.  







## Transmission matrix  

Heat tiles can be useful to visualize matrices. One example is to display "who-infected-whom" in an outbreak. This assumes that you have information on transmission events in your linelist.  

We begin from the case linelist:  

* There is one row per case  
* There is a column that contains the case_id of the infector, who is also a case in the linelist  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

The first 50 rows of the linelist are shown below for demonstration:  


```{r, echo=F}
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```


```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")
```


```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Convert to "long" {-}  

**Objective**: We need to achieve a "long"-style dataframe that contains the frequency of transmission events between each age category. This will take several data manuipulation steps to achieve.  

#### Make cases data frame {-} 

To begin, we create a dataframe of the cases, their ages, and their infectors - we call the data frame `case_ages`. The first rows are displayed below.  

```{r}
case_ages <- linelist %>% 
  select(case_id, infector, age_cat) %>% 
  rename("case_age_cat" = "age_cat")
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(case_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Make infectors data frame {-}  

Next, we create a dataframe of the infectors - at the moment it consists of a single column. These are the infector IDs from the linelist. Not every case has a known infector, so we remove missing values. The first rows are displayed below.  


```{r}
infectors <- linelist %>% 
  select(infector) %>% 
  filter(!is.na(infector))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infectors, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Next, we use joins to procure the ages of the infectors. This is not simple, because in the `linelist`, the infector's ages are not listed as such. We achieve this result by joining the case `linelist` to the infectors. We begin with the infectors, and `left_join()` (add) the case `linelist` such that the `infector` id column left-side "baseline" data frame joins to the `case_id` column in the right-side `linelist` data frame.  

Thus, the data from the infector's case record in the linelist (including age) is added to the infector row. The first rows are displayed below.  

```{r}
infector_ages <- infectors %>%             # begin with infectors
  left_join(                               # add the linelist data to each infector  
    linelist,
    by = c("infector" = "case_id")) %>%    # match infector to their information as a case
  select(infector, age_cat) %>%            # keep only columns of interest
  rename("infector_age_cat" = "age_cat")   # rename for clarity
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infector_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Then, we combine the cases and their ages with the infectors and their ages. Each of these dataframe has the column `infector`, so it is used for the join. The first rows are displayed below:    

```{r}
ages_complete <- case_ages %>%  
  left_join(
    infector_ages,
    by = "infector") %>%        # each has the column infector
  drop_na()                     # drop rows with any missing data
```


```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(ages_complete, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below, a simple cross-tabulation of counts between the case and infector age groups. Labels added for clarity.  

```{r}
table(cases = ages_complete$case_age_cat,
      infectors = ages_complete$infector_age_cat)
```
We can convert this table to a dataframe with `data.frame()` from **base** R, which also automatically converts it to "long" format, which is desired for the `ggplot()`. The first rows are shown below.  

```{r}
long_counts <- data.frame(table(
    cases     = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_counts, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


The same but we apply `prop.table()` from **base** R to the table so instead of counts we get proportion of all values. The first rows are shown below.    

```{r}
long_prop <- data.frame(prop.table(table(
    cases = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat)))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_prop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now finally we can create the heatmap with **ggplot2** package, using the `geom_tile()` function.  

* In the aesthetics `aes()` of `geom_tile()` set the x and y as the case age and infector age  
* Also in `aes()` set the argument `fill = ` to the Freq column - this is the value that will be converted to a tile color  
* Set a scale color with `scale_fill_gradient()` - you can specify the high/low colors  
  * Note that `scale_color_gradient()` is different! In this case you want the fill  
* Because the color is made via "fill", you can use the `fill = ` argument in `labs()` to change the legend title  

```{r}
ggplot(data = long_prop)+       # use long data, with proportions as Freq
  geom_tile(                    # visualize it in tiles
    aes(
      x = cases,         # x-axis is case age
      y = infectors,     # y-axis is infector age
      fill = Freq))+            # color of the tile is the Freq column in the data
  scale_fill_gradient(          # adjust the fill color of the tiles
    low = "blue",
    high = "orange")+
  labs(                         # labels
    x = "Case age",
    y = "Infector age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )
  
```



<!-- ======================================================= -->
## Reporting metrics over time { }

Often in public health, one objective is to assess trends over time for many entities (facilities, jurisdictions, etc.). One way to visualize such trends over time is a heatmap where the x-axis is time and the y-axis are the many entities.  



### Preparation {-}

We begin by importing a dataset of daily malaria reports from many facilities. The reports contain a date, province, district, and malaria counts. See the page on [Download book and data] for informaton on how to download these data. Below are the first 30 rows:  

```{r, echo=F}
facility_count_data <- rio::import(here::here("data", "facility_count_data.rds")) %>% 
  select(location_name, data_date, District, malaria_tot)
```

```{r, eval=F}
facility_count_data <- import("facility_count_data.rds")
```


```{r, echo=F}
DT::datatable(head(facility_count_data,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


#### Aggregate and summarize {-}

**The objective in this example** is to transform the daily facility total malaria case counts (seen in previous tab) into *weekly* summary statistics of facility reporting performance - in this case *the proportion of days per week that the facility reported any data*. For this example we will show data only for **Spring District** from April-May 2019.  

To achieve this we will do the following data management steps:  

1) Filter the data as appropriate (by place, date)  
2) Create a week column using `floor_date()` from package **lubridate**  
    + This function returns the start-date of a given date's week, using a specified start date of each week (e.g. "Mondays")  
3) The data are grouped by columns "location" and "week" to create analysis units of "facility-week"  
4) The function `summarise()` creates new columns to reflecting summary statistics per facility-week group:  
    + Number of days per week (7 - a static value)  
    + Number of reports received from the facility-week (could be more than 7!)  
    + Sum of malaria cases reported by the facility-week (just for interest)  
    + Number of *unique* days in the facility-week for which there is data reported  
    + **Percent of the 7 days per facility-week for which data was reported**  
5) The dataframe is joined (`right_join()`) to a comprehensive list of all possible facility-week combinations, to make the dataset complete. The matrix of all possible combinations is created by applying `expand()` to those two columns of the dataframe as it is at that moment in the pipe chain (represented by "."). Because a `right_join()` is used, all rows in the `expand()` dataframe are kept, and added to agg_weeks if necessary. These new rows appear with `NA` (missing) summarized values.  


Below we demonstrate step-by-step:  

```{r, message=FALSE, warning=FALSE}
# Create weekly summary dataset
agg_weeks <- facility_count_data %>% 
  
  # filter the data as appropriate
  filter(
    District == "Spring",
    data_date < as.Date("2019-06-01")) 
```

Now the dataset has `r nrow(agg_weeks)` rows, when it previously had `r nrow(facility_count_data) `  

Next we create a `week` column reflecting the start date of the week for each record. This is achieved with the **lubridate** package and the function `floor_date()`, which is set to "week" and for the weeks to begin on Mondays (day 1 of the week - Sundays would be 7). The top rows are shown below.  

```{r}
agg_weeks <- agg_weeks %>% 
  # Create week column from data_date
  mutate(
    week = lubridate::floor_date(                     # create new column of weeks
      data_date,                                      # date column
      unit = "week",                                  # give start of the week
      week_start = 1))                                # weeks to start on Mondays 
```

The new week column can be seen on the far right of the dataframe  

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Now we group the data into facility-weeks and summarise them to produce statistics per facility-week. See the page on [Descriptive tables] for tips. The grouping itself doesn't change the dataframe, but it impacts how the subsequent summary statistics are calculated.  

The top rows are shown below. Note how the columns have completely changed to reflect the desired summary statistics. Each row reflects one facility-week. 

```{r, warning=F, message=F}
agg_weeks <- agg_weeks %>%   

  # Group into facility-weeks
  group_by(
    location_name, week) %>%
  
  # Create summary statistics columns on the grouped data
  summarize(
    n_days          = 7,                                          # 7 days per week           
    n_reports       = dplyr::n(),                                 # number of reports received per week (could be >7)
    malaria_tot     = sum(malaria_tot, na.rm = T),                # total malaria cases reported
    n_days_reported = length(unique(data_date)),                  # number of unique days reporting per week
    p_days_reported = round(100*(n_days_reported / n_days)))      # percent of days reporting
```

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```
Finally, we run the command below to ensure that ALL possible facility-weeks are present in the data, even if they were missing before.  

We are using a `right_join()` on itself (the dataset is represented by ".") but having been expanded to include all possible combinations of the columns `week` and `location_name`. See documentation on the `expand()` function in the page on [Pivoting]. Before running this code the dataset contains `r nrow(agg_weeks)` rows.   

```{r, message=F, warning=F}
# Create dataframe of every possible facility-week
expanded_weeks <- agg_weeks %>% 
  mutate(week = as.factor(week)) %>%         # convert date to a factor so expand() works correctly
  tidyr::expand(., week, location_name) %>%  # expand dataframe to include all possible facility-week combinations
                                             # note: "." represents the dataset at that moment in the pipe chain
  mutate(week = as.Date(week))               # re-convert week to class Date so the subsequent right_join works
                                             

# Use right-join with the expanded facility-week list to fill-in the missing gaps in the data
agg_weeks <- agg_weeks %>%      
  right_join(expanded_weeks) %>%                            # Ensure every possible facility-week combination appears in the data
  mutate(p_days_reported = replace_na(p_days_reported, 0))  # convert missing values to 0                           
```

Before running this code the dataset contains `r nrow(agg_weeks)` rows.   

<!-- ======================================================= -->
### Create heatmap {-}


The `ggplot()` is made using `geom_tile()` from the **ggplot2** package:  

* Weeks on the x-axis is transformed to dates, allowing use of `scale_x_date()`  
* location_name on the y-axis will show all facility names  
* The `fill` is the performance for that facility-week (numeric)  
* `scale_fill_gradient()` is used on the numeric fill, specifying colors for high, low, and `NA`  
* `scale_x_date()` is used on the x-axis specifying labels every 2 weeks and their format  
* Aesthetic themes and labels can be adjusted as necessary




<!-- ======================================================= -->
#### Basic {-}  

A basic heatmap is produced below,using the default colors, scales, etc. Within the `aes()` for `geom_tile()` you must provide an x-axis column, y-axis column, **and** a column for the the `fill = ` - these are the numeric values that are converted to tile color.  

```{r}
ggplot(data = agg_weeks)+
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported))
```

#### Cleaned plot {-}

We can make this plot look better by adding additional **ggplot2** functions, as shown below. See the page on [ggplot tips] for details.  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, April-May 2019",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
#### Ordered y-axis {-}  

Currently, the facilities are ordered "alphabetically" from the bottom to the top. If you want to adjust the order the y-axis facilities, convert them to class factor and provide the order. See the page on [Factors] for tips.  

Below, the column `location_name` is converted to a factor, and the order of its levels is set based on the total number of reporting days filed by the facility across the whole time-span.  

To do this, we create a dataframe which represents the total number of reports per facility, arranged in ascending order. We can use this vector to order the factor levels in the plot.   

```{r}
facility_order <- agg_weeks %>% 
  group_by(location_name) %>% 
  summarize(tot_reports = sum(n_days_reported, na.rm=T)) %>% 
  arrange(tot_reports) # ascending order
```

See the dataframe below:  

```{r, echo=F}
DT::datatable(facility_order, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```




Now use the above vector (`facility_order$location_name`) to be the order of the factor levels of location_name in the dataframe `agg_weeks`:  

```{r, warning=F, message=F}
# load package 
pacman::p_load(forcats)

# create factor and define levels manually
agg_weeks <- agg_weeks %>% 
  mutate(location_name = as_factor(location_name),
         location_name = fct_relevel(location_name, 
                                     levels = facility_order$location_name))
```

And now the data are re-plotted, with location_name being an ordered factor:  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, April-May 2019",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
#### Display values {-}  


You can add a `geom_text()` layer on top of the tiles, to display the actual numbers of each tile. Be aware this may not look pretty if you have many small tiles!  

The following code has been added: `geom_text(aes(label = p_days_reported))`. This adds text onto every tile. The text displayed is the value assigned to the argument `label = `, which in this case has been set to the same numeric column `p_days_reported` that is used to create the color gradient.  



  
```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  # text
  geom_text(
    aes(
      x = week,
      y = location_name,
      label = p_days_reported))+          # add text on top of tile
  
  # fill scale
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, April-May 2019",
       caption = "7-day weeks beginning on Mondays.")
```




<!-- ======================================================= -->
## Resources { }




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/heatmaps.Rmd-->


# Transmission chains { }


<!-- ======================================================= -->
## Overview {  }

The primary tool to handle, analyse and visualise transmission chains and contact
tracing data is the package **epicontacts**, developed by the folks at
RECON. Try out the interactive plot below by hovering over the nodes for more
information, dragging them to move them and clicking on them to highlight downstream cases.

```{r out.width=c('25%', '25%'), fig.show='hold', echo=F}

## install development version of epicontacts
if(
  !"epicontacts" %in% rownames(installed.packages()) |
  packageVersion("epicontacts") != "1.2.0"
) remotes::install_github("reconhub/epicontacts@timeline")

## install and load packages
pacman::p_load(tidyverse, epicontacts, magrittr, here, webshot, visNetwork)

## load linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds")) %>%
  filter(!duplicated(case_id))

## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(from)

## generate epicontacts
epic <- epicontacts::make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## subset object
epic %<>% subset(
  node_attribute = list(date_onset = c(as.Date(c("2014-06-01", "2014-07-01"))))
) %>%
  thin("contacts")

## plot with date of onset as x-axis
plot(
  epic,
  x_axis = "date_onset",
  label = FALSE,
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  node_shape = "gender",
  shapes = c(f = "female", m = "male"),
  unlinked_pos = "bottom",
  date_labels = "%b %d %Y",
  node_size = 35,
  font_size = 20,
  arrow_size = 0.5,
  height = 800,
  width = 700,
  edge_linetype = "location",
  legend_width = 0.15,
  highlight_downstream = TRUE,
  selector = FALSE
)

```

<!-- ======================================================= -->
## Preparation {  }

### Load packages {-}  

First load the standard packages required for data import and manipulation. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  
 
	
```{r transmission_chains_packages, eval = FALSE}
pacman::p_load(
   rio,          # File import
   here,         # File locator
   tidyverse,    # Data management + ggplot2 graphics
   remotes       # Package installation from github
)
```
	
You will require the development version of **epicontacts**, which can be
installed from github using the **remotes** package. You only need to run the code
below once, not every time you use the package.

```{r transmission_chains_epicontacts_install, eval = FALSE}
remotes::install_github("reconhub/epicontacts@timeline")
```


### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below. Of particular interest are the columns `case_id`, `generation`, `infector`, and `source`.  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Creating an epicontacts object

We then need to create an **epicontacts** object, which requires two types of
data:

* a linelist documenting cases where columns are variables and rows correspond to unique cases
* a list of edges defining links between cases on the basis of their unique IDs (these can be contacts,
  transmission events, etc.)

As we already have a linelist, we just need to create a list of edges between
cases, more specifically between their IDs. We can extract transmission links from the
linelist by linking the `infector` column with the `case_id` column. At this point we can also add "edge
properties", by which we mean any variable describing the link between the two
cases, not the cases themselves. For illustration, we will add a `location`
variable describing the location of the transmission event, and a duration
variable describing the duration of the contact in days.

In the code below, the **dplyr** function `transmute` is similar to `mutate`, except it only keeps
the columns we have specified within the function. The `drop_na` function will
filter out any rows where the specified columns have an `NA` value; in this
case, we only want to keep the rows where the infector is known.

```{r transmission_chains_create_contacts, eval=T}
## generate contacts
contacts <- linelist %>%
  transmute(
    infector = infector,
    case_id = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(infector)
```

We can now create the **epicontacts** object using the `make_epicontacts`
function. We need to specify which column in the linelist points to the unique case
identifier, as well as which columns in the contacts point to the unique
identifiers of the cases involved in each link. These links are directional in
that infection is going _from_ the infector _to_ the case, so we need to specify
the `from` and `to` arguments accordingly. We therefore also set the `directed`
argument to `TRUE`, which will affect future operations.

```{r transmission_chains_create_epicontacts, eval=T}
## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts,
  id = "case_id",
  from = "infector",
  to = "case_id",
  directed = TRUE
)
```
Upon examining the **epicontacts** objects, we can see that the `case_id` column
in the linelist has been renamed to `id` and the `case_id` and `infector`
columns in the contacts have been renamed to `from` and `to`. This ensures
consistency in subsequent handling, visualisation and analysis operations.

```{r transmission_chains_view_epicontacts, eval=T}
## view epicontacts object
epic
```

<!-- ======================================================= -->
## Handling {  }

### Subsetting

The `subset()` method for `epicontacts` objects allows for, among other things,
filtering of networks based on properties of the linelist ("node attributes") and the contacts
database ("edge attributes"). These values must be passed as named lists to the
respective argument. For example, in the code below we are keeping only the
male cases in the linelist that have an infection date between April and
July 2014 (dates are specified as ranges), and transmission links that occured
in the hospital.

```{r transmission_chains_subset_nodes, eval=T}
sub_attributes <- subset(
  epic,
  node_attribute = list(
    gender = "m",
    date_infection = as.Date(c("2014-04-01", "2014-07-01"))
  ), 
  edge_attribute = list(location = "Nosocomial")
)
sub_attributes
```

We can use the `thin` function to either filter the linelist to include cases
that are found in the contacts by setting the argument `what = "linelist"`, or
filter the contacts to include cases that are found in the linelist by setting
the  argument `what = "contacts"`. In the code below, we are further filtering the
epicontacts object to keep only the transmission links involving the male cases
infected between April and July which we had filtered for above. We can see that
only two known transmission links fit that specification.

```{r transmission_chains_thin, eval=T}
sub_attributes <- thin(sub_attributes, what = "contacts")
nrow(sub_attributes$contacts)
```

In addition to subsetting by node and edge attributes, networks can be pruned to
only include components that are connected to certain nodes. The `cluster_id`
argument takes a vector of case IDs and returns the linelist of individuals that
are linked, directly or indirectly, to those IDs. In the code below, we can see
that a total of 13 linelist cases are involved in the clusters containing
`2ae019` and `71577a`.

```{r}
sub_id <- subset(epic, cluster_id = c("2ae019","71577a"))
nrow(sub_id$linelist)
```

The `subset()` method for `epicontacts` objects also allows filtering by cluster
size using the `cs`, `cs_min` and `cs_max` arguments. In the code below, we are
keeping only the cases linked to clusters of 10 cases or larger, and can see that
271 linelist cases are involved in such clusters.
    
```{r}   
sub_cs <- subset(epic, cs_min = 10)
nrow(sub_cs$linelist)
```

### Accessing IDs

The `get_id()` function retrieves information on case IDs in the
dataset, and can be parameterized as follows:

- **linelist**: IDs in the line list data
- **contacts**: IDs in the contact dataset ("from" and "to" combined)
- **from**: IDs in the "from" column of contact datset
- **to** IDs in the "to" column of contact dataset
- **all**: IDs that appear anywhere in either dataset
- **common**: IDs that appear in both contacts dataset and line list
    
For example, what are the first ten IDs in the contacts dataset?
```{r transmission_chains_get_ids, eval=T}
contacts_ids <- get_id(epic, "contacts")
head(contacts_ids, n = 10)
```

How many IDs are found in both the linelist and the contacts?
```{r transmission_chains_get_both, eval=T}
length(get_id(epic, "common"))
```

<!-- ======================================================= -->
## Visualization {  }

### Basic plotting

All visualisations of **epicontacts** objects are handled by the `plot`
function. We will first filter the **epicontacts** object to include only the
cases with onset dates in June 2014 using the `subset` function, and only
include the contacts linked to those cases using the `thin` function.
	
```{r transmission_chains_basic_plot_sub, eval=T}
## subset epicontacts object
sub <- epic %>%
  subset(
    node_attribute = list(date_onset = c(as.Date(c("2014-06-30", "2014-06-01"))))
  ) %>%
 thin("contacts")
```

We can then create the basic, interactive plot very simply as follows:

```{r transmission_chains_basic_plot, eval=T}
## plot epicontacts object
plot(
  sub,
  width = 700,
  height = 700
)
```

You can move the nodes around by dragging them, hover over them for more
information and click on them to highlight connected cases.

There are a large number of arguments to further modify this plot. We will cover
the main ones here, but check out the documentation via `?vis_epicontacts` (the
function called when using `plot` on an **epicontacts** object) to get a full
description of the function arguments.

#### Visualising node attributes

Node color, node shape and node size can be mapped to a given column in the linelist 
using the `node_color`, `node_shape` and `node_size` arguments. This is similar
to the `aes` syntax you may recognise from **ggplot2**. 

The specific colors, shapes and sizes of nodes can be specified as follows:

* **Colors** via the `col_pal` argument, either by providing a name list for manual
specification of each color as done below, or by providing a color palette
function such as `colorRampPalette(c("black", "red", "orange"))`, which would
provide a gradient of colours between the ones specified.

* **Shapes** by passing a named list to the `shapes` argument, specifying one shape
  for each unique element in the linelist column specified by the `node_shape`
  argument. See `codeawesome` for available shapes.

* **Size** by passing a size range of the nodes to the `size_range` argument.

Here an example, where color represents the outcome, shape the gender and size
the age:

```{r transmission_chains_node_attribute, eval=T}
plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = 'age',
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  height = 700,
  width = 700
)
```

#### Visualising edge attributes

Edge color, width and linetype can be mapped to a given column in the contacts
dataframe using the `edge_color`, `edge_width` and `edge_linetype`
arguments. The specific colors and widths of the edges can be specified as follows:

* **Colors** via the `edge_col_pal` argument, in the same manner used for `col_pal`.

* **Widths** by passing a size range of the nodes to the `width_range` argument.

Here an example:

```{r transmission_chains_edge_attribute, eval=T}

plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = 'age',
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  edge_color = 'location',
  edge_linetype = 'location',
  edge_width = 'duration',
  edge_col_pal = c(Community = "orange", Nosocomial = "purple"),
  width_range = c(1, 3),
  height = 700,
  width = 700
)

```

### Temporal axis

We can also visualise the network along a temporal axis by mapping the `x_axis`
argument to a column in the linelist. In the example below, the x-axis
represents the date of symptom onset. We have also specified the `arrow_size`
argument to ensure the arrows are not too large, and set `label = FALSE` to make
 the figure less cluttered.

```{r transmission_chains_x_axis, eval=T}
plot(
  sub,
  x_axis = "date_onset",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

There are a large number of additional arguments to futher specify how this
network is visualised along a temporal axis, which you can check out
via `?vis_temporal_interactive` (the function called when using `plot` on
an **epicontacts** object with `x_axis` specified). We'll go through some
below.

#### Specifying transmission tree shape

There are two main shapes that the transmission tree can assume, specified using
the `network_shape` argument. The first is a `branching` shape as shown above,
where a straight edge connects any two nodes. This is the most intuitive
representation, however can result in overlapping edges in a densely connected
network. The second shape is `rectangle`, which produces a tree resembling a
phylogeny. For example:

```{r transmission_chains_rectangle, eval=T}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

Each case node can be assigned a unique vertical position by toggling the
`position_dodge` argument. The position of unconnected cases (i.e. with no
reported contacts) is specified using the `unlinked_pos` argument.

```{r transmission_chains_dodge, eval=T}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  position_dodge = TRUE,
  unlinked_pos = "bottom",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

The position of the parent node relative to the children nodes can be
specified using the `parent_pos` argument. The default option is to place the
parent node in the middle, however it can be placed at the bottom (`parent_pos =
'bottom'`) or at the top (`parent_pos = 'top'`).

```{r transmission_chains_parent_pos, eval=T}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

#### Saving plots and figures

You can save a plot as an interactive, self-contained html file with the
`visSave` function from the **VisNetwork** package:

```{r transmission_chains_save, eval=F}

plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
) %>%
  visNetwork::visSave("network.html")

```

Saving these network outputs as an image is unfortunately less easy and requires
you to save the file as an html and then take a screenshot of this file using
the `webshot` package. In the code below, we are converting the html file saved
above into a PNG:

```{r transmission_chains_webshot, eval=F}
webshot(url = "network.html", file = "network.png")
```

### Timelines

You can also case timelines to the network, which are represented on the x-axis
of each case. This can be used to visualise case locations, for example, or time
to outcome. To generate a timeline, we need to create a data.frame of at least
three columns indicating the case ID, the start date of the "event" and the end
of date of the "event". You can also add any number of other columns which can
then be mapped to node and edge properties of the timeline. In the code below,
we generate a timeline going from the date of symptom onset to the date of
outcome, and keep the outcome and hospital variables which we use to define the
node shape and colour. Note that you can have more than one timeline row/event
per case, for example if a case is transferred between multiple hospitals.

```{r transmission_chains_create_timeline, eval=T}

## generate timeline
timeline <- linelist %>%
  transmute(
    id = case_id,
    start = date_onset,
    end = date_outcome,
    outcome = outcome,
    hospital = hospital
  )

```

We then pass the timeline element to the `timeline` argument. We can map
timeline attributes to timeline node colours, shapes and sizes in the same way
defined in previous sections, except that we have _two_ nodes: the start and end
node of each timeline, which have seperate arguments. For example,
`tl_start_node_color` defines which timeline column is mapped to the colour of
the start node, while `tl_end_node_shape` defines which timeline column is
mapped to the shape of the end node. We can also map colour, width, linetype and
labels to the timeline _edge_ via the `tl_edge_*` arguments. 

See `?vis_temporal_interactive` (the function called when plotting an
epicontacts object) for detailed documentation on the arguments. Each argument
is annotated in the code below too:

```{r transmission_chains_vis_timeline, eval=T}

## define shapes
shapes <- c(
  f = "female",
  m = "male",
  Death = "user-times",
  Recover = "heartbeat",
  "NA" = "question-circle"
)

## define colours
colours <- c(
  Death = "firebrick",
  Recover = "green",
  "NA" = "grey"
)

## make plot
plot(
  sub,
  ## max x coordinate to date of onset
  x_axis = "date_onset",
  ## use rectangular network shape
  network_shape = "rectangle",
  ## mape case node shapes to gender column
  node_shape = "gender",
  ## we don't want to map node colour to any columns - this is important as the
  ## default value is to map to node id, which will mess up the colour scheme
  node_color = NULL,
  ## set case node size to 30 (as this is not a character, node_size is not
  ## mapped to a column but instead interpreted as the actual node size)
  node_size = 30,
  ## set transmission link width to 4 (as this is not a character, edge_width is
  ## not mapped to a column but instead interpreted as the actual edge width)
  edge_width = 4,
  ## provide the timeline object
  timeline = timeline,
  ## map the shape of the end node to the outcome column in the timeline object
  tl_end_node_shape = "outcome",
  ## set the size of the end node to 15 (as this is not a character, this
  ## argument is not mapped to a column but instead interpreted as the actual
  ## node size)
  tl_end_node_size = 15,
  ## map the colour of the timeline edge to the hospital column
  tl_edge_color = "hospital",
  ## set the width of the timeline edge to 2 (as this is not a character, this
  ## argument is not mapped to a column but instead interpreted as the actual
  ## edge width)
  tl_edge_width = 2,
  ## map edge labels to the hospital variable
  tl_edge_label = "hospital",
  ## specify the shape for everyone node attribute (defined above)
  shapes = shapes,
  ## specify the colour palette (defined above)
  col_pal = colours,
  ## set the size of the arrow to 0.5
  arrow_size = 0.5,
  ## use two columns in the legend
  legend_ncol = 2,
  ## set font size
  font_size = 15,
  ## define formatting for dates
  date_labels = c("%d %b %Y"),
  ## don't plot the ID labels below nodes
  label = FALSE,
  ## specify height
  height = 1000,
  ## specify width
  width = 1200,
  ## ensure each case node has a unique y-coordinate - this is very important
  ## when using timelines, otherwise you will have overlapping timelines from
  ## different cases
  position_dodge = TRUE
)

```

<!-- ======================================================= -->
## Analysis {  }

### Summarising

We can get an overview of some of the network properties using the
`summary` function.

```{r transmission_chains_summarise_epicontacts, eval=T}
## summarise epicontacts object
summary(epic)
```

For example, we can see that only 57% of contacts have both cases in the
linelist; this means that the we do not have linelist data on a significant
number of cases involved in these transmission chains.

### Pairwise characteristics

The `get_pairwise()` function allows processing of variable(s) in the line list
according to each pair in the contact dataset. For the following example, date
of onset of disease is extracted from the line list in order to compute the
difference between disease date of onset for each pair. The value that is
produced from this comparison represents the **serial interval (si)**.

```{r transmission_chains_pairwise, eval=T}
si <- get_pairwise(epic, "date_onset")   
summary(si)
tibble(si = si) %>%
  ggplot(aes(si)) +
  geom_histogram() +
  labs(
    x = "Serial interval",
    y = "Frequency"
  )
```

The `get_pairwise()` will interpret the class of the column being used for
comparison, and will adjust its method of comparing the values accordingly. For
numbers and dates (like the **si** example above), the function will subtract
the values. When applied to columns that are characters or categorical,
`get_pairwise()` will paste values together. Because the function also allows
for arbitrary processing (see "f" argument), these discrete combinations can be
easily tabulated and analyzed.
    
```{r transmission_chains_pairwise_2, eval=T}
head(get_pairwise(epic, "gender"), n = 10)
get_pairwise(epic, "gender", f = table)
fisher.test(get_pairwise(epic, "gender", f = table))
```

Here, we see a significant association between transmission links and gender.

### Identifying clusters

The `get_clusters()` function can be used for to identify connected components
in an `epicontacts` object. First, we use it to retrieve a `data.frame`
containing the cluster information:

```{r transmission_chains_cluster, eval=T}
clust <- get_clusters(epic, output = "data.frame")
table(clust$cluster_size)
ggplot(clust, aes(cluster_size)) +
  geom_bar() +
  labs(
    x = "Cluster size",
    y = "Frequency"
  )
```

Let us look at the largest clusters. For this, we add cluster information to the
`epicontacts` object and then subset it to keep only the largest clusters:

```{r transmission_chains_cluster_2, eval=T}
epic <- get_clusters(epic)
max_size <- max(epic$linelist$cluster_size)
plot(subset(epic, cs = max_size))
```

### Calculating degrees

The degree of a node corresponds to its number of edges or connections to other
nodes. `get_degree()` provides an easy method for calculating this value for
`epicontacts` networks. A high degree in this context indicates an individual
who was in contact with many others. The `type` argument indicates that we want
to count both the in-degree and out-degree, the `only_linelist` argument
indicates that we only want to calculate the degree for cases in the linelist.

```{r transmission_chains_degree, eval=T}
deg_both <- get_degree(epic, type = "both", only_linelist = TRUE)
```

Which individuals have the ten most contacts?

```{r}
head(sort(deg_both, decreasing = TRUE), 10)
```

What is the mean number of contacts?

```{r}
mean(deg_both)
```

<!-- ======================================================= -->
## Resources {  }

The
[epicontacts page](https://www.repidemicsconsortium.org/epicontacts/index.html)
provides an overview of the package functions and includes some more in-depth
vignettes.

The [github page](http://github.com/reconhub/epicontacts) can be used to raise
issues and request features.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/transmission_chains.Rmd-->


# Phylogenetic trees { }  



**Phylogenetic trees** are used to visualize and describe the relatedness and evolution of organisms based on the sequence of their genetic code. They can be constructed from genetic sequences using distance-based methods (such as neighbor-joining method) or character-based methods (such as maximum likelihood and Bayesian Markov Chain Monte Carlo method). Next-generation sequencing (NGS) has become more affordable and is becoming more widely used in public health to describe pathogens causing infectious diseases. Portable devices decrease the turn around time and make data available for the support of outbreak investigation in real-time. NGS data can be used to identify the origin or source of an outbreak strain and its propagation, as well as determine presence of antimicrobial resistance genes. To visualize the genetic relatedness between samples a phylogenetic tree is constructed. In this page we will learn how to use the **ggtree()** package, which allows for combination of phylogenetic trees with additional sample data in form of a dataframe in order to help observe patterns and improve understanding of the outbreak dynamic.

```{r, phylogenetic_trees_overview_graph, out.width=c('80%'), fig.align='center', fig.show='hold', echo = FALSE, warning=F, message=F}

pacman::p_load(here, ggplot2, dplyr, ape, ggtree, treeio, ggnewscale)

tree <- ape::read.tree(here::here("data", "Shigella_tree.nwk"))

sample_data <- read.csv(here::here("data","sample_data_Shigella_tree.csv"),sep=",", na.strings=c("NA"), head = TRUE, stringsAsFactors=F)


ggtree(tree, layout="circular", branch.length='none') %<+% sample_data + # the %<+% is used to add your dataframe with sample data to the tree
  aes(color=I(Belgium))+ # color the branches according to a variable in your dataframe
  scale_color_manual(name = "Sample Origin", # name of your color scheme (will show up in the legend like this)
                     breaks = c("Yes", "No"), # the different options in your variable
                     labels = c("NRCSS Belgium", "Other"), # how you want the different options named in your legend, allows for formatting
                     values= c("blue", "grey"), # the color you want to assign to the variable if its "nrc_bel"
                     na.value="grey")+ # for the NA values we choose the color grey
  new_scale_color()+ # allows to add an additional color scheme for another variable
     geom_tippoint(aes(color=Continent), size=1.5)+ # color the tip point by continent, you may change shape adding "shape = "
scale_color_brewer(name = "Continent",  # name of your color scheme (will show up in the legend like this)
                       palette="Set1", # we choose a premade set of colors coming with the brewer package
                   na.value="grey")+ # for the NA values we choose the color grey
  theme(legend.position= "none")
```

<!-- ======================================================= -->

## Preparation {  }

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, phylogenetic_trees_loading_packages}
 
# load/install packages
pacman::p_load(here, ggplot2, dplyr, ape, ggtree, treeio, ggnewscale)

```

There are several different formats in which a phylogenetic tree can be stored (eg. Newick, NEXUS, Phylip). A common one, which we will also use here in this example is the Newick file format (.nwk), which is the standard for representing trees in computer-readable form. Which means, an entire tree can be expressed in a string format such as  "((t2:0.04,t1:0.34):0.89,(t5:0.37,(t4:0.03,t3:0.67):0.9):0.59); " listing all nodes and tips and their relationship  (branch length) to each other. 

It is important to understand that the phylogenetic tree file in itself does not contain sequencing data, but is merely the result of the distances between the sequences. We therefore cannot extract sequencing data from a tree file.

We use the **ape()** package to import a phylogenetic tree file and store it in a list object of class "phylo". We inspect our tree object and see it contains 299 tips (or samples) and 236 nodes. 

```{r, phylogenetic_trees_loading_treefile, warning=F, message=F}

# read in the tree: we use the here package to specify the location of our R project and data files:
tree <- ape::read.tree(here::here("data", "Shigella_tree.nwk"))

# inspect the tree file:
tree

```

Second we import a table with additional information for each sequenced sample such as gender, country of origin and attributes for antimicrobial resistance: 

```{r, echo=F}

# We read in a csv file into a dataframe format:
sample_data <- read.csv(here::here("data","sample_data_Shigella_tree.csv"),sep=",", na.strings=c("NA"), head = TRUE, stringsAsFactors=F)

```

```{r, eval=F}

# We read in a csv file into a dataframe format:
sample_data <- read.csv("sample_data_Shigella_tree.csv", sep = ",", na.strings = c("NA"), head = TRUE, stringsAsFactors=F)

```

Below are the first 30 rows of these data:  

```{r, echo=F}
DT::datatable(head(sample_data,30), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T))
```


We clean and inspect our data: In order to assign the correct sample data to the phylogenetic tree, the Sample_IDs in the sample_data file need to match the tip.labels in the tree file: 
```{r, phylogenetic_trees_inspect_sampledata, warning=F, message=F}

# We clean the data: we select certain columns to be protected from cleaning in order to main tain their formating (eg. for the sample names, as they have to match the names in the phylogenetic tree file)
#sample_data <- linelist::clean_data(sample_data, protect = c(1, 3:5)) 

# We check the formatting of the tip labels in the tree file: 

head(tree$tip.label) # these are the sample names in the tree - we inspect the first 6 with head()

# We make sure the first column in our dataframe are the Sample_IDs:
colnames(sample_data)   

# We look at the sample_IDs in the dataframe to make sure the formatting is the same than in the tip.labels (eg. letters are all capital, no extra _ between letters and numbers etc.)
head(sample_data$Sample_ID) # we inspect only the first 6 using head()

```

Upon inspection we can see that the format of sample_ID in the dataframe corresponds to the format of sample names at the tree tips. These do not have to be sorted in the same order to be matched.

We are ready to go!

<!-- ======================================================= -->

## Simple tree visualization {  }

### Different tree layouts {-}  

**ggtree()** offers many different layout formats and some may be more suitable for your specific purpose than others:
```{r, phylogenetic_trees_example_formats, out.width=c('50%'), fig.show='hold'}
# Examples:
ggtree(tree) # most simple linear tree
ggtree(tree,  branch.length = "none") # most simple linear tree with all tips aligned
ggtree(tree, layout="circular") # most simple circular tree
ggtree(tree, layout="circular", branch.length = "none") # most simple circular tree with all tips aligned

# for other options see online: http://yulab-smu.top/treedata-book/chapter4.html

```

### Simple tree with addition of sample data {-}  

The most easy annotation of your tree is the addition of the sample names at the tips, as well as coloring of tip points and if desired branches:

```{r, phylogenetic_trees_adding_sampledata, out.width=c('120%'),fig.align='center', message=FALSE, warning=F, message=F}

# A: Plot Circular tree:
ggtree(tree, layout="circular", branch.length='none') %<+% sample_data + # the %<+% is used to add your dataframe with sample data to the tree
  aes(color=I(Belgium))+     # color the branches according to a variable in your dataframe
  scale_color_manual(
    name = "Sample Origin",  # name of your color scheme (will show up in the legend like this)
    breaks = c("Yes", "No"), # the different options in your variable
    labels = c("NRCSS Belgium", "Other"), # how you want the different options named in your legend, allows for formatting
    values= c("blue", "grey"),            # the color you want to assign to the variable if its "nrc_bel"
    na.value="grey")+     # for the NA values we choose the color grey
  new_scale_color()+      # allows to add an additional color scheme for another variable
     geom_tippoint(       # color the tip point by continent, you may change shape adding "shape = "
       aes(color=Continent),
       size=1.5)+ 
  scale_color_brewer(
    name = "Continent",   # name of your color scheme (will show up in the legend like this)
    palette="Set1",       # we choose a premade set of colors coming with the brewer package
    na.value="grey")+     # for the NA values we choose the color grey
  geom_tiplab(            # add the name of the sample to the tip of its branch (you can add as many text lines as you like with the + , you just need to change the offset value to place them next to each other)
    color='black',
    offset = 1,
    size = 1,
    geom = "text",
    align=TRUE)+ 
  ggtitle("Phylogenetic tree of Shigella sonnei")+ # title of your graph
  theme(
    axis.title.x=element_blank(), # removes x-axis title
    axis.title.y=element_blank(), # removes y-axis title
    legend.title=element_text(face="bold", size =12),  # defines font size and format of the legend title
    legend.text=element_text(face="bold", size =10),   # defines font size and format of the legend text
    plot.title = element_text(size =12, face="bold"),  # defines font size and format of the plot title
    legend.position="bottom", # defines placement of the legend
    legend.box="vertical",
    legend.margin=margin())   # defines placement of the legend

# Export your tree graph:
ggsave(here::here("images", "example_tree_circular_1.png"), width = 12, height = 14)

```


<!-- ======================================================= -->

## Manipulation of phylogenetic trees {  }

Sometimes you may have a very large phylogenetic tree and you are only interested in one part of the tree. For example if you produced a tree including historical or international samples to get a large overview of where your dataset might fit in in the bigger picture. But then to look closer at your data you want to inspect only that portion of the bigger tree.

Since the phylogenetic tree file is just the output of sequencing data analysis, we can not manipulate the order of the nodes and branches in the file itself. These have already been determined in previous analysis from the raw NGS data. We are able though to zoom into parts, hide parts and seven subset part of the tree. 

### Zooming in on one part of the tree:
If you don't want to "cut" your tree, but only inspect part of it more closely you can zoom in to view a specific part:

```{r, phylogenetic_trees_zoom_in, out.width=c('70%'), fig.show='hold', fig.align='center'}

# First we plot the whole tree:
p <- ggtree(tree,) %<+% sample_data +
  geom_tiplab(size =1.5) + # labels the tips of all branche with the sample name in the tree file
  geom_text2(aes(subset=!isTip, label=node), size =5, color = "darkred", hjust=1, vjust =1) # labels all the nodes in the tree
p

```

We want to zoom into the branch which is sticking out, after node number 452 to get a closer look:
```{r phylogenetic_trees_zoom_in_452, out.width=c('70%'), fig.show='hold', fig.align='center'}

viewClade(p , node=452)

```

### Collapsing one part of the tree:
The other way around we may want to ignore this branch which is sticking out and can do so by collapsing it at the node (indicated here by the blue square):

```{r phylogenetic_trees_collapse_452, out.width=c('70%'), fig.show='hold', fig.align='center'}
#First we collapse at node 452
p_collapsed <- collapse(p, node=452)

#To not forget that we collapsed this node we assign a symbol to it:
p_collapsed + geom_point2(aes(subset=(node == 452)), size=5, shape=23, fill="steelblue")

```

### Subsetting a tree {-}  

If we want to make a more permanent change and create a new tree to work with we can subset part of it and even save it as new newick tree file.  

```{r, phylogenetic_trees_subsetting, out.width=c('70%'), fig.show='hold', fig.align='center', warning=F, message=F}

# To do so you can add the node and tip labels to your tree to see which part you want to subset:
ggtree(tree, branch.length='none', layout='circular') %<+% sample_data +
  geom_tiplab(size =1) +       # labels the tips of all branche with the sample name in the tree file
  geom_text2(                  # labels all the nodes in the tree
    aes(subset = !isTip, label=node),
    size = 3,
    color = "darkred")+  
 theme(
   legend.position = "none", # removes the legend all together
   axis.title.x = element_blank(),
   axis.title.y=element_blank(),
   plot.title = element_text(size =12, face="bold"))

# A: Subset tree based on node:
sub_tree1 <- tree_subset(
  tree,
  node = 528) # we subset the tree at node 528

# lets have a look at the subset tree:
ggtree(sub_tree1)+
  geom_tiplab(size = 3) +
  ggtitle("Subset tree 1")

# B: Subset the same part of the tree based on a samplem in this case S17BD07692:
sub_tree2 <- tree_subset(
  tree,
  "S17BD07692",
  levels_back = 9) # levels back defines how many nodes backwards from the sample tip you want to go

# lets have a look at the subset tree:
ggtree(sub_tree2)+
  geom_tiplab(size = 3)+
  ggtitle("Subset tree 2")

```

You can also save your new tree as a Newick file:
```{r, eval=F, phylogenetic_trees_write_tree}

ape::write.tree(sub_tree2, file='data/Shigelle_subtree_2.nwk')

```

### Rotating nodes in a tree {-}  

As mentioned before we cannot change the order of tips or nodes in the tree, as this is based on their genetic relatedness and is not subject to visual manipulation. But we can rote branches around nodes if that eases our visualization.

First we plot our new subsetted tree with node labels to choose the node we want to manipulate:

```{r, phylogenetic_trees_rotating_1, out.width=c('70%'), fig.show='hold', fig.align='center', warning=F, message=F}

p <- ggtree(sub_tree2) +
  geom_tiplab(size = 4) +
  geom_text2(                       # label all the nodes in the tree
    aes(subset=!isTip, label=node),
    size = 5,
    color = "darkred",
    hjust = 1,
    vjust = 1) 
p
```

We choose to manipulate node number 39: we do so by applying **ggtree::rotate()** or **ggtree::flip()** indirectly to node 36 so node 39 moves to the bottom and nodes 37 and 38 move to the top:

```{r, phylogenetic_trees_rotating_2, out.width=c('70%'), fig.show='hold', fig.align='center'}
# 
# p1 <- p + geom_hilight(39, "steelblue", extend =0.0015)+ # highlights the node 39 in blue
#    geom_hilight(37, "yellow", extend =0.0015)  + # highlights the node 37 in yellow
#   ggtitle("Original tree")
# 
# # we want to rotate node 36 so node 39 is on the bottom and nodes 37 and 38 move to the top:
# # 
# rotate(p1, 39) %>% rotate(37)+
#   ggtitle("Rotated Node 36")
# 
# # #or we can use the flip command to achieve the same thing:
# flip(p1, 39, 37)

```

### Example subtree with sample data annotation:

Lets say we are investigating the cluster of cases with clonal expansion which occured in 2017 and 2018 at node 39 in our sub-tree. We add the year of strain isolation as well as travel history and color by country to see origin of other closely related strains:

```{r, phylogenetic_trees_inspect_subset_example, out.width=c('80%'), fig.show='hold', fig.align='center', message = FALSE, warning=F, message=F}

# Add sample data:
ggtree(sub_tree2) %<+% sample_data + 
  geom_tiplab(                      # labels the tips of all branches with the sample name in the tree file
    size =2.5,
    offset = 0.001, 
    align = TRUE) + 
  theme_tree2()+
  xlab("genetic distance")+ # add a label to the x-azis
  xlim(0, 0.015)+           # set the x-axis limits of our tree
  geom_tippoint(            # color the tip point by continent
    aes(color=Country),
    size=1.5)+ 
  scale_color_brewer(
    name = "Country", 
    palette="Set1", 
    na.value="grey")+
  geom_tiplab(              # add isolation year
    aes(label = Year),
    color='blue',
    offset = 0.0045,
    size = 3,
    linetype = "blank",
    geom = "text", 
    align=TRUE)+
  geom_tiplab(              # add travel history
    aes(label = Travel_history),
    color='red',
    offset = 0.006,
    size = 3,
    linetype = "blank",
    geom = "text",
    align=TRUE)+ 
  ggtitle("Phylogenetic tree of Belgian S. sonnei strains with travel history")+ # add plot title
  theme(
    axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    legend.title=element_text(face="bold", size =12),
    legend.text=element_text(face="bold", size =10),
    plot.title = element_text(size =12, face="bold"))

```

Our observation points towards an import of strains from Asia, which then circulated in Belgium over the years and seem to have caused our latest outbreak.

<!-- ======================================================= -->

## More complex trees: adding heatmaps of sample data {  }

We can add more complex information, such as categorical presence of antimicrobial resistance genes and numeric values for actually  measured resistance to antimicrobials in form of a heatmap using the **ggtree::gheatmap()** function.

First we need to plot our tree (this can be either linear or circular): We will use the sub_stree from part 3.)
```{r, phylogenetic_trees_sampledata_heatmap, out.width=c('60%'), fig.align='center', fig.show='hold', warning=F, message=F}
# A: Circular tree:
p <- ggtree(sub_tree2, branch.length='none', layout='circular') %<+% sample_data +
  geom_tiplab(size =3) + 
  theme(
    legend.position = "none",
    axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    plot.title = element_text(size =12, face="bold",hjust = 0.5, vjust = -15))

p

```

Second we prepare our data. To visualize different variables with new color schemes, we subset our dataframe to the desired variable.

For example we want to look at gender and mutations that could confer resistance to ciprofloxacin:

```{r, phylogenetic_trees_sampledata_heatmap_data}

# Create your gender dataframe:
gender <- data.frame("gender" = sample_data[,c("Gender")])

# Its important to add the Sample_ID as rownames otherwise it cannot match the data to the tree tip.labels:
rownames(gender) <- sample_data$Sample_ID

# Create your ciprofloxacin dataframe based on mutations in the gyrA gene:
cipR <- data.frame("cipR" = sample_data[,c("gyrA_mutations")])
rownames(cipR) <- sample_data$Sample_ID

# Create your ciprofloxacin dataframe based on the measured minimum inhibitory concentration (MIC) from the laboratory:
MIC_Cip <- data.frame("mic_cip" = sample_data[,c("MIC_CIP")])
rownames(MIC_Cip) <- sample_data$Sample_ID

```

We create a first plot adding a binary heatmap for gender to the phylogenetic tree:  

```{r, phylogenetic_trees_sampledata_heatmap_gender, out.width=c('70%'), fig.show='hold', fig.align='center', message = FALSE, warning=F, message=F}

# First we add gender:
h1 <-  gheatmap(
  p,
  gender,
  offset = 12,        # offset shifts the heatmap to the right
  width=0.10,         # width defines the width of the heatmap column
  color=NULL,         # color defines the border of the heatmap columns
  colnames = FALSE)+  # hides column names for the heatmap
  scale_fill_manual(  # define the coloring scheme and legend for gender
    name = "Gender", 
    values = c("#00d1b1", "purple"),
    breaks = c("Male", "Female"),
    labels = c("Male", "Female"))+
  theme(
    legend.position="bottom",
    legend.title = element_text(size=12),
    legend.text = element_text(size =10),
    legend.box="vertical", legend.margin=margin())
h1

```

Then we add information on ciprofloxacin resistance genes:  

```{r, phylogenetic_trees_sampledata_heatmap_cip_genes, out.width=c('70%'), fig.show='hold', fig.align='center', message = FALSE}

# First we assigng a new color scheme to our existing plot, this enables us to define and change the colors for our second variable
h2 <- h1 + new_scale_fill() 

# then we combine these into a new plot:
h3 <- gheatmap(
  h2,
  cipR,
  offset = 14,
  width=0.10, # adds the second row of heatmap describing ciprofloxacin resistance genes
  colnames = FALSE)+
  scale_fill_manual(
    name = "Ciprofloxacin resistance \n conferring mutation",
    values = c("#fe9698","#ea0c92"),
    breaks = c( "gyrA D87Y", "gyrA S83L"),
    labels = c( "gyrA d87y", "gyrA s83l"))+
  theme(
    legend.position="bottom",
    legend.title = element_text(size=12),
    legend.text = element_text(size =10),
    legend.box = "vertical",
    legend.margin = margin())+
  guides(
    fill = guide_legend(nrow=2,byrow=TRUE))

h3
```

Next we add continuous data on actual resistance determined by the laboratory 
as the minimum inhibitory concentration (MIC) of ciprofloxacin:  

```{r, phylogenetic_trees_sampledata_heatmap_cip_MIC, out.width=c('70%'), fig.show='hold', fig.align='center', message = FALSE, warning=F, message=F}
# First we add the new coloring scheme:
h4 <- h3 + new_scale_fill()

# then we combine the two into a new plot:
h5 <- gheatmap(
  h4,
  MIC_Cip,
  offset = 16,
  width=0.10,
  colnames = FALSE)+
  scale_fill_continuous(
    name = "MIC for ciprofloxacin",
    low = "yellow",
    high = "red",
    breaks = c(0, 0.50, 1.00),
    na.value = "white")+
  guides(
    fill = guide_colourbar(barwidth = 5, barheight = 1))+
  theme(
    legend.position="bottom",
    legend.title = element_text(size=12),
    legend.text = element_text(size =10),
    legend.box="vertical",
    legend.margin=margin())
h5

```

We can do the same exercise for a linear tree:  

```{r, phylogenetic_trees_sampledata_heatmap_linear, out.width=c('80%'), fig.show='hold', fig.align='center', message = FALSE, warning=F, message=F}
# B: Lineartree:
p <- ggtree(sub_tree2) %<+% sample_data +
  geom_tiplab(size =3) + # labels the tips
  theme_tree2()+
  xlab("genetic distance")+
  xlim(0, 0.015)+
 theme(
   legend.position = "none",
   axis.title.y=element_blank(),
   plot.title = element_text(size =12, face="bold",hjust = 0.5, vjust = -15))


# First we add gender:

h1 <- gheatmap(
  p, gender,
  offset = 0.003,
  width=0.1,
  color="black", 
  colnames = FALSE)+
  scale_fill_manual(
    name = "Gender",
    values = c("#00d1b1", "purple"),
    breaks = c("Male", "Female"),
    labels = c("Male", "Female"))+
   theme(
     legend.position="bottom",
     legend.title = element_text(size=12),
     legend.text = element_text(size =10),
     legend.box="vertical", legend.margin=margin())
# h1

# Then we add ciprofloxacin after adding another colorscheme layer:

h2 <- h1 + new_scale_fill()
h3 <- gheatmap(
  h2, cipR,
  offset = 0.004,
  width=0.1,
  color="black",
  colnames = FALSE)+
  scale_fill_manual(
    name = "Ciprofloxacin resistance \n conferring mutation",
    values = c("#fe9698","#ea0c92"),
    breaks = c( "gyrA D87Y", "gyrA S83L"),
    labels = c( "gyrA d87y", "gyrA s83l"))+
  theme(
    legend.position="bottom",
    legend.title = element_text(size=12),
    legend.text = element_text(size =10),
    legend.box="vertical",
    legend.margin=margin())+
  guides(
    fill=guide_legend(nrow=2,byrow=TRUE))
# h3

# Then we add the minimum inhibitory concentration determined by the lab (MIC):
h4 <- h3 + new_scale_fill()
h5 <- gheatmap(
  h4, MIC_Cip,
  offset = 0.005,
  width=0.1,
  color="black",
  colnames = FALSE)+
  scale_fill_continuous(
    name = "MIC for ciprofloxacin",
    low = "yellow",
    high = "red",
    breaks = c(0,0.50,1.00),
    na.value = "white")+
   guides(
     fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(
     legend.position="bottom",
     legend.title = element_text(size=10),
     legend.text = element_text(size =8),
     legend.box="horizontal", legend.margin=margin())+
  guides(
    shape = guide_legend(override.aes = list(size = 2)))
h5

```


<!-- ======================================================= -->
## Resources {  }

http://hydrodictyon.eeb.uconn.edu/eebedia/index.php/Ggtree# Clade_Colors
https://bioconductor.riken.jp/packages/3.2/bioc/vignettes/ggtree/inst/doc/treeManipulation.html
https://guangchuangyu.github.io/ggtree-book/chapter-ggtree.html
https://bioconductor.riken.jp/packages/3.8/bioc/vignettes/ggtree/inst/doc/treeManipulation.html


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/phylogenetic_trees.Rmd-->


# Interactive plots { }  

Data visualisation is increasingly required to be interrogable by the audience. Consequently, is is becoming common to create interactive plots. There are several ways to include these but the two most common are **plotly** and **shiny**. 

In this page we will focus on converting an existing `ggplot()` plot into an interactive plot with **plotly**. You can read more about **shiny** in the [Shiny and dashboards] page.  
Below is a basic epicurve that has been transformed to be interactive using the integration of **ggplot2** and **plotly** (hover your mouse over the plot). 

```{r plotly_demo, out.width=c('75%'), out.height=c('500px'), echo=F, warning=F, message=F}
pacman::p_load(plotly, rio, here, ggplot2, dplyr, lubridate)
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

## these buttons are superfluous/distracting
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- linelist %>% 
  mutate(outcome = if_else(is.na(outcome), "Unknown", outcome),
         date_earliest = if_else(is.na(date_infection), date_onset, date_infection),
         week_earliest = floor_date(date_earliest, unit = "week",week_start = 1))%>% 
  count(week_earliest, outcome) %>% 
  ggplot()+
  geom_col(aes(week_earliest, n, fill = outcome))+
  xlab("Week of infection/onset") + ylab("Cases per week")+
  theme_minimal()

p %>% 
  ggplotly() %>% 
  partial_bundle() %>% 
  config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)

```

<!-- ======================================================= -->
## Preparation {  }

### Load packages {-}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary and loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r}
pacman::p_load(
  rio,       # import/export
  here,      # filepaths
  lubridate, # working with dates
  plotly,    # interactive plots
  scales,    # quick percents
  tidyverse  # data management and visualization
  ) 
```

### Start with a `ggplot()` {-}  

In this page we assume that you are beginning with a `ggplot()` plot that you want to convert to be interactive. We will build several of these plots in this page, using the case `linelist` used in many pages of this handbook.  


### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Manipulate and add columns (best taught in the epicurves section).  

```{r manipulate_show, eval=F}
# linelist <- linelist %>% 
#   dplyr::mutate(
#     ## If the outcome column is NA, change to "Unknown"
#     outcome = dplyr::if_else(condition = is.na(outcome),
#                              true = "Unknown",
#                              false = outcome),
#     ## If the date of infection is NA, use the date of onset instead
#     date_earliest = dplyr::if_else(condition = is.na(date_infection),
#                                    true = date_onset,
#                                    false = date_infection),
#     ## Summarise earliest date to earliest week 
#     week_earliest = lubridate::floor_date(x = date_earliest,
#                                           unit = "week",
#                                           week_start = 1)
#     )
```
  
Count for plotting
```{r manipulate_show2, eval=F}
## Find number of cases in each week by their outcome
# linelist <- linelist %>% 
#   dplyr::count(week_earliest, outcome)
```


  
<!-- ======================================================= -->
## Plot with `ggplotly()` {  }

The function `ggplotly()` from the **plotly** package makes it easy to convert a `ggplot()` to be interactive. Simply save your `ggplot()` and then pipe it to the `ggplotly()` function.  


Below, we plot a simple line representing the proportion of cases who died in a given week:  

We begin by creating a summary dataset of each epidemiological week, and the percent of cases with a known outcome that died.  

```{r}
weekly_deaths <- linelist %>%
  group_by(epiweek = floor_date(date_onset, "week")) %>% 
  summarise(
    n_known_outcome = sum(!is.na(outcome), na.rm=T),
    n_death  = sum(outcome == "Death", na.rm=T),
    pct_death = 100*(n_death / n_known_outcome)
  )
```
Here is the first 50 rows of the `weekly_deaths` dataset.  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_deaths, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
Then we create the plot with **ggplot2**, using `geom_line()`.  

```{r, warning=F, message=F}
deaths_plot <- ggplot(data = weekly_deaths)+    # begin with weekly deaths data
  geom_line(                                    # create line
    mapping = aes(x = epiweek, y = pct_death),  # map epiweek to x-axis, and pct_death to y-axis
    stat = "identity")                          # line height reflects y value, not number of rows (default)

deaths_plot   # print
```


We can make this interactive by simply passing this plot to `ggplotly()`, as below. Hover your mouse over the line to show the x and y values. 

You can also see icons in the upper-right of the plot. In order, they allow you to:  

* Download the current view as a PNG image  
* Zoom in with a select box  
* "Pan", or move across the plot by clicking and dragging the plot  
* Zoom in, zoom out, or return to default zoom  
* Reset axes to defaults  
* Toggle on/off "spike lines" which are dotted lines from the interactive point extending to the x and y axes  
* Adjustments to whether data show when you are not hovering on the line  


```{r}
deaths_plot %>% plotly::ggplotly()
```

Grouped data work with plotly as well. Below, a weekly epicurve is made, grouped by outcome. The stacked bars are interactive. Try clicking on the different items in the legend (they will appear/disappear).  


```{r plot_show, eval=F}
# Make epidemic curve with incidence2 pacakge
p <- incidence2::incidence(
  linelist,
  date_index = date_onset,
  interval = "weeks",
  groups = outcome) %>% plot(fill = outcome)
```

```{r, echo=T, eval=F}
# Plot interactively  
p %>% ggplotly()
```
  
```{r, eval=TRUE, out.width=c('95%'), out.height=c('500px'), echo=FALSE}
p %>% 
  plotly::ggplotly() %>% 
  plotly::partial_bundle() 
```
  
<!-- ======================================================= -->
## Modifications {  }

### File size {-}  

When exporting in an Rmarkdown generated HTML (like this book!) you want to make the plot as small data size as possible (with no negative side effects in most cases). For this, just pipe the interactive plot to `partial_bundle()`, also from **plotly**.  

```{r plot_tidyshow, eval=F}
p <- p %>% 
  ggploty() %>% 
  partial_bundle()
```

### Buttons {-}  

Some of the buttons on a standard plotly are superfluous and can be distracting, so it's best to remove them. You can do this simply by piping the output into `config()` from **plotly** and specifying which buttons to remove. In the below example we specify in advance the names of the buttons to remove, and provide them to the argument `modeBarButtonsToRemove = `. We also set `displaylogo = FALSE` to remove the plotly logo.  

```{r plot_tidyshow2, eval=F}
## these buttons are distracting and we want to remove them
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- p %>%          # re-define interactive plot without these buttons
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```



<!-- ======================================================= -->
## Heat tiles {  }

You can make almost any `ggplot()` plot interactive, including heat tiles. In the page on [Heat tiles] you can read about how to make the below `ggplot()`, which displays the proportion of days per week that certain facilities reported data to their province.  

Here is the code, although we will not describe it in depth here.  

```{r eval=T, message=F, warning=F}
# import data
facility_count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# aggregate data into Weeks for Spring district
agg_weeks <- facility_count_data %>% 
  filter(District == "Spring",
         data_date < as.Date("2019-06-01")) %>% 
  mutate(week = aweek::date2week(
    data_date,
    start_date = "Monday",
    floor_day = TRUE,
    factor = TRUE)) %>% 
  group_by(location_name, week, .drop = F) %>%
  summarize(
    n_days          = 7,
    n_reports       = dplyr::n(),
    malaria_tot     = sum(malaria_tot, na.rm = T),
    n_days_reported = length(unique(data_date)),
    p_days_reported = round(100*(n_days_reported / n_days))) %>% 
  right_join(tidyr::expand(., week, location_name)) %>% 
  mutate(week = aweek::week2date(week))

# create plot
metrics_plot <- ggplot(agg_weeks,
       aes(x = week,
           y = location_name,
           fill = p_days_reported))+
  geom_tile(colour="white")+
  scale_fill_gradient(low = "orange", high = "darkgreen", na.value = "grey80")+
  scale_x_date(expand = c(0,0),
               date_breaks = "2 weeks",
               date_labels = "%d\n%b")+
  theme_minimal()+ 
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),
    legend.key.width  = grid::unit(0.6,"cm"),
    axis.text.x = element_text(size=12),
    axis.text.y = element_text(vjust=0.2),
    axis.ticks = element_line(size=0.4),
    axis.title = element_text(size=12, face="bold"),
    plot.title = element_text(hjust=0,size=14,face="bold"),
    plot.caption = element_text(hjust = 0, face = "italic")
    )+
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, April-May 2019",
       caption = "7-day weeks beginning on Mondays.")

metrics_plot # print
```

Below, we make it interactive and modify it for simple buttons and file size.  

```{r, eval=T, out.width=c('95%'), out.height=c('500px')}
metrics_plot %>% 
  ggplotly() %>% 
  partial_bundle() %>% 
  config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```

## Maps {-}  

You can also make `ggplot()` GIS maps interactive, although it makes a bit more care. 

THIS SECTION IS UNDER CONSTRUCTION 

Although **plotly** works well with `ggplot2::geom_sf` in RStudio, when you try to include its outputs in Rmarkdown HTML files (like this book), it doesn't work well.  
  
So instead you can use {**plotly**}'s own mapping tools which can be tricky but are easy when you know how. Read on...  
  
We're going to use Covid-19 incidence across African countries for this example. The data used can be found on the [World Health Organisation website](https://covid19.who.int/table).  
  
You'll also need a new type of file, a GeoJSON, which is sort of similar to a shp file for those familiar with GIS. For this book, we used one from [here](https://geojson-maps.ash.ms).  
  
GeoJSON files are stored in R as complex lists and you'll need to maipulate them a little.

```{r, echo=T, eval=T}
## You need two new packages: {rjson} and {purrr}
pacman::p_load(plotly, rjson, purrr)

## This is a simplified version of the WHO data
df <- rio::import(here::here("data", "covid_incidence.csv"))

## Load your geojson file
geoJSON <- rjson::fromJSON(file=here::here("data", "africa_countries.geo.json"))

## Here are some of the properties for each element of the object
head(geoJSON$features[[1]]$properties)

```


This is the tricky part. For {**plotly**} to match your incidence data to GeoJSON, the countries in the geoJSON need an id in a specific place in the list of lists. For this we need to build a basic function:
```{r}
## The property column we need to choose here is "sovereignt" as it is the names for each country
give_id <- function(x){
  
  x$id <- x$properties$sovereignt  ## Take sovereignt from properties and set it as the id
  
  return(x)
}

## Use {purrr} to apply this function to every element of the features list of the geoJSON object
geoJSON$features <- purrr::map(.x = geoJSON$features, give_id)
```

<!-- ======================================================= -->
### Maps - plot {  }

```{r, echo=T, eval=T, out.width=c('95%'), out.height=c('500px'),warning=F}
plotly::plot_ly() %>% 
  plotly::add_trace(                    #The main plot mapping functionn
    type="choropleth",
    geojson=geoJSON,
    locations=df$Name,          #The column with the names (must match id)
    z=df$Cumulative_incidence,  #The column with the incidence values
    zmin=0,
    zmax=57008,
    colorscale="Viridis",
    marker=list(line=list(width=0))
  ) %>%
  plotly::colorbar(title = "Cases per million") %>%
  plotly::layout(title = "Covid-19 cumulative incidence",
                 geo = list(scope = 'africa')) %>% 
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```

<!-- ======================================================= -->
## Resources {  }

Plotly is not just for R, but also works well with Python (and really any data science language as it's built in JavaScript). You can read more about it on the [plotly website](https://plotly.com/r/)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/interactive_plots.Rmd-->

# (PART) Advanced {-}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/cat_advanced.Rmd-->


# Directory interactions { }  

In this page we cover common scenarios where you interact with, save, and import with directories (folders).  


## Preparation  

### **fs** package {-}  

The **fs** package is a **tidyverse** package that facilitate directory interactions, improving on some of the **base** R functions. In the sections below we will often use functions from **fs**.  

```{r}
pacman::p_load(fs)
```


### Print directory as a dendrogram tree
Use the function `dir_tree()` from **fs**.  

Provide the folder filepath to `path = ` and decide whether you want to show only one level (`recurse = FALSE`) or all files in all sub-levels (`recurse = TRUE`). Below we use `here("data)` as shorthand for the R project and it's sub-folder "data", which contains all the data used for this R handbook. We use set it to display all files within "data" and its sub-folders (e.g. "cache", "epidemic models", "population", "shp", and "weather").  


```{r}
fs::dir_tree(path = here("data"), recurse = TRUE)
```


## Accessing files in the directory



## Running other files  

### `source()` {-}  

To run one R script from another R script, you can use the `source()` command (from **base** R).

```{r, eval=F}
source(here("scripts", "cleaning_scripts", "clean_testing_data.R"))
```

This is equivalent to viewing the above R script and clicking the "Source" button in the upper-right of the script. This will execute the script but will do it silently (no output to the R console) unless specifically intended. See the page on [Interactive console] for examples of using `source()` to interact with a user via the R console in question-and-answer mode.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "source_button.png"))
```


### `render()` {-}  

`render()` is a variation on `source()` most often used for R markdown scripts. You provide the `input = ` which is the R markdown file, and also the `output_format = ` (typically either "html_document", "pdf_document", "word_document", "") 

See the page on [R markdown] for more details. Also see the documentation for `render()` [here](https://rmarkdown.rstudio.com/docs/reference/render.html) or by entering `?render`.  



### Run files in a directory {-}

You can create a *for loop* and use it to `source()` every file in a directory, as identified with `dir()`. 

```{r, eval=F}
for(script in dir(here("scripts"), pattern = ".R$")) {   # for each script name in the R Project's "scripts" folder (with .R extension)
  source(here("scripts", script))                        # source the file with the matching name that exists in the scripts folder
}
```

If you only want to run certain scripts, you can identify them by name like this:  

```{r, eval=F}

scripts_to_run <- c(
     "epicurves.R",
     "demographic_tables.R",
     "survival_curves.R"
)

for(script in scripts_to_run) {
  source(here("scripts", script))
}

```



Here is a [comparison](https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html) of the **fs** and **base** R functions.  

### Import files in a directory  {-}

See the page on [Import and export] for importing and exporting individual files.  
See the page on [Iteration and loops] for an example with the package **purrr** demonstrating:  

* Splitting a dataframe and saving it as multiple CSV files  
* Splitting a dataframe and saving each part as a separate sheet within one Excel workbook  
* Importing multiple CSV files and combining them into one dataframe  
* Importing an Excel workbook with multiple sheets and combining them into one dataframe  




## **base** R  

See below the functions `list.files()` and `dir()`, which perform the same operation of listing files within a specified directory. You can specify `ignore.case =` or a specific patter to look for. 

```{r, eval=F}
list.files(path = here("data"))

list.files(path = here("data"), pattern = ".csv")
# dir(path = here("data"), pattern = ".csv")

list.files(path = here("data"), pattern = "evd", ignore.case = TRUE)

```

If a file is currently "open", it will display with a tilde in front, like "~$hospital_linelists.xlsx".  


<!-- ======================================================= -->
## Resources {  }

https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/directories.Rmd-->

# R Markdown { }  

R Markdown is a fantastic tool for creating automated, reproducible, and share-worthy outputs. It can generate static or interactive outputs, in the form of html, word, pdf, powerpoint, and others. 

<!-- ======================================================= -->
## Overview {  }

Using markdown will allow you easily recreate an entire formatted document, including tables/figures/text, using new data (e.g. daily surveillance reports) and/or subsets of data (e.g. reports for specific geographies). 

This guide will go through the basics. See 'resources' tab for further info.

<!-- ======================================================= -->
## Preparation {  }

**Background to Markdown**

To explain some of the concepts and packages involved:

* **Markdown** is a lightweight markup language, with syntax that allows for plain text formatting so that it can be converted to html and other formats. It is not specific to R, and usually a markdown file has an '.md' extension. 
* **R Markdown - the language**: This is an extension of markdown that _is_ specific to R, with file extensions '.Rmd'. This allows R code to be embedded in 'chunks' so that the code itself can be run, rather than just having a text document. 
* **Rmarkdown - the package**: This is used by R to render the .Rmd file into the desire output. However its focus is the markdown (text) syntax, so we also need...
* **Knitr**: This package will read the code chunks, execute it, and 'knit' it back into the document. This is how tables and graphs are included alongside the text.
* **Pandoc**: Finally, pandoc is needed to actually convert documents into e.g. word/pdf/powerpoint etc. It is separate from R. 

The R Studio website describes how these all link in together (https://rmarkdown.rstudio.com/authoring_quick_tour.html): 

>Creating documents with R Markdown starts with an .Rmd file that contains a combination of markdown (content with simple text formatting) and R code chunks. The .Rmd file is fed to knitr, which executes all of the R code chunks and creates a new markdown (.md) document which includes the R code and its output.
>
>The markdown file generated by knitr is then processed by pandoc which is responsible for creating a finished web page, PDF, MS Word document, slide show, handout, book, dashboard, package vignette or other format.
>
>This may sound complicated, but R Markdown makes it extremely simple by encapsulating all of the above processing into a single render function. Better still, RStudio includes a “Knit” button that enables you to render an .Rmd and preview it using a single click or keyboard shortcut.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/0_rmd.png"))
```

**Installation**

To create R Markdown, you need to have the following installed:

* The Rmarkdown package, as described above: `install.packages('rmarkdown')`
* Pandoc, which should come with RStudio. If you are not using RStudio, you can download it here: http://pandoc.org. 
* If you want to generate PDF output (a bit trickier), you will need to install LaTeX. For R Markdown users who have not installed LaTeX before, we recommend that you install TinyTeX (https://yihui.name/tinytex/): 

```
install.packages('tinytex')
tinytex::install_tinytex()  # install TinyTeX
```

**Workflow**

Preparation of an R Markdown workflow involves ensuring you have set up an R project and have a folder structure that suits the desired workflow. 

For instance, you may want an 'output' folder for your rendered documents, an 'input' folder for new cleaned data files, as well as subfolders within them which are date-stamped or reflect the subgeographies of interest. The markdown itself can go in a 'rmd' subfolder, particularly if you have multiple Rmd files within the same project. 

You can set code up to create output subfolders for you each time you run reports (see "Producing an output"), but you should have the overall design in mind. 

Because R Markdown can run into pandoc issues when running on a shared network drive, it is recommended that your folder is on your local machine, e.g. in a project within 'My Documents'. If you use Git (much recommended!), this will be familiar. 


<!-- ======================================================= -->
## The R Markdown file {  }

An R Markdown document looks like and can be edited just like a standard R script, in R Studio. However, it contains more than just the usual R code and hashed comments. There are three basic components:

**1. Metadata**: This is referred to as the ‘YAML metadata’ and is at the top of the R Markdown document between two ‘- - -‘s. It will tell your Rmd file what type of output to produce, formatting preferences, and other metadata sucsh as document title, author, and date. There are other uses not mentioned here (but referred to in ‘Producing an output’). Note that indentation matters. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_yaml.png"))
```
**2. Text**: This is the narrative of your document, including the titles. It is written in the markdown language, used across many different programmes. This means you can add basic formatting, for instance:

* `_text_` or `*text*` to _italicise_
* `**text**` for **bold text**
* `#` at the start of a new line for a title (and `##` for second-level title, `##` for third-level title etc)
* `*` at the start of a new line for bullet points 
* ```text``` to display text as code (as above)

The actual appearance of the font can be set by using specific templates (specified in the YAML metadata; see example tabs).  

You can also include minimal R code within backwards ticks, for within-text values. See example below.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_text.png"))
```

**3. Code chunks**: This is where the R code goes, for the actual data management and visualisation. To note:
These ‘chunks’ will appear to have a slightly different background colour from the narrative part of the document. 

Each chunk always starts with three backticks and chunk information within squiggly brackets, and ends with three more backticks.

Some notes about the content of the squiggly brackets:

*	They start with ‘r’ to indicate that the language name within the chunk is r
*	Followed by the chunk name - note this should ALWAYS be a unique name or else R will complain when you try to render. 
*	It can include other options too, but many of these can be configured with point-and-click using the setting buttons at the top right of the chunk. Here, you can specify which parts of the chunk you want the rendered document to include, namely the code, the outputs, and the warnings. This will come out as written preferences within the squiggly brackets, e.g. ‘echo=FALSE’ if you specify you want to ‘Show output only’. 

There are also two arrows at the top right of each chunk, which are useful to run code within a chunk, or all code in prior chunks. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunk.png"))
```
<!-- ======================================================= -->
## Producing an output {  }

**General notes**

Everything used by this markdown must be referenced within the Rmd file. For instance, you need to load any required packages or data. 

**A single or test run from within R Markdown**

To render a single document, for instance if you are testing it or if you only need to produce one rendered document at a time, you can do it from within the open R Markdown file. Click the "knit" button" at the top of the document. 

The 'R Markdown' tab will start processing to show you the overall progress, and a complete document will automatically open when complete. This document will also be saved in the same folder as your markdown, and with the same file name aside from the file extension. This is obviously not ideal for version control, as you will then rename the file yourself.

**A single run from an separate script**

To run the markdown so that a date-stamped file is produced, you can create a separate script and call the Rmd file from within it. You can also specify the folder and file name, and include a dynamic date and time, so that file will be date stamped on production. 


```
rmarkdown::render(("rmd_reports/create_RED_report.Rmd"),  
                        output_file = paste0("outputs/Report_", Sys.Date, ".docx")) # Use 'paste0' to combine text and code for a dynamic file name
``` 
**Routine runs into newly created date-stamped sub folders**

Add a couple lines of code to define the date you are running the report (e.g. using Sys.Date as in the example above) and create new sub folders. If you want the date to reflect a specific date rather than the current date, you can also enter it as an object. 

```
# Set the date of report
refdate <- as.Date("2020-12-21")

# Create the folders
outputfolder <- paste0("outputs/", refdate) # This is the new folder name
dir.create(outputfolder) # Creates the folder (in this case assumed 'outputs' already exists)

#Run the loop
rmarkdown::render(("rmd_reports/create_report.Rmd"),  
                        output_file = paste0(outputfolder, "/Report_", refdate, ".docx")) #Dyanmic folder name now included
``` 

You may want some dynamic information to be included in the markdown itself. This is addressed in the next section. 

<!-- ======================================================= -->
## Parametrised reports {  }

Parameterised reports are the next step so that the content of the R Markdown itself can also be dynamic. For example, the title can change according to the subgeography you are running, and the data can filter to that subgeography of interest. 

Let's say you want to run the markdown to produce a report with surveillance data for Area1 and Area2. You will:

1. Edit your R Markdown:
  a) Change your YAML metadata to include a 'params' section, which specifies the dynamic object. 
  b) Refer to this parameterised object within the code as needed. E.g. `filter(area == params$areanumber)` rather than `filter(area=="Area1")`. 
  
For instance (simplified version which does not include setup code such as library/data loading):

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parameterized.png"))
```

You can change the content by editing the YAML as needed, or set up a loop in a separate script to iterate through the areas. As with the previous section, you can set up the folders as well. 

As you can see below, you set up a list which includes all areas of interest (`arealist`), and when rendering the markdown you specify that the parameterized `areanumber` for a specific iteration is the Nth value of the arealist. For instance, for the first iteration, areanumber will equate to "Area1". The code below also specifies that the Nth area name will be included in the output file name. 

Note that this will work even if an area or date are specified within the YAML itself - that YAML information will get overwritten by the loop. 

```
# Set the date of report
refdate <- as.Date("2020-12-21")

# Set the list (note that this can also be an imported list)
arealist <- c("Area1", "Area2", "Area3", "Area4", "Area5")

# Create the folders
outputfolder <- paste0("outputs/", refdate) # This is the new folder name
dir.create(outputfolder) # Creates the folder (in this case assumed 'outputs' already exists)

#Run the loop

for(i in 1:length(arealist))  { # This will loop through from the first value to the last value in 'arealist'

rmarkdown::render(here("rmd_reports/create_report.Rmd"), 
                        params = list(areanumber = arealist[1], #Assigns the nth value of arealist to the current areanumber
                                      refdate = refdate),
                        output_file = paste0(outputfolder, "/Report_", arealist[1], refdate, ".docx")) 
                        
}


``` 


<!-- ======================================================= -->
## Resources {  }

Further information can be found via:

* https://bookdown.org/yihui/rmarkdown/
* https://rmarkdown.rstudio.com/articles_intro.html

A good explainer of markdown vs knitr vs Rmarkdown is here: https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/rmarkdown.Rmd-->


# Routine reports {  }  

UNDER CONSTRUCTION  

This page will cover the **reportfactory** package and other tips for routinizing your data flows and reports. **reportfactory** is a package developed by RECON (R Epidemics Consortium). It "facilitates workflows for handling multiple .Rmd reports, compiling one or several reports in one go, and storing outputs in well-organised, timestamped folders."  


```{r, eval=FALSE}
pacman::p_load(reportfactory)
```

<!-- ======================================================= -->
## Resources {  }

See the package's [Github page](https://github.com/reconverse/reportfactory)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/reportfactory.Rmd-->


# Errors & warnings  


This page lists common errors and suggests solutions for troubleshooting them


## Data management errors {}  



```
No such file or directory:
```
If you see an error like this when you try to export or import: Check the spelling of the file and filepath, and if the path contains slashes make sure they are forward `/` and not backward `\`. Also make sure you used the correct file extension (e.g. .csv, .xlsx).  



```
#Tried to add a value ("Missing") to a factor (with replace_na operating on a factor)
Problem with `mutate()` input `age_cat`.
i invalid factor level, NA generated
i Input `age_cat` is `replace_na(age_cat, "Missing")`.invalid factor level, NA generated
```

You likely have a column of class Factor (which contains pre-defined levels) and tried to add a new value to it. Convert it to class Character before adding a new value.  


## Package masked errors {}  

```
Error in select(data, var) : unused argument (var)
```
You think you are using `dplyr::select()` but the `select()` function has been masked by `MASS::select()` - specify `dplyr::` or re-order your package loading so that dplyr is after all the others.

Other common masking errors stem from: `plyr::summarise()` and `stats::filter()`. Consider using the [**conflicted** package](https://www.tidyverse.org/blog/2018/06/conflicted/).


## Plotting errors {}  


```
# ran recode without re-stating the x variable in mutate(x = recode(x, OLD = NEW)
Error: Problem with `mutate()` input `hospital`.
x argument ".x" is missing, with no default
i Input `hospital` is `recode(...)`.
```


`Error: Insufficient values in manual scale. 3 needed but only 2 provided.`
ggplot() scale_fill_manual() values = c("orange", "purple") ... insufficient for number of factor levels ... consider whether NA is now a factor level...


```
Error: unexpected symbol in:
"  geom_histogram(stat = "identity")+
  tidyquant::geom_ma(n=7, size = 2, color = "red" lty"
```
If you see "unexpected symbol" check for missing commas  



consider whether you re-arranged dplyr verbs and didn't replace a pipe in the middle, or didn't remove a pipe from the end.


Can't add x object ... Have a `+` at the end of a ggplot command that you need to delete.
 



<!-- ======================================================= -->
## Resources { }


https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/errors_warnings.Rmd-->


# Advanced RStudio { }  

THIS PAGE IS UNDER CONSTRUCTION



## Find in Files 

This is an advanced search function that allows you to "Find and replace" terms across many scripts at one time.  

## Keyboard shortcuts  

https://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/


## Connections  

Query SQL  



## Customize appearance  


## Package management with **renv**  
The renv package is replacing the Packrat package that RStudio used to maintain.


## View function source code  


## Environments  


## RStudio Connect and Cloud  


## Using Python with RStudio  



## Github  

See the page on [Collaboration with Github] for tips on how to use RStudio with Github.  




<!-- ======================================================= -->
## Resources {  }



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/rstudio_advanced.Rmd-->

# Relational databases { }  

THIS PAGE IS UNDER CONSTRUCTION

<!-- ======================================================= -->
## Resources {  }



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/relational_databases.Rmd-->


# Shiny and dashboards { }  

THIS PAGE IS UNDER CONSTRUCTION  

<!-- ======================================================= -->
## Resources {  }

This tab should stay with the name "Resources".
Links to other online tutorials or resources.



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/shiny_basics.Rmd-->


# Collaboration with Github { }  


<!-- ======================================================= -->
## Overview {  }

* Package management  
* Using Github and R  




## Using Github and R to contribute

Here is an [online guide](https://happygitwithr.com/rstudio-git-github.html) to using Github and R. Some of the below text is adapted from this guide.  

### Overview of GitHub  
Github is a website that supports **collaborative projects** with **version control**.  In a nutshell, the project's files exist in the **Github repository** as a **"master"** version (called a **"branch"**). If you want to make a change to those files you must create a different branch (version) to build and test the changes in. Master remains unaffected by your changes until your branch is **merged** (after some verification steps) into the master branch. A **"commit"** is the saving of a smaller group of changes you make within your branch. A **Pull Request** is your request to merge your changes into the master branch.  

The way RStudio and Github interact is as follows:  

* There is a REMOTE version of the `Epi_R_handbook` R project that lives on Github website repository - master and other branches all exist and are viewable on this Github repository. Pull requests, issue tracking, and de-conflicting merges happens online here.  
* On your LOCAL computer, you **clone** a version of the entire Github repository (all the R project files, from *all* its branches/versions). Locally, you can make changes to the files of any branch and "commit" those changes (save them with an explanatory note). These changes are only stored locally on your computer until...  
* Your LOCAL repository/Rproject interacts with the REMOTE one by 1) **pulling** (updating local files from the remote ones of the same branch) and **pushing** (pushing local changes to the same branch of the remote repository)  
* The software *Git* on your computer underlies all this, and is used by RStudio. You don't have to interact with Git except *through* RStudio. While you can write Git command-line into the RStudio terminal, it is easier to just interact with Git through RStudio point-and-click buttons. As noted below, you may *occasionally* have to write Git commands in the RStudio terminal.  


```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "GitHub-Flow.png"))
``` 
*Image [source](https://build5nines.com/introduction-to-git-version-control-workflow/)*


### First steps

1) **Register** for a free account with Github 
2) **Have R and RStudio** installed/updated  
3) **Install Git** to your computer (remember Git is a software on your computer accessed by RStudio, Github is a website)  
4) **Familiarize yourself** to the Github workflow by [reading about it](https://guides.github.com/introduction/flow/)  
5) **Become a contributor** to the [Epi_R_handbook Github repository](https://github.com/nsbatra/Epi_R_handbook) (email neale.batra@gmail.com)
6) **Clone** the Github repository to your computer  
     * In RStudio start a new project *File > New Project > Version Control > Git*  
     * In “Repository URL”, paste the URL *https://github.com/nsbatra/Epi_R_handbook.git* (link also available from repo main page, green "Code" button, HTTPS)
     * Accept the default project directory name `Epi_R_handbook`  
     * Take charge of – or at least notice! – where the Project will be saved locally  
     * Check "Open in new session" and click "Create project"  
     * You should now be in a new local RStudio project that is a clone of the `Epi_R_handbook` repository on Github
     
In your RStudio you will now have a Git tab in the same tab as your R Environment: 


```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Git_console.png"))
```  
Please note the buttons circled as they will be referenced later (from left to right):  

* Button to begin "commiting" your changes to your branch (will open a new window)  
* Arrows to PULL (update your local version of the branch with any changes to made your branch by others) and to PUSH (send any completed commits stored in your local version of the branch to the remote/Github version of your branch)  
* The Git tab in RStudio  
* Button to create a NEW branch of whichever version is listed to the right. **You almost always want to branch off of the master (after you PULL to update the master first)**.  
* The branch you are currently working in.  
* Below all this, changes you make to code or files will begin to appear  

### **To work on your Handbook page:**  

*Note: Last I heard, Github will soon change their terminology of "master" to "main", as it is an unnecessary reference to slavery*  

1) **Create a branch**  


* **Be in master branch** and then click the branch button/icon.  
* **Name your branch** with a one-word descriptive name (can use underscores if needed). You will see that locally, you are still in the project Epi_R_handbook, but you are no longer working on the master branch. Once created, the new branch will also appear on the Github website as a branch.  
*  **Make your changes**... to files, code, etc. Your changes are tracked.  
* **Commit the changes**. Every series of changes you make that are substantial (e.g. adding or updating a section, etc.), stop and *commit* those changes. Think of a commit as a "batch" of changes related to a common purpose.  
     * Press "Commit" in the git tab, opens new window  
     * Review the changes you made (green, red etc.)  
     * Highlight all the changes for the commit and "stage" them by checking their boxes or highlighting all the rows and clicking "stage all"  
     * Write a commit message that is short but descriptive (required) 
     * Press "commit" on the right side  
* Make and commit more changes, as many times as you would like  
* **PULL** - click the PULL icon (downward arrow) which updates the branch version on your local computer with any changes that have been made to it and stored in the remote/Github version  
     * PULL often. Don't hesitate. **Always pull before pushing**.  
* **PUSH** your changes up to the remote/Github version of your branch.  
     * You may be asked to enter your Github username and password.  
     * The first time you are asked, you may need to enter two Git command lines into the *Terminal* (the tab next to the R Console):
        * **git config --global user.email "you@example.com"**   (your Github email address), and  
        * **git config --global user.name "Your Github username"**  
     


2) **Request to merge your branch with master**  

Once done with your commits and pushed everything up to the remote Github repository, you may want to request that your branch be merged with the master branch. 

* Go to Epi_R_handbook Github repository  
* Use the branch drop-down to view your branch, not master
* At top you will see green button saying "Compare and Pull Request" for your branch. If not, look for another button that says pull request.
* Write a detailed comment and click "Create Pull Request"
* On the right, request a review from members of the project's core team. You need at least one review to be able to complete the merge.
* Once completed, delete your branch as explained below

3) **Delete your branch on Github**  

GO to the repository on Github and click the button to view all the branches (next to the drop-down to select branches). Now find your branch and click the trash icon next to it. Read more [here](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository#deleting-a-branch)  

Be sure to also delete the branch locally on your computer:

* From RStudio, make sure you are in Master branch
* Switch to typing in the "terminal" (tab adjacent to the R console), and enter this: **git branch -d branch_name** , where "branch_name" is the name of your branch to be deleted  
* Refresh your Git tab and this branch should be gone.


**TEST IT**
You can test your ability to make changes, commits, pull requests, etc. by modifying this R script which is saved to the main Rproject folder: `test_your_abilities.R`


**Asked to provide password too often??**  
Instructions for connecting to the repository via a SSH key (more complicated): 
See chapters 10 and 11 of [this tutorial](https://happygitwithr.com/credential-caching.html#credential-caching)




<!-- ======================================================= -->
## Resources {  }



https://happygitwithr.com/reset.html

https://ohi-science.org/news/github-going-back-in-time

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/collaboration.Rmd-->


# Writing functions { }  

PAGE IS UNDER CONSTRUCTION


<!-- ======================================================= -->
## Resources {  }



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/writing_functions.Rmd-->


# R on network drives { }  

 


<!-- ======================================================= -->
## Overview {  }

Using R on network or "company" shared drives can be extremely frustrating. This page contains approaches, common errors, and suggestions on troubleshooting, including for the particularly delicate situations involving Rmarkdown.  

**Using R on Network Drives: Overarching principles**  

1) Must have administrator access on your computer. Setup RStudio specifically to run as administrator.  
2) Use your "\\\" package library as little as possible, save packages to "C:" library when possible.  
3) the **rmarkdown** package must not be in a "\\\" library, as then it can't talk to TinyTex or Pandoc.  



<!-- ======================================================= -->
## Preparation {  }


**Using R on Network Drives: Overarching principles**  

1) Must have administrator access on your computer. Setup RStudio specifically to run as administrator.  
2) Use your "\\\" package library as little as possible, save packages to "C:" library when possible.  
3) the **rmarkdown** package must not be in a "\\\" library, as then it can't talk to TinyTex or Pandoc.  

**Useful commands**

```{r, eval=F}
# Find libraries
.libPaths()                   # Your library paths, listed in order that R installs/searches. 
                              # Note: all libraries will be listed, but to install to some (e.g. C:) you 
                              # may need to be running RStudio as an administrator (it won't appear in the 
                              # install packages library drop-down menu) 

# Switch order of libraries
# this can effect the priority of R finding a package. E.g. you may want your C: library to be listed first
myPaths <- .libPaths() # get the paths
myPaths <- c(myPaths[2], myPaths[1]) # switch them
.libPaths(myPaths) # reassign them

# Find Pandoc
Sys.getenv("RSTUDIO_PANDOC")  # Find where RStudio thinks your Pandoc installation is

# Find a package
# gives first location of package (note order of your libraries)
find.package("rmarkdown", lib.loc = NULL, quiet = FALSE, verbose = getOption("verbose")) 
```



<!-- ======================================================= -->
## Troubleshooting common errors {  }


**"Failed to compile...tex in rmarkdown"**  

check/install tinytex, to C: location

```{r, eval=F}
# check/install tinytex, to C: location
tinytex::install_tinytex()
tinytex:::is_tinytex() # should return TRUE (note three colons)
```


**Internet routines cannot be loaded**  

For example, "Error in tools::startDynamicHelp() : internet routines cannot be loaded"  

* Try selecting 32-bit version from RStudio via Tools/Global Options.  
  * note: if 32-bit version does not appear in menu, make sure not using RStudio v1.2.  
* Or try uninstalling R and re-installing with different bit (32 instead of 64)


**C: library does not appear as an option when I try to install packages manually**

* Must run RStudio as an administrator, then it will appear.  
* To set-up RStudio to always run as administrator (advantageous when using an Rproject where you don't click RStudio icon to open)... right-click the Rstudio icon, open properties, compatibility, and click the checkbox Run as Administrator.  


**Pandoc 1 error**  

If you are getting pandoc error 1 when knitting Rmarkdowns on network drives:  

* this can help (of two library locations, have the one with lettered drive listed first)  
* This worked when knitting on local drive but while on network internet connection  
* See https://ciser.cornell.edu/rmarkdown-knit-to-html-word-pdf/  

```{r, eval=F}
myPaths <- .libPaths() # get the library paths
myPaths <- c(myPaths[2], myPaths[1]) # switch them
.libPaths(myPaths) # reassign them
```


**Pandoc Error 83 (can't find file...rmarkdown...lua...)**  
This means that it was unable to find this file.  

See https://stackoverflow.com/questions/58830927/rmarkdown-unable-to-locate-lua-filter-when-knitting-to-word  

Possibilities:  

1) Rmarkdown package is not installed  
2) Rmarkdown package is not findable  
3) an admin rights issue.  

R is not able to find the 'rmarkdown' package file, so check which library the rmarkdown package lives. 
If it is in a library that in inaccessible (e.g. starts with "\\\") consider manually moving it to C: or other named drive library.  
But be aware that the rmarkdown package has to be able to reach tinytex, so rmarkdown package can't live on a network drive.


**Pandoc Error 61**
For example: "Error: pandoc document conversion failed with error 61"  

"Could not fetch..."  

* Try running RStudio as administrator (right click icon, select run as admin, see above instructions)  
* Also see if the specific package that was unable to be reached can be moved to C: library.

**LaTex error (see below)**

"! Package pdftex.def Error: File `cict_qm2_2020-06-29_files/figure-latex/unnamed-chunk-5-1.png' not found: using draft setting."

"Error: LaTeX failed to compile file_name.tex."  
See https://yihui.org/tinytex/r/#debugging for debugging tips. 
See file_name.log for more info.


**Pandoc Error 127**
This could be a RAM (space) issue. Re-start your R session and try again. 


**Mapping network drives**

How does one open a file "through a mapped network drive"?  

* First, you'll need to know the network location you're trying to access.  
* Next, in the Windows file manager, you will need to right click on "This PC" on the right hand pane, and select "Map a network drive".  
* Go through the dialogue to define the network location from earlier as a lettered drive.  
* Now you have two ways to get to the file you're opening. Using the drive-letter path should work.  

From: https://stackoverflow.com/questions/48161177/r-markdown-openbinaryfile-does-not-exist-no-such-file-or-directory/55616529?noredirect=1#comment97966859_55616529


**ISSUES WITH HAVING A SHARED LIBRARY LOCATION ON NETWORK DRIVE**  

**Error in install.packages()**  

Try removing... /../.../00LOCK (directory)  

* Manually delete the 00LOCK folder directory from your package the library. Try installing again.  
* You can try the command pacman::p_unlock() (you can also put this command in the Rprofile so it runs every time project opens.)  
* Then try installing the package again. It may take several tries.  
* If all else fails, install the package to another library and then manually copy it over.  



<!-- ======================================================= -->
## Resources {  }

This tab should stay with the name "Resources".
Links to other online tutorials or resources.




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=3)
```

<!--chapter:end:new_pages/network_drives.Rmd-->

