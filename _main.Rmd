---
knit: "bookdown::render_book"
title: "The Epidemiologist R Handbook"
author: "the handbook team"
description: "This is a R reference manual for applied epidemiologists and public health practitioners."  
date: "`r Sys.Date()`"
#url: 'https://github.com/nsbatra/Epi_R_handbook'
github-repo: nsbatra/Epi_R_handbook
#twitter-handle: 
#cover-image: images/R_Handbook_Logo.png
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
---


# The Epidemiologist R Handbook {-}

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "R Handbook Logo.png"))
```



<span style="color: red;">**THIS IS A DRAFT**.</span>

<span style="color: orange;">**IF YOU ARE REVIEWING THIS BOOK, PLEASE PROVIDE FEEDBACK FOR EACH PAGE AT THIS [LINK](https://forms.gle/4RNdRRLGx67xW9yq9)**</span>


<!-- ======================================================= -->
## About this handbook {-}

<span style="color: brown;">**This is a free open-access R reference manual for applied epidemiologists and public health practitioners.**</span>

**This book strives to:**  

* Serve as a quick reference manual - not as a textbook or comprehensive R training  
* Address common epidemiological problems via task-centered examples  
* Be accessible in settings with low internet-connectivity via this (**[downloadable version**](https://github.com/nsbatra/Epi_R_handbook/tree/master/offline_long))  

**What gaps does this book address?**  

* Many epidemiologists are transitioning to R from SAS, STATA, SPSS, Excel, or other software  
* Let's avoid hours of online searching and have a repository for best-practice code for the common epi user  
* Epidemiologists sometimes work in low internet-connectivity environments and have limited support  

**How is this different than other R books?**  

* It is written by epidemiologists, for epidemiologists - leveraging experience in local, national, academic, and emergency settings  
* It provides examples of epidemic curves, transmission chains, epidemic modeling and projections, age and sex pyramids and standardization, record matching, outbreak detection, survey analysis, causal diagrams, survival analysis, GIS basics, phylogenetic trees, automated reports, etc...  



<!-- ======================================================= -->
## How to read this handbook {-} 

* Search via the search box above the Table of Contents 
* Click the "copy" icons to copy code  
* See the "Resources" section of each page for further resources  


<!-- ======================================================= -->
## Edit or contribute {-}

We welcome your feedback or comments at this survey LINK.

If you want to directly contribute or modify content, please post an issue or submit a pull request at this [github repository](https://github.com/nsbatra/R_epi_handbook).  




<!-- ======================================================= -->
## Acknowledgements {-}  


### Contributors {-}  

This book has been conceived, written, and edited by a collaboration of epidemiologists from around the world who draw upon experiences with a constellation of organizations including local/state/provincial/national health departments and ministries, the World Health Organization (WHO), MSF (Medecins sans frontiers / Doctors without Borders), hospital systems, and academic institutions.

**Editor-in-Chief:** Neale Batra 

**Core team:** Neale Batra, Alex Spina, Amrish Baidjoe, Henry Laurenson-Schafer, Finlay Campbell, Pat Keating  

**Authors** *(in order of contributions)*: Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, Isaac Florence, Natalie Fischer, Daniel Molling, Liza Coyer, Jonny Polonski, Yurie Izawa, Sara Hollis, Isha Berry  

**Reviewers:** ...(list)...  

**Advisers**  ...(list)...  


### Funding and programmatic support {-}  

The handbook received funding via a COVID-19 emergency capacity-building grant from Training Programs in Epidemiology and Public Health Interventions Network ([TEPHINET](https://www.tephinet.org/)).  

Programmatic support was provided by the EPIET Alumni Network ([EAN](https://epietalumni.net/)).  



### Inspiration {-}  

The multitude of tutorials and vignettes that provided foundational knowledge for development of handbook content are credited within their respective pages.  

More generally, the following sources provided inspiration and laid the groundwork for this handbook:  
[The "R4Epis" project](https://r4epis.netlify.app/) (a collaboration between MSF and RECON)  
[R Epidemics Consortium (RECON)](https://www.repidemicsconsortium.org/)  
[R for Data Science book (R4DS)](https://r4ds.had.co.nz/)  
[bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/)  
[Netlify](https://www.netlify.com) hosts this website  

### Image credits {-}  


Logo (US CDC Public Health Image Library):  
[2013 Yemen looking for mosquito breeding sites](https://phil.cdc.gov/Details.aspx?pid=19623)  
[Ebola virus](https://phil.cdc.gov/Details.aspx?pid=23186)  
[Survey in Rajasthan](https://phil.cdc.gov/Details.aspx?pid=19838)  



### License and Terms of Use {-}  

This handbook is **not** an approved product of any specific organization.  

Although we strive for accuracy, we provide no guarantee of the content in this book.  

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))
```

<!--chapter:end:index.Rmd-->

# (PART) Preview pages {-}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))
```

<!--chapter:end:new_pages/cat_preview.Rmd-->

# Univariate and multivariate regression { }

<!-- ======================================================= -->

## Overview {  }

This tab demonstrates the use of **gtstummary** and regression packages to 
look at associations between variables (e.g. odds ratios, risk ratios and hazard
ratios)

1.  Univariate: two-by-two tables 
2.  Stratified: mantel-haenszel estimates 
3.  Multivariable: variable selection, model selection, final table
4.  Forest plot


<!-- ======================================================= -->

## Preparation {  }


### Packages {-}

This code chunk shows the loading of packages required for the analyses.

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse,    # data management + ggplot2 graphics, 
  stringr,      # manipulate text strings 
  purrr,        # loop over objects in a tidy way
  gtsummary,    # summary statistics and tests 
  broom,        # tidy up results from regressions
  parameters,   # alternative to tidy up results from regressions
  see           # alternative to visualise forest plots
  )
```

### Load data {-}

The example dataset used in this section is a linelist of individual cases from a simulated epidemic. The dataset is imported using the `import()` function from the *rio* package. See the page on [Import and export] for various ways to import data.

```{r echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

```

```{r eval=F}
# import the linelist
linelist <- rio::import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T) )
```

### Clean data {-}

**Convert to 1's and 0's**  

Below we convert certain columns from "yes"/"no" to 1/0, to better function with models. To do this efficiently, we define a vector of the column names of our explanatory variables.  

```{r}
## define variables of interest 
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")
```

We apply the function `case_when()` to convert specified values to 1's and 0's. This function is applied all the `explanatory_vars`  columns using `across()` (see page on [Grouping data]).  

```{r}
## convert dichotomous variables to 0/1 
linelist <- linelist %>% 
  mutate(
    ## for each of the variables listed and "outcome"
    across(
      all_of(c(explanatory_vars, "outcome")), 
      ## recode male, yes and death to 1; female, no and recover to 0
      ## otherwise set to missing
           ~case_when(
             . %in% c("m", "yes", "Death")   ~ 1,
             . %in% c("f", "no",  "Recover") ~ 0, 
             TRUE                            ~ NA_real_
           ))
  )
```

**Drop rows with missing values**  
To do this, we add the column `age` to the `explanatory_vars` (`age` would have produced an error in the previous `case_when()` operation). Then we pipe the `linelist` to `drop_na()` to remove any rows with missing values in the `outcome` column or any of the `explanatory_vars` columns.  

```{r}
## add in age_category to the explanatory vars 
explanatory_vars <- c(explanatory_vars, "age_cat")

## drop rows with missing information for variables of interest 
linelist <- linelist %>% 
  drop_na(any_of(c("outcome", explanatory_vars)))

```

The number of rows remaining in `linelist` is `r nrow(linelist)`.  



<!-- ======================================================= -->

## Univariate {  }

Just like in the page on [Descriptive analysis], your use case will determine which R package you use. We present two options for doing univariate analysis:  

* Use functions available in **base** to quickly print results to the console. Accompanied with the **broom** package to tidy up the outputs.  
* Use the **gtsummary** package or you can use the individual regression 
 






<!-- ======================================================= -->

### **gtsummary** package {-}

Below we present the use of `tbl_uvregression()` from the **gtsummary** package. Just like in the page on [Descriptive analysis], **gtsummary** functions do a good job of running statistics *and* producing professional-looking outputs.  

In this case, we select only the necessary columns from the `linelist` and then pipe into `tbl_uvregression()`. We are going to run univariate regression on each of the columns we defined as `explanatory_vars` in the Preparation section (gender, fever, chills, cough, aches, vomit, and age_cat).  

Within the function itself, we provide the `method = ` as `glm` (no quotes), the outcome column as `outcome`, specify that we want to run logistic regression (it's in a list because you could specify multiple), and we tell it to exponentiate the results.  

The output is HTML and contains the counts

```{r odds_gt}

univ_tab <- linelist %>% 
  ## select variables of interest
  dplyr::select(explanatory_vars, outcome) %>% 
  ## produce univariate table
  tbl_uvregression(
    ## define regression want to run (generalised linear model)
    method = glm, 
    ## define outcome variable
    y = outcome, 
    ## define what type of glm want to run (logistic)
    method.args = list(family = binomial), 
    ## exponentiate the outputs to produce odds ratios (rather than log odds)
    exponentiate = TRUE
    )

## view univariate results table 
univ_tab
```


<!-- ======================================================= -->

### **base** R {-}

To produce odds ratios or for logistic regression we use the `glm()` function from the **stats** package (part of base R). GLM is an acronym for Generalized Linear Model. Unlike **gtsummary**, you would need to run this multiple times (e.g. within a loop) to run univariate regression on multiple variables. An example of this is given below.  


#### Univariate `glm()` on one variable {-}

The model is provided to `glm()` as an equation, with the outcome on the left and explanatory variables on the right of a tilde `~`. In this example we are assessing the association between different age categories and the outcome of death (now coded as 1, see Preparation section).  

The dataset is specified to the `dataset = ` argument.  

The `family = ` is set to "binomial" to indicate logistic regression. Options for `family = ` are below. If necessary, you can also specify the link function in the syntax `glm(formula, family=familytype(link=linkfunction), data=)`.  


Family                 | Default link function 
-----------------------|-------------------------------------------  
`"'binomial"` | `(link = "logit")`  
`"gaussian"` | `(link = "identity")`  
`"Gamma"` | `(link = "inverse")`  
`"inverse.gaussian"` | `(link = "1/mu^2")`  
`"poisson"` | `(link = "log")`  
`"quasi"` | `(link = "identity", variance = "constant")`  
`"quasibinomial"` | `(link = "logit")`  
`"quasipoisson"` | `(link = "log")`  

Type `?glm` for other options. 
 
Below is how the output of `glm()` prints to the console - the log odds. The baseline reference category is the first factor level of `age_cat`.  

```{r}
glm(outcome ~ age_cat, family = "binomial", data = linelist)
```

For a single exposure variable, use `tidy()` from 
the **broom** package to get the *exponentiated* log odds ratio estimates and confidence
intervals. Here we demonstrate how to combine model outputs with a table of 
counts. 

```{r odds_base_single}

model <- glm(
  ## define the variables of interest
  outcome ~ age_cat, 
  ## define the type of regression (logistic)
  family = "binomial", 
  ## define your dataset
  data = linelist) %>% 
  ## clean up the outputs of the regression (exponentiate and produce CIs)
  tidy(
      exponentiate = TRUE, 
      conf.int = TRUE)
```

Below is the output of `model` (tidied via **broom**):  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(model, rownames = FALSE, options = list(pageLength = nrow(model), scrollX=T), class = 'white-space: nowrap' )
```

We can combine these model results with a table of linelist counts. Below, we operate on the `linelist` used for the model:  

* Group rows by outcome, and get counts by age category  
* Pivot wider so the column are `age_cat`, `0`, and `1`  
* Remove row for `NA` `age_cat`, if applicable, to align with the model results  

```{r}

counts_table <- linelist %>% 
  ## get counts of variable of interest grouped by outcome
  group_by(outcome) %>% 
  count(age_cat) %>% 
  ## spread to wide format (as in cross-tabulation)
  pivot_wider(names_from = outcome, values_from = n) %>% 
  ## drop rows with missings
  filter(!is.na(age_cat))

```


```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(counts_table, rownames = FALSE, options = list(pageLength = nrow(counts_table), scrollX=T), class = 'white-space: nowrap' )
```
Now we can bind the two tables together horizontally with `bind_cols()`. In this case, the `.` represents the piped object `counts_table` and we bind it to `model`. Then we use `select()` to pick the columns to keep and their order.  

```{r}
combined <- counts_table %>% 
  ## merge with the outputs of the regression 
  bind_cols(., model) %>% 
  ## only keep columns interested in 
  select(term, 2:3, estimate, conf.low, conf.high, p.value)
```

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(combined, rownames = FALSE, options = list(pageLength = nrow(combined), scrollX=T), class = 'white-space: nowrap' )
```


#### Looping through multiple variables {-}  

To run over several exposure variables to produce univariate odds ratios (i.e. 
not controlling for each other), you can pass a vector of variable names to the 
`map()` function in the **purrr** package. This will loop over each of the variables
running regressions for each one. See the page on [Iteration] for tips.  


```{r odds_base_multiple}

models <- explanatory_vars %>% 
  ## combine each name of the variables of interest with the name of outcome variable
  str_c("outcome ~ ", .) %>% 
  ## for each string above (outcome ~ "variable of interest)
  map(
    ## run a general linear model 
    ~glm(
      ## define formula as each of the strings above
      as.formula(.x), 
      ## define type of glm (logistic)
      family = "binomial", 
      ## define your dataset
      data = linelist)
  ) %>% 
  ## for each of the output regressions from above 
  map(
    ## tidy the output
    ~tidy(
      ## each of the regressions 
      .x, 
      ## exponentiate and produce CIs
      exponentiate = TRUE, 
      conf.int = TRUE)
  ) %>% 
  ## collapse the list of regressions outputs in to one data frame
  bind_rows()
```

Once again, `models` contains the log odds and other outputs... but is longer this time because multiple outputs were combined using `bind_rows()`. Click through to see all the rows of `model`.  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(models, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

As before, we can create a counts table from the `linelist`, bind it to `models`, and make a nice table. See the page on [Tables] for ideas on how to convert this table to an HTML output.  

```{r, warning=F, message=F}

## for each explanatory variable
univ_tab_base <- map(explanatory_vars, 
      ~{linelist %>% 
          ## group data set by outcome
          group_by(outcome) %>% 
          ## produce counts for variable of interest
          count(.data[[.x]]) %>% 
          ## spread to wide format (as in cross-tabulation)
          pivot_wider(names_from = outcome, values_from = n) %>% 
          ## drop rows with missings
          filter(!is.na(.data[[.x]])) %>% 
          ## change the variable of interest column to be called "variable"
          rename("variable" = .x) %>% 
          ## change the variable of interest column to be a character 
          ## otherwise non-dichotomous (categorical) variables come out as factor and cant be merged
          mutate(variable = as.character(variable))
                 }
      ) %>% 
  ## collapse the list of count outputs in to one data frame
  bind_rows() %>% 
  ## merge with the outputs of the regression 
  bind_cols(., models) %>% 
  ## only keep columns interested in 
  select(term, 2:3, estimate, conf.low, conf.high, p.value)

```

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(univ_tab_base, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

## Stratified {  }

Stratified analysis is currently still being worked on for **gtsummary**, 
this page will be updated in due course. 


<!-- ======================================================= -->

### **gtsummary** package {-}

TODO

<!-- ======================================================= -->

### **base** R {-}

TODO

<!-- ======================================================= -->



## Multivariable {-}

For multivariable analysis, there is not much difference between using
**gtsummary** or `glm()` with **broom** to present the data. The workflow is the same for both, as shown below, and only the last step of pulling a table together is different.


<!-- ======================================================= -->
### **base** R `glm()`  

You can again use `glm()` but add more variables to the right side of the equation, separated by plus symbols (`+`). To run the model with all of our explanatory variables we would run:  

```{r}
glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = "binomial", data = linelist)
```

Optionally, you can leverage the pre-defined vector of column names and create the above command using `str_c()` as shown below. This might be useful if your explanatory variable names are changing, or you don't want to type them all out again.  

```{r mv_regression}

## run a regression with all variables of interest 
mv_reg <- explanatory_vars %>% 
  ## combine all names of the variables of interest separated by a plus
  str_c(collapse = "+") %>% 
  ## combined the names of variables of interest with outcome in formula style
  str_c("outcome ~ ", .) %>% 
  glm(## define type of glm (logistic)
      family = "binomial", 
      ## define your dataset
      data = linelist) 
```

Note the class of the saved model.  

```{r}
class(mv_reg)
```

Finally, you must take the model object and apply the `step()` function from the **stats** package, to specify which variable selection direction you want use when building the model.      

```{r}
## choose a model using forward selection based on AIC
## you can also do "backward" or "both" by adjusting the direction
final_mv_reg <- mv_reg %>%
  step(direction = "forward", trace = FALSE)
```


You can also turn off scientific notation, for clarity:  

```{r}
options(scipen=999)
```

Pass to `tidy()` as described above to exponentiate the log odds and CIs. We do one final step where we join the   

```{r mv_regression_base}

mv_tab_base <- final_mv_reg %>% 
  ## get a tidy dataframe of estimates 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE)
```



<!-- ======================================================= -->

### Combine univariate and multivariate {-}

#### With **gtsummary**  {-}  

The `gtsummary` package provides the `tbl_regression` function, which will 
take the outputs from a regression (`glm` in this case) and produce an easy 
summary table. 
You can also combine several different output tables produced by `gtsummary` with 
the `tbl_merge` function. 

```{r mv_regression_gt}

## show results table of final regression 
mv_tab <- tbl_regression(final_mv_reg, exponentiate = TRUE)

## combine with univariate results 
tbl_merge(
  tbls = list(univ_tab, mv_tab), 
  tab_spanner = c("**Univariate**", "**Multivariable**"))

```

#### With **dplyr** join {-}  

To combine the `glm()`/`tidy()`univariate and multivariate outputs, you can also do the following with **dplyr** join functions.  

* Join the univariate results from earlier (which contains counts) with the tidied multivariate results  
* Use `select()` to keep only the columns we want, specify their order, and re-name them  
* Use `round()` with two decimal places on all the column that are class Double  

```{r}
## combine univariate and multivariable tables 
left_join(univ_tab_base, mv_tab_base, by = "term") %>% 
  ## choose columns and rename them
  select( # new name =  old name
    "characteristic" = term, 
    "recovered"      = "0", 
    "dead"           = "1", 
    "univ_or"        = estimate.x, 
    "univ_ci_low"    = conf.low.x, 
    "univ_ci_high"   = conf.high.x,
    "univ_pval"      = p.value.x, 
    "mv_or"          = estimate.y, 
    "mvv_ci_low"     = conf.low.y, 
    "mv_ci_high"     = conf.high.y,
    "mv_pval"        = p.value.y 
  ) %>% 
  mutate(across(is.double, round, 2))

```



#### Interaction terms {-}

If you want to specify an interaction between two variables in `glm()`, separate the variables with a colon `:`. You can separate them with an asterisk `*` as shorthand to includ the two variables *and* their interaction:  

```{r, eval=F}
glm(outcome ~ gender + age_cat * fever, family = "binomial", data = linelist)
```


<!-- ======================================================= -->

## Forest plot {  }

This section shows how to produce a plot with the outputs of your regression.
There are two options, you can build a plot yourself using `ggplot2` or use a 
package called **easystats**.  


<!-- ======================================================= -->

### **ggplot2** package {-}

You can build a forest plot with `ggplot()` by plotting elements of the multivariate regression results. Add the layers:  

* estimates with `geom_point()`  
* confidence intervals with `geom_errorbar()`  
* a vertical line at OR = 1 with `geom_vline()`  

You may want to re-arrange the order of the variables/levels on the y-axis (see how the order of age_cat levels is alphabetical and not sensical). To do this, use `fct_relevel()` from the **forcats** package to classify the column `term` as a factor and specify the order manually. See the page on [Factors] for more details.  

```{r ggplot_forest}

## remove the intercept term from your multivariable results
mv_tab_base %>% 
  filter(term != "(Intercept)") %>% 
  ## plot with variable on the y axis and estimate (OR) on the x axis
  ggplot(aes(x = estimate, y = term)) +
  ## show the estimate as a point
  geom_point() + 
  ## add in an error bar for the confidence intervals
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  ## show where OR = 1 is for reference as a dashed line
  geom_vline(xintercept = 1, linetype = "dashed")
  
```


<!-- ======================================================= -->

### **easystats** packages {-}

The alternative if you do not want to decide all of the different things required
for a `ggplot`, is to use a combination of `easystats` packages. 
In this case the `paramaters` package function `model_parameters` does the equivalent
of `broom` package function `tidy`. The `see` package then accepts those outputs
and creates a default forest plot as a `ggplot` object. 

```{r easystats_forest}

## remove the intercept term from your multivariable results
final_mv_reg %>% 
  model_parameters(exponentiate = TRUE) %>% 
  plot()
  
```


<!-- ======================================================= -->

## Resources {  }

Much of the information in this page is adapted from these resources and vignettes online:  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)  

[sthda stepwise regression](http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/)   

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))
```

<!--chapter:end:new_pages/stat_tests.Rmd-->

