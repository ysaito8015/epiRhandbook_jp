<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>23 Time series and outbreak detection | The Epidemiologist R Handbook</title>

    <meta name="author" content="the handbook team" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/header-attrs-2.8/header-attrs.js"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.2.5.1/tabs.js"></script>
    <script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script>

  <!-- CSS -->
    <link rel="stylesheet" href="style_bs4.css" />
    
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">The Epidemiologist R Handbook</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="time-series-and-outbreak-detection" class="section level1" number="23">
<h1><span class="header-section-number">23</span> Time series and outbreak detection</h1>
<!-- ======================================================= -->
<div id="overview-2" class="section level2" number="23.1">
<h2><span class="header-section-number">23.1</span> Overview</h2>
<p>This tab demonstrates the use of several packages for time series analysis.
It primarily relies on packages from the <a href="https://tidyverts.org/"><strong>tidyverts</strong></a>
family, but will also use the RECON <a href="https://github.com/reconhub/trending"><strong>trending</strong></a>
package to fit models that are more appropriate for infectious disease epidemiology.</p>
<p>Note in the below example we use a dataset from the <strong>surveillance</strong> package
on Campylobacter in Germany (see the <a href="https://epirhandbook.com/download-handbook-and-data.html">data chapter</a>,
of the handbook for details). However, if you wanted to run the same code on a dataset
with multiple countries or other strata, then there is an example code template for this in the
<a href="https://github.com/R4EPI/epitsa">r4epis github repo</a>.</p>
<p>Topics covered include:</p>
<ol style="list-style-type: decimal">
<li>Time series data</li>
<li>Descriptive analysis</li>
<li>Fitting regressions</li>
<li>Relation of two time series</li>
<li>Outbreak detection</li>
<li>Interrupted time series</li>
</ol>
<!-- ======================================================= -->
</div>
<div id="preparation-14" class="section level2" number="23.2">
<h2><span class="header-section-number">23.2</span> Preparation</h2>
<div id="packages-2" class="section level3 unnumbered">
<h3>Packages</h3>
<p>This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize <code>p_load()</code> from <strong>pacman</strong>, which installs the package if necessary <em>and</em> loads it for use. You can also load packages with <code>library()</code> from <strong>base</strong> R. See the page on <a href="https://epirhandbook.com/r-basics.html">R basics</a> for more information on R packages.</p>
</div>
<div id="load-data" class="section level3 unnumbered">
<h3>Load data</h3>
<p>You can download all the data used in this handbook via the instructions in the <a href="download-handbook-and-data.html#download-handbook-and-data">Download handbook and data</a> page.</p>
<p>The example dataset used in this section is weekly counts of campylobacter cases reported in Germany between 2001 and 2011. <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
You can click here to download<span> this data file (.xlsx).</span></a></p>
<p>This dataset is a reduced version of the dataset available in the <a href="https://cran.r-project.org/web/packages/surveillance/"><strong>surveillance</strong></a> package.
(for details load the surveillance package and see <code>?campyDE</code>)</p>
<p>Import these data with the <code>import()</code> function from the <strong>rio</strong> package (it handles many file types like .xlsx, .csv, .rds - see the <a href="import-and-export.html#import-and-export">Import and export</a> page for details).</p>
<p>The first 10 rows of the counts are displayed below.</p>
</div>
<div id="clean-data-1" class="section level3 unnumbered">
<h3>Clean data</h3>
<p>The code below makes sure that the date column is in the appropriate format.
For this tab we will be using the <strong>tsibble</strong> package and so the <code>yearweek</code>
function will be used to create a calendar week variable. There are several other
ways of doing this (see the <a href="https://epirhandbook.com/working-with-dates.html">Working with dates</a>
page for details), however for time series its best to keep within one framework (<strong>tsibble</strong>).</p>
</div>
<div id="download-climate-data" class="section level3 unnumbered">
<h3>Download climate data</h3>
<p>In the <em>relation of two time series</em> section of this page, we will be comparing
campylobacter case counts to climate data.</p>
<p>Climate data for anywhere in the world can be downloaded from the EU’s Copernicus
Satellite. These are not exact measurements, but based on a model (similar to
interpolation), however the benefit is global hourly coverage as well as forecasts.</p>
<p>You can download each of these climate data files from the <a href="download-handbook-and-data.html#download-handbook-and-data">Download handbook and data</a> page.</p>
<p>For purposes of demonstration here, we will show R code to use the <strong>ecmwfr</strong> package to pull these data from the Copernicus
climate data store. You will need to create a free account in order for this to
work. The package website has a useful <a href="https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds">walkthrough</a>
of how to do this. Below is example code of how to go about doing this, once you
have the appropriate API keys. You have to replace the X’s below with your account
IDs. You will need to download one year of data at a time otherwise the server times-out.</p>
<p>If you are not sure of the coordinates for a location you want to download data
for, you can use the <strong>tmaptools</strong> package to pull the coordinates off open street
maps. An alternative option is the <a href="https://github.com/rCarto/photon"><strong>photon</strong></a>
package, however this has not been released on to CRAN yet; the nice thing about
<strong>photon</strong> is that it provides more contextual data for when there are several
matches for your search.</p>
</div>
<div id="load-climate-data" class="section level3 unnumbered">
<h3>Load climate data</h3>
<p>Whether you downloaded the climate data via our handbook, or used the code above, you now should have 10 years of “.nc” climate data files stored in the same folder on your computer.</p>
<p>Use the code below to import these files into R with the <strong>stars</strong> package.</p>
<p>Once these files have been imported as the object <code>data</code>, we will convert them to a data frame.</p>
<!-- ======================================================= -->
</div>
</div>
<div id="time-series-data" class="section level2" number="23.3">
<h2><span class="header-section-number">23.3</span> Time series data</h2>
<p>There are a number of different packages for structuring and handling time series
data. As said, we will focus on the <strong>tidyverts</strong> family of packages and so will
use the <strong>tsibble</strong> package to define our time series object. Having a data set
defined as a time series object means it is much easier to structure our analysis.</p>
<p>To do this we use the <code>tsibble()</code> function and specify the “index”, i.e. the variable
specifying the time unit of interest. In our case this is the <code>epiweek</code> variable.</p>
<p>If we had a data set with weekly counts by province, for example, we would also
be able to specify the grouping variable using the <code>key =</code> argument.
This would allow us to do analysis for each group.</p>
<p>Looking at <code>class(counts)</code> tells you that on top of being a tidy data frame
(“tbl_df”, “tbl”, “data.frame”), it has the additional properties of a time series
data frame (“tbl_ts”).</p>
<p>You can take a quick look at your data by using <strong>ggplot2</strong>. We see from the plot that
there is a clear seasonal pattern, and that there are no missings. However, there
seems to be an issue with reporting at the beginning of each year; cases drop
in the last week of the year and then increase for the first week of the next year.</p>
<p><span style="color: red;"><strong><em>DANGER:</em></strong> Most datasets aren’t as clean as this example.
You will need to check for duplicates and missings as below. </span></p>
<!-- ======================================================= -->
<div id="duplicates" class="section level3 unnumbered">
<h3>Duplicates</h3>
<p><strong>tsibble</strong> does not allow duplicate observations. So each row will need to be
unique, or unique within the group (<code>key</code> variable).
The package has a few functions that help to identify duplicates. These include
<code>are_duplicated()</code> which gives you a TRUE/FALSE vector of whether the row is a
duplicate, and <code>duplicates()</code> which gives you a data frame of the duplicated rows.</p>
<p>See the page on <a href="https://epirhandbook.com/de-duplication.html">De-duplication</a>
for more details on how to select rows you want.</p>
<!-- ======================================================= -->
</div>
<div id="missings" class="section level3 unnumbered">
<h3>Missings</h3>
<p>We saw from our brief inspection above that there are no missings, but we also
saw there seems to be a problem with reporting delay around new year.
One way to address this problem could be to set these values to missing and then
to impute values. The simplest form of time series imputation is to draw
a straight line between the last non-missing and the next non-missing value.
To do this we will use the <strong>imputeTS</strong> package function <code>na_interpolation()</code>.</p>
<p>See the <a href="https://epirhandbook.com/missing-data.html">Missing data</a> page for other options for imputation.</p>
<p>Another alternative would be to calculate a moving average, to try and smooth
over these apparent reporting issues (see next section, and the page on <a href="https://epirhandbook.com/moving-averages.html">Moving averages</a>).</p>
<!-- ======================================================= -->
</div>
</div>
<div id="descriptive-analysis" class="section level2" number="23.4">
<h2><span class="header-section-number">23.4</span> Descriptive analysis</h2>
<!-- ======================================================= -->
<div id="timeseries_moving" class="section level3 unnumbered">
<h3>Moving averages</h3>
<p>If data is very noisy (counts jumping up and down) then it can be helpful to
calculate a moving average. In the example below, for each week we calculate the
average number of cases from the four previous weeks. This smooths the data, to
make it more interpretable. In our case this does not really add much, so we will
stick to the interpolated data for further analysis.
See the <a href="https://epirhandbook.com/moving-averages.html">Moving averages</a> page for more detail.</p>
<!-- ======================================================= -->
</div>
<div id="periodicity" class="section level3 unnumbered">
<h3>Periodicity</h3>
<p>Below we define a custom function to create a periodogram. See the <a href="writing-functions-1.html#writing-functions-1">Writing functions</a> page for information about how to write functions in R.</p>
<p>First, the function is defined. Its arguments include a dataset with a column <code>counts</code>, <code>start_week =</code> which is the first week of the dataset, a number to indicate how many periods per year (e.g. 52, 12), and lastly the output style (see details in the code below).</p>
<p><span style="color: black;"><strong><em>NOTE:</em></strong> It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) </span></p>
<!-- ======================================================= -->
</div>
<div id="decomposition" class="section level3 unnumbered">
<h3>Decomposition</h3>
<p>Classical decomposition is used to break a time series down several parts, which
when taken together make up for the pattern you see.
These different parts are:</p>
<ul>
<li>The trend-cycle (the long-term direction of the data)<br />
</li>
<li>The seasonality (repeating patterns)<br />
</li>
<li>The random (what is left after removing trend and season)</li>
</ul>
<!-- ======================================================= -->
</div>
<div id="autocorrelation" class="section level3 unnumbered">
<h3>Autocorrelation</h3>
<p>Autocorrelation tells you about the relation between the counts of each week
and the weeks before it (called lags).</p>
<p>Using the <code>ACF()</code> function, we can produce a plot which shows us a number of lines
for the relation at different lags. Where the lag is 0 (x = 0), this line would
always be 1 as it shows the relation between an observation and itself (not shown here).
The first line shown here (x = 1) shows the relation between each observation
and the observation before it (lag of 1), the second shows the relation between
each observation and the observation before last (lag of 2) and so on until lag of
52 which shows the relation between each observation and the observation from 1
year (52 weeks before).</p>
<p>Using the <code>PACF()</code> function (for partial autocorrelation) shows the same type of relation
but adjusted for all other weeks between. This is less informative for determining
periodicity.</p>
<p>You can formally test the null hypothesis of independence in a time series (i.e. 
that it is not autocorrelated) using the Ljung-Box test (in the <strong>stats</strong> package).
A significant p-value suggests that there is autocorrelation in the data.</p>
<!-- ======================================================= -->
</div>
</div>
<div id="fitting-regressions" class="section level2" number="23.5">
<h2><span class="header-section-number">23.5</span> Fitting regressions</h2>
<p>It is possible to fit a large number of different regressions to a time series,
however, here we will demonstrate how to fit a negative binomial regression - as
this is often the most appropriate for counts data in infectious diseases.</p>
<!-- ======================================================= -->
<div id="fourier-terms" class="section level3 unnumbered">
<h3>Fourier terms</h3>
<p>Fourier terms are the equivalent of sin and cosin curves. The difference is that
these are fit based on finding the most appropriate combination of curves to explain
your data.</p>
<p>If only fitting one fourier term, this would be the equivalent of fitting a sin
and a cosin for your most frequently occurring lag seen in your periodogram (in our
case 52 weeks). We use the <code>fourier()</code> function from the <strong>forecast</strong> package.</p>
<p>In the below code we assign using the <code>$</code>, as <code>fourier()</code> returns two columns (one
for sin one for cosin) and so these are added to the dataset as a list, called
“fourier” - but this list can then be used as a normal variable in regression.</p>
<!-- ======================================================= -->
</div>
<div id="negative-binomial" class="section level3 unnumbered">
<h3>Negative binomial</h3>
<p>It is possible to fit regressions using base <strong>stats</strong> or <strong>MASS</strong>
functions (e.g. <code>lm()</code>, <code>glm()</code> and <code>glm.nb()</code>). However we will be using those from
the <strong>trending</strong> package, as this allows for calculating appropriate confidence
and prediction intervals (which are otherwise not available).
The syntax is the same, and you specify an outcome variable then a tilde (~)
and then add your various exposure variables of interest separated by a plus (+).</p>
<p>The other difference is that we first define the model and then <code>fit()</code> it to the
data. This is useful because it allows for comparing multiple different models
with the same syntax.</p>
<p><span style="color: darkgreen;"><strong><em>TIP:</em></strong> If you wanted to use rates, rather than
counts you could include the population variable as a logarithmic offset term, by adding
<code>offset(log(population)</code>. You would then need to set population to be 1, before
using <code>predict()</code> in order to produce a rate. </span></p>
<p><span style="color: darkgreen;"><strong><em>TIP:</em></strong> For fitting more complex models such
as ARIMA or prophet, see the <a href="https://fable.tidyverts.org/index.html"><strong>fable</strong></a> package.</span></p>
<!-- ======================================================= -->
</div>
<div id="residuals" class="section level3 unnumbered">
<h3>Residuals</h3>
<p>To see how well our model fits the observed data we need to look at the residuals.
The residuals are the difference between the observed counts and the counts
estimated from the model. We could calculate this simply by using <code>case_int - estimate</code>,
but the <code>residuals()</code> function extracts this directly from the regression for us.</p>
<p>What we see from the below, is that we are not explaining all of the variation
that we could with the model. It might be that we should fit more fourier terms,
and address the amplitude. However for this example we will leave it as is.
The plots show that our model does worse in the peaks and troughs (when counts are
at their highest and lowest) and that it might be more likely to underestimate
the observed counts.</p>
<!-- ======================================================= -->
</div>
</div>
<div id="relation-of-two-time-series" class="section level2" number="23.6">
<h2><span class="header-section-number">23.6</span> Relation of two time series</h2>
<p>Here we look at using weather data (specifically the temperature) to explain
campylobacter case counts.</p>
<!-- ======================================================= -->
<div id="merging-datasets" class="section level3 unnumbered">
<h3>Merging datasets</h3>
<p>We can join our datasets using the week variable. For more on merging see the
handbook section on <a href="https://epirhandbook.com/joining-data.html">joining</a>.</p>
<!-- ======================================================= -->
</div>
<div id="descriptive-analysis-1" class="section level3 unnumbered">
<h3>Descriptive analysis</h3>
<p>First plot your data to see if there is any obvious relation.
The plot below shows that there is a clear relation in the seasonality of the two
variables, and that temperature might peak a few weeks before the case number.
For more on pivoting data, see the handbook section on <a href="https://epirhandbook.com/pivoting-data.html">pivoting data</a>.</p>
<!-- ======================================================= -->
</div>
<div id="lags-and-cross-correlation" class="section level3 unnumbered">
<h3>Lags and cross-correlation</h3>
<p>To formally test which weeks are most highly related between cases and temperature.
We can use the cross-correlation function (<code>CCF()</code>) from the <strong>feasts</strong> package.
You could also visualise (rather than using <code>arrange</code>) using the <code>autoplot()</code> function.</p>
<p>We see from this that a lag of 4 weeks is most highly correlated,
so we make a lagged temperature variable to include in our regression.</p>
<!-- ======================================================= -->
</div>
<div id="negative-binomial-with-two-variables" class="section level3 unnumbered">
<h3>Negative binomial with two variables</h3>
<p>We fit a negative binomial regression as done previously. This time we add the
temperature variable lagged by four weeks.</p>
<p>To investigate the individual terms, we can pull the original negative binomial
regression out of the <strong>trending</strong> format using <code>get_model()</code> and pass this to the
<strong>broom</strong> package <code>tidy()</code> function to retrieve exponentiated estimates and associated
confidence intervals.</p>
<p>What this shows us is that lagged temperature, after controlling for trend and seasonality,
is similar to the case counts (estimate ~ 1) and significantly associated.
This suggests that it might be a good variable for use in predicting future case
numbers (as climate forecasts are readily available).</p>
<p>A quick visual inspection of the model shows that it might do a better job of
estimating the observed case counts.</p>
<div id="residuals-1" class="section level4 unnumbered">
<h4>Residuals</h4>
<p>We investigate the residuals again to see how well our model fits the observed data.
The results and interpretation here are similar to those of the previous regression,
so it may be more feasible to stick with the simpler model without temperature.</p>
<!-- ======================================================= -->
</div>
</div>
</div>
<div id="outbreak-detection" class="section level2" number="23.7">
<h2><span class="header-section-number">23.7</span> Outbreak detection</h2>
<p>We will demonstrate two (similar) methods of detecting outbreaks here.
The first builds on the sections above.
We use the <strong>trending</strong> package to fit regressions to previous years, and then
predict what we expect to see in the following year. If observed counts are above
what we expect, then it could suggest there is an outbreak.
The second method is based on similar principles but uses the <strong>surveillance</strong> package,
which has a number of different algorithms for aberration detection.</p>
<p><span style="color: orange;"><strong><em>CAUTION:</em></strong> Normally, you are interested in the current year (where you only know counts up to the present week). So in this example we are pretending to be in week 52 of 2011.</span></p>
<!-- ======================================================= -->
<div id="trending-package" class="section level3 unnumbered">
<h3><strong>trending</strong> package</h3>
<p>For this method we define a baseline (which should usually be about 5 years of data).
We fit a regression to the baseline data, and then use that to predict the estimates
for the next year.</p>
<!-- ======================================================= -->
<div id="cut-off-date" class="section level4 unnumbered">
<h4>Cut-off date</h4>
<p>It is easier to define your dates in one place and then use these throughout the
rest of your code.</p>
<p>Here we define a start date (when our observations started) and a cut-off date
(the end of our baseline period - and when the period we want to predict for starts).
~We also define how many weeks are in our year of interest (the one we are going to
be predicting)~.
We also define how many weeks are between our baseline cut-off and the end date
that we are interested in predicting for.</p>
<p><span style="color: black;"><strong><em>NOTE:</em></strong> In this example we pretend to currently be at the end of September 2011 (“2011 W39”).</span></p>
<!-- ======================================================= -->
</div>
<div id="add-rows-1" class="section level4 unnumbered">
<h4>Add rows</h4>
<p>To be able to forecast in a tidyverse format, we need to have the right number
of rows in our dataset, i.e. one row for each week up to the <code>end_date</code>defined above.
The code below allows you to add these rows for by a grouping variable - for example
if we had multiple countries in one dataset, we could group by country and then
add rows appropriately for each.
The <code>group_by_key()</code> function from <strong>tsibble</strong> allows us to do this grouping
and then pass the grouped data to <strong>dplyr</strong> functions, <code>group_modify()</code> and
<code>add_row()</code>. Then we specify the sequence of weeks between one after the maximum week
currently available in the data and the end week.</p>
<!-- ======================================================= -->
</div>
<div id="fourier-terms-1" class="section level4 unnumbered">
<h4>Fourier terms</h4>
<p>We need to redefine our fourier terms - as we want to fit them to the baseline
date only and then predict (extrapolate) those terms for the next year.
To do this we need to combine two output lists from the <code>fourier()</code> function together;
the first one is for the baseline data, and the second one predicts for the
year of interest (by defining the <code>h</code> argument).</p>
<p><em>N.b.</em> to bind rows we have to use <code>rbind()</code> (rather than tidyverse <code>bind_rows</code>) as
the fourier columns are a list (so not named individually).</p>
<!-- ======================================================= -->
</div>
<div id="split-data-and-fit-regression" class="section level4 unnumbered">
<h4>Split data and fit regression</h4>
<p>We now have to split our dataset in to the baseline period and the prediction
period. This is done using the <strong>dplyr</strong> <code>group_split()</code> function after <code>group_by()</code>,
and will create a list with two data frames, one for before your cut-off and one
for after.</p>
<p>We then use the <strong>purrr</strong> package <code>pluck()</code> function to pull the datasets out of the
list (equivalent of using square brackets, e.g. <code>dat[[1]]</code>), and can then fit
our model to the baseline data, and then use the <code>predict()</code> function for our data
of interest after the cut-off.</p>
<p>See the page on <a href="https://epirhandbook.com/iteration-loops-and-lists.html">Iteration</a> to learn more about <strong>purrr</strong>.</p>
<p>As previously, we can visualise our model with <strong>ggplot</strong>. We highlight alerts with
red dots for observed counts above the 95% prediction interval.
This time we also add a vertical line to label when the forecast starts.</p>
<!-- ======================================================= -->
</div>
<div id="prediction-validation" class="section level4 unnumbered">
<h4>Prediction validation</h4>
<p>Beyond inspecting residuals, it is important to investigate how good your model is
at predicting cases in the future. This gives you an idea of how reliable your
threshold alerts are.</p>
<p>The traditional way of validating is to see how well you can predict the latest
year before the present one (because you don’t yet know the counts for the “current year”).
For example in our data set we would use the data from 2002 to 2009 to predict 2010,
and then see how accurate those predictions are. Then refit the model to include
2010 data and use that to predict 2011 counts.</p>
<p>As can be seen in the figure below by <em>Hyndman et al</em> in <a href="https://otexts.com/fpp3/">“Forecasting principles
and practice”</a>.</p>
<p><img src="https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png" />
<em>figure reproduced with permission from the authors</em></p>
<p>The downside of this is that you are not using all the data available to you, and
it is not the final model that you are using for prediction.</p>
<p>An alternative is to use a method called cross-validation. In this scenario you
roll over all of the data available to fit multiple models to predict one year ahead.
You use more and more data in each model, as seen in the figure below from the
same [<em>Hyndman et al</em> text]((<a href="https://otexts.com/fpp3/" class="uri">https://otexts.com/fpp3/</a>).
For example, the first model uses 2002 to predict 2003, the second uses 2002 and
2003 to predict 2004, and so on.
<img src="https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png" />
<em>figure reproduced with permission from the authors</em></p>
<p>In the below we use <strong>purrr</strong> package <code>map()</code> function to loop over each dataset.
We then put estimates in one data set and merge with the original case counts,
to use the <strong>yardstick</strong> package to compute measures of accuracy.
We compute four measures including: Root mean squared error (RMSE), Mean absolute error
(MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).</p>
<!-- ======================================================= -->
</div>
</div>
<div id="surveillance-package" class="section level3 unnumbered">
<h3><strong>surveillance</strong> package</h3>
<p>In this section we use the <strong>surveillance</strong> package to create alert thresholds
based on outbreak detection algorithms. There are several different methods
available in the package, however we will focus on two options here.
For details, see these papers on the <a href="https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf">application</a>
and <a href="https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf">theory</a>
of the alogirthms used.</p>
<p>The first option uses the improved Farrington method. This fits a negative
binomial glm (including trend) and down-weights past outbreaks (outliers) to
create a threshold level.</p>
<p>The second option use the glrnb method. This also fits a negative binomial glm
but includes trend and fourier terms (so is favoured here). The regression is used
to calculate the “control mean” (~fitted values) - it then uses a computed
generalized likelihood ratio statistic to assess if there is shift in the mean
for each week. Note that the threshold for each week takes in to account previous
weeks so if there is a sustained shift an alarm will be triggered.
(Also note that after each alarm the algorithm is reset)</p>
<p>In order to work with the <strong>surveillance</strong> package, we first need to define a
“surveillance time series” object (using the <code>sts()</code> function) to fit within the
framework.</p>
<!-- ======================================================= -->
<div id="farrington-method" class="section level4 unnumbered">
<h4>Farrington method</h4>
<p>We then define each of our parameters for the Farrington method in a <code>list</code>.
Then we run the algorithm using <code>farringtonFlexible()</code> and then we can extract the
threshold for an alert using <code>farringtonmethod@upperbound</code>to include this in our
dataset. It is also possible to extract a TRUE/FALSE for each week if it triggered
an alert (was above the threshold) using <code>farringtonmethod@alarm</code>.</p>
<p>We can then visualise the results in ggplot as done previously.</p>
<!-- ======================================================= -->
</div>
<div id="glrnb-method" class="section level4 unnumbered">
<h4>GLRNB method</h4>
<p>Similarly for the GLRNB method we define each of our parameters for the in a <code>list</code>,
then fit the algorithm and extract the upper bounds.</p>
<p><span style="color: orange;"><strong><em>CAUTION:</em></strong> This method uses “brute force” (similar to bootstrapping) for calculating thresholds, so can take a long time!</span></p>
<p>See the <a href="https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf">GLRNB vignette</a>
for details.</p>
<p>Visualise the outputs as previously.</p>
<!-- ======================================================= -->
</div>
</div>
</div>
<div id="interrupted-timeseries" class="section level2" number="23.8">
<h2><span class="header-section-number">23.8</span> Interrupted timeseries</h2>
<p>Interrupted timeseries (also called segmented regression or intervention analysis),
is often used in assessing the impact of vaccines on the incidence of disease.
But it can be used for assessing impact of a wide range of interventions or introductions.
For example changes in hospital procedures or the introduction of a new disease
strain to a population.
In this example we will pretend that a new strain of Campylobacter was introduced
to Germany at the end of 2008, and see if that affects the number of cases.
We will use negative binomial regression again. The regression this time will be
split in to two parts, one before the intervention (or introduction of new strain here)
and one after (the pre and post-periods). This allows us to calculate an incidence rate ratio comparing the
two time periods. Explaining the equation might make this clearer (if not then just
ignore!).</p>
<p>The negative binomial regression can be defined as follows:</p>
<p><span class="math display">\[\log(Y_t)= β_0 + β_1 \times t+ β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+ + log(pop_t) + e_t\]</span></p>
<p>Where:
<span class="math inline">\(Y_t\)</span>is the number of cases observed at time <span class="math inline">\(t\)</span><br />
<span class="math inline">\(pop_t\)</span> is the population size in 100,000s at time <span class="math inline">\(t\)</span> (not used here)<br />
<span class="math inline">\(t_0\)</span> is the last year of the of the pre-period (including transition time if any)<br />
<span class="math inline">\(δ(x\)</span> is the indicator function (it is 0 if x≤0 and 1 if x&gt;0)<br />
<span class="math inline">\((x)^+\)</span> is the cut off operator (it is x if x&gt;0 and 0 otherwise)<br />
<span class="math inline">\(e_t\)</span> denotes the residual
Additional terms trend and season can be added as needed.</p>
<p><span class="math inline">\(β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+\)</span> is the generalised linear
part of the post-period and is zero in the pre-period.
This means that the <span class="math inline">\(β_2\)</span> and <span class="math inline">\(β_3\)</span> estimates are the effects of the intervention.</p>
<p>We need to re-calculate the fourier terms without forecasting here, as we will use
all the data available to us (i.e. retrospectively). Additionally we need to calculate
the extra terms needed for the regression.</p>
<p>We then use these terms to fit a negative binomial regression, and produce a
table with percentage change. What this example shows is that there was no
significant change.</p>
<p>As previously we can visualise the outputs of the regression.</p>
<!-- ======================================================= -->
</div>
<div id="resources-16" class="section level2" number="23.9">
<h2><span class="header-section-number">23.9</span> Resources</h2>
<p><a href="https://otexts.com/fpp3/">forecasting: principles and practice textbook</a><br />
<a href="https://github.com/EPIET/TimeSeriesAnalysis">EPIET timeseries analysis case studies</a><br />
<a href="https://online.stat.psu.edu/stat510/lesson/1">Penn State course</a>
<a href="https://www.jstatsoft.org/article/view/v070i10">Surveillance package manuscript</a></p>

</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>The Epidemiologist R Handbook</strong>" was written by the handbook team. It was last built on 2021-05-30.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
